+ export TORCH_HOME=../../pretrained_models
+ TORCH_HOME=../../pretrained_models
+ model=ast
+ dataset=custom
+ nocall=True
+ timetest = True
/var/spool/slurm/d/job3243145/slurm_script: line 24: timetest: command not found
+ imagenetpretrain=True
+ audiosetpretrain=True
+ bal=False
+ '[' True == True ']'
+ lr=1e-5
+ freqm=24
+ timem=96
+ mixup=0
+ epoch=6
+ batch_size=48
+ fstride=10
+ tstride=10
+ base_exp_dir=./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_50class_solid
+ '[' -d ./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_50class_solid ']'
+ mkdir -p
mkdir: missing operand
Try 'mkdir --help' for more information.
+ (( fold=1 ))
+ (( fold<=1 ))
+ echo 'now process fold1'
now process fold1
+ exp_dir=./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_50class_solid/fold1
+ tr_data=./data/datafiles/custom_train_data_1.json
+ te_data=./data/datafiles/custom_eval_data_1.json
+ CUDA_CACHE_DISABLE=1
+ python -W ignore ../../src/run.py --model ast --dataset custom --data-train ./data/datafiles/custom_train_data_1.json --data-val ./data/datafiles/custom_eval_data_1.json --exp-dir ./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_50class_solid/fold1 --label-csv ./data/custom_labels.csv --n_class 50 --lr 1e-5 --n-epochs 6 --batch-size 48 --save_model False --freqm 24 --timem 96 --mixup 0 --bal False --tstride 10 --fstride 10 --imagenet_pretrain True --audioset_pretrain True
wandb: Currently logged in as: birdsongs (use `wandb login --relogin` to force relogin)
I am process 95420, running on ai02.kuacc.ku.edu.tr: starting (Sun May  8 20:31:08 2022)
wandb: Tracking run with wandb version 0.12.9
wandb: Syncing run daily-dew-3
wandb:  View project at https://wandb.ai/birdsongs/istangul-50-class
wandb:  View run at https://wandb.ai/birdsongs/istangul-50-class/runs/m7jrmowk
wandb: Run data is saved locally in /scratch/users/fsofian19/COMP491_model/models/ast_custom/egs/custom/wandb/run-20220508_203109-m7jrmowk
wandb: Run `wandb offline` to turn off syncing.

now train a audio spectrogram transformer model
balanced sampler is not used
---------------the train dataloader---------------
now using following mask: 24 freq, 96 time
now using mix-up with rate 0.000000
now process custom
use dataset mean -6.442 and std 4.178 to normalize the input.
number of classes is 50
---------------the evaluation dataloader---------------
now using following mask: 0 freq, 0 time
now using mix-up with rate 0.000000
now process custom
use dataset mean -6.442 and std 4.178 to normalize the input.
number of classes is 50
---------------AST Model Summary---------------
ImageNet pretraining: True, AudioSet pretraining: True
frequncey stride=10, time stride=10
number of patches=600

Creating experiment directory: ./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_50class_solid/fold1
Now starting training for 6 epochs
2022-05-08 20:31:32.580435
running on cuda
state dict to model completed => pretrained weights loaded
<generator object Module.named_parameters at 0x2b53af3e5660>
Total parameter number is : 87.295 million
Total trainable parameter number is : 87.295 million
scheduler for custom dataset is used
now training with custom, main metrics: mAP, loss function: BCEWithLogitsLoss(), learning rate scheduler: <torch.optim.lr_scheduler.MultiStepLR object at 0x2b53dca48670>
current #steps=0, #epochs=1
start training...
---------------
2022-05-08 20:31:32.986960
current #epochs=1, #steps=0
LOSS is 0.7215569842606784
LOSS is 0.6364006283196311
LOSS is 0.5565837942156941
LOSS is 0.47824263949568074
LOSS is 0.42162801411623757
LOSS is 0.38772176702817285
LOSS is 0.35093564371888836
LOSS is 0.3231490012025461
LOSS is 0.30386245908836523
LOSS is 0.2768084634949143
LOSS is 0.26764956208566826
LOSS is 0.2465993208003541
LOSS is 0.2362731347512454
LOSS is 0.23039756299617392
LOSS is 0.22175749494073294
LOSS is 0.21356326399215808
LOSS is 0.20388063839171083
LOSS is 0.19938178060886763
LOSS is 0.19369580765410016
LOSS is 0.18816497105018548
LOSS is 0.18557136056323847
LOSS is 0.1812973140583684
LOSS is 0.17908197563452027
LOSS is 0.1724830079280461
LOSS is 0.16984602622687817
LOSS is 0.1670326576847583
LOSS is 0.16433675842980544
LOSS is 0.16249477084105216
LOSS is 0.16148552533704788
LOSS is 0.16080921545314295
LOSS is 0.15720089929644018
LOSS is 0.1536255363902698
LOSS is 0.15380720340957246
LOSS is 0.1510531105939299
LOSS is 0.1517191599759584
LOSS is 0.15050337679684164
LOSS is 0.1497032778679083
LOSS is 0.14637302383935702
LOSS is 0.14708622527463983
LOSS is 0.1464160715804125
LOSS is 0.1448165125058343
LOSS is 0.14269084919864933
LOSS is 0.14300024800002575
LOSS is 0.14229143572039904
LOSS is 0.1392814470936234
LOSS is 0.14051239855897923
LOSS is 0.1366373564566796
LOSS is 0.1380698136674861
LOSS is 0.13695974121025453
LOSS is 0.13760328988699863
LOSS is 0.13553344874021908
LOSS is 0.13526519204179446
LOSS is 0.1371567811056351
LOSS is 0.13299804764954995
LOSS is 0.13350244788142543
LOSS is 0.1337603161809966
LOSS is 0.1343776834399129
LOSS is 0.13318001945037394
LOSS is 0.13393900975274545
LOSS is 0.12961040351695072
LOSS is 0.13247107027719418
LOSS is 0.13068149097574255
LOSS is 0.1300215910297508
LOSS is 0.12865879902926586
LOSS is 0.12884729366128644
LOSS is 0.12885439849924296
LOSS is 0.128532589461344
LOSS is 0.12868396427948028
LOSS is 0.12656889520119877
LOSS is 0.1249050097198536
LOSS is 0.1266935147934904
LOSS is 0.12597037089212487
LOSS is 0.1253623199633633
LOSS is 0.1273213840990017
LOSS is 0.12589675592258573
LOSS is 0.12510537239102026
LOSS is 0.12378561947339525
LOSS is 0.12716667703352869
LOSS is 0.12359806387995681
LOSS is 0.12461666121302795
LOSS is 0.12291430208521585
LOSS is 0.1231824137751634
LOSS is 0.12243644925610474
LOSS is 0.12273160110072544
LOSS is 0.1217372231433789
LOSS is 0.12144015488758063
LOSS is 0.12197659150231631
LOSS is 0.12076214484249553
LOSS is 0.11998544033616781
LOSS is 0.12051600334389757
LOSS is 0.12155689363678297
LOSS is 0.12027051153903207
LOSS is 0.11932877390024564
LOSS is 0.11892046988010407
LOSS is 0.11746228924176345
LOSS is 0.12049054846012344
LOSS is 0.1181666370558863
LOSS is 0.11943934264282387
LOSS is 0.1173734846773247
LOSS is 0.11799989756351958
LOSS is 0.11777763851297399
Epoch: [1][100/2183]	Per Sample Total Time 1.25210	Per Sample Data Time 1.23310	Per Sample DNN Time 0.01900	Train Loss 0.1751	
LOSS is 0.11605644642375411
LOSS is 0.11763830662937835
LOSS is 0.11826108204123255
LOSS is 0.11669991064040612
LOSS is 0.11716400589638701
LOSS is 0.1161628541102012
LOSS is 0.11505733186301466
LOSS is 0.11406765871914104
LOSS is 0.11429565157275648
LOSS is 0.11610491728022075
LOSS is 0.11460319829018166
LOSS is 0.11548090457217769
LOSS is 0.1132218609051779
LOSS is 0.11618439487957707
LOSS is 0.11132009942705433
LOSS is 0.11402649672469124
LOSS is 0.10982777733976642
LOSS is 0.1124397889361717
LOSS is 0.1113570089909869
LOSS is 0.1110153541349185
LOSS is 0.11275524829824767
LOSS is 0.1116064981914436
LOSS is 0.11182055656487744
LOSS is 0.11002130943117663
LOSS is 0.11138455266986663
LOSS is 0.11199596908952421
LOSS is 0.1117577656172216
LOSS is 0.11146317095495761
LOSS is 0.11156534302203606
LOSS is 0.11312065328937025
LOSS is 0.11279948016162962
LOSS is 0.10898156656650827
LOSS is 0.1065079156922487
LOSS is 0.11087629425882672
LOSS is 0.1077337499226754
LOSS is 0.11011185011050355
LOSS is 0.1094892773544416
LOSS is 0.10764614234756058
LOSS is 0.1099192593448485
LOSS is 0.10768128418829293
LOSS is 0.10795841856005912
LOSS is 0.10794482262649885
LOSS is 0.1024062495166436
LOSS is 0.10697827351589999
LOSS is 0.10958537442920109
LOSS is 0.10447241022950039
LOSS is 0.1073134643250766
LOSS is 0.10843144555420926
LOSS is 0.10151480805827305
LOSS is 0.10436460027548795
LOSS is 0.10622950854711236
LOSS is 0.10652051083588353
LOSS is 0.10634787483218437
LOSS is 0.10288497647503392
LOSS is 0.10330130383449918
LOSS is 0.10531700635794551
LOSS is 0.1027858159551397
LOSS is 0.10374122734647245
LOSS is 0.10280522331828253
LOSS is 0.10229631451812263
LOSS is 0.10683895617723466
LOSS is 0.10565593053819612
LOSS is 0.10243739990827938
LOSS is 0.10437799628668776
LOSS is 0.09882329718830686
LOSS is 0.1010816908782969
LOSS is 0.10397061848004038
LOSS is 0.10247631222630542
LOSS is 0.09963538118638099
LOSS is 0.09817458897518616
LOSS is 0.10014109051243092
LOSS is 0.10069843471748754
LOSS is 0.10122995211044328
LOSS is 0.09889751329009111
LOSS is 0.0992730072924557
LOSS is 0.09699352427618578
LOSS is 0.1001199907111004
LOSS is 0.09786355649741987
LOSS is 0.09973963478580118
LOSS is 0.09490074862648423
LOSS is 0.09625524455759055
LOSS is 0.09635664617797982
LOSS is 0.09820337102670844
LOSS is 0.10078161950223148
LOSS is 0.09934330120294665
LOSS is 0.09543539892183617
LOSS is 0.09617214973938341
LOSS is 0.09816764883153763
LOSS is 0.09958641521089401
LOSS is 0.09449510842794553
LOSS is 0.09937268263582762
LOSS is 0.09622340514014165
LOSS is 0.09232878867847225
LOSS is 0.09656654088835542
LOSS is 0.09949995424598457
LOSS is 0.09635991755562524
LOSS is 0.09291313872051736
LOSS is 0.09392454584517207
LOSS is 0.09649468001754334
LOSS is 0.09699740752888224
Epoch: [1][200/2183]	Per Sample Total Time 2.07876	Per Sample Data Time 2.06097	Per Sample DNN Time 0.01780	Train Loss 0.1405	
LOSS is 0.09205562731639172
LOSS is 0.09008165520460656
LOSS is 0.09684959693423784
LOSS is 0.09235322430729867
LOSS is 0.0926362304086797
LOSS is 0.09064448410451102
LOSS is 0.09585627130077531
LOSS is 0.09466385891195388
LOSS is 0.09235820567623403
LOSS is 0.09594339910351361
LOSS is 0.09285977678063016
LOSS is 0.09211143847166872
LOSS is 0.09273835761860634
LOSS is 0.08950506710447372
LOSS is 0.09190011139260605
LOSS is 0.09265490003939097
LOSS is 0.09315559162098605
LOSS is 0.08750785577421387
LOSS is 0.09049779882266497
LOSS is 0.09321918864074785
LOSS is 0.092269489497412
LOSS is 0.088006011430795
LOSS is 0.09320961254881696
LOSS is 0.0910709106277985
LOSS is 0.08660100375379746
LOSS is 0.08805213869859775
LOSS is 0.090964452738408
LOSS is 0.08957263023359702
LOSS is 0.08504176309138227
LOSS is 0.08784512496398142
LOSS is 0.08717334791009004
LOSS is 0.08727177370494853
LOSS is 0.08649420488703376
LOSS is 0.08778058896151682
LOSS is 0.08421033817653856
LOSS is 0.09007881418025743
LOSS is 0.08941370671149344
LOSS is 0.08624933020599808
LOSS is 0.08569997349055485
LOSS is 0.0868586260600326
LOSS is 0.08838105126904945
LOSS is 0.08194015453530787
LOSS is 0.08335578240454197
LOSS is 0.08642667862664288
LOSS is 0.08593200068261164
LOSS is 0.09006785942629601
LOSS is 0.09104266765294597
LOSS is 0.08522914918993289
LOSS is 0.08634490744521221
LOSS is 0.08657592130398067
LOSS is 0.0873107505492711
LOSS is 0.08903513117770975
LOSS is 0.08493735674070195
LOSS is 0.08527836177653324
LOSS is 0.0879470570315607
LOSS is 0.08533445240774502
LOSS is 0.08646654446492903
LOSS is 0.0827840052661486
LOSS is 0.08764393698191271
LOSS is 0.08821029302741712
LOSS is 0.08129377651843243
LOSS is 0.08027320101701965
LOSS is 0.08375238114269451
LOSS is 0.08647707562427968
LOSS is 0.08043525314152551
LOSS is 0.08399669585439067
LOSS is 0.08262606440189606
LOSS is 0.08175651411525905
LOSS is 0.08026576014002786
LOSS is 0.0892649514891673
LOSS is 0.08123024448830014
LOSS is 0.08124234423507005
LOSS is 0.08633638676838018
LOSS is 0.08288768373740217
LOSS is 0.0780359867886485
LOSS is 0.08110481206599313
LOSS is 0.07830827833230918
LOSS is 0.08207866062837033
LOSS is 0.07804495544987732
LOSS is 0.08686737789811257
LOSS is 0.08105956906219945
LOSS is 0.07773406283309063
LOSS is 0.07900239125088168
LOSS is 0.07999723789709
LOSS is 0.08498728195127721
LOSS is 0.07636491194212189
LOSS is 0.07899763156815122
LOSS is 0.08092242990930876
LOSS is 0.08241927112646712
LOSS is 0.08134461126445483
LOSS is 0.08095194618256453
LOSS is 0.07896345698623919
LOSS is 0.07954517798498273
LOSS is 0.07670082629968723
LOSS is 0.07689810038272603
LOSS is 0.07892252314913398
LOSS is 0.08022298571420834
LOSS is 0.07417564353090711
LOSS is 0.07889563126722351
LOSS is 0.07442898785695434
Epoch: [1][300/2183]	Per Sample Total Time 2.90905	Per Sample Data Time 2.89166	Per Sample DNN Time 0.01739	Train Loss 0.1223	
LOSS is 0.08263432024240805
LOSS is 0.07521885376345988
LOSS is 0.07653586846621087
LOSS is 0.07020982670442512
LOSS is 0.0791721752875795
LOSS is 0.08219293504875774
LOSS is 0.07915846393443644
LOSS is 0.07786226260087764
LOSS is 0.07638332741335035
LOSS is 0.07513381460060677
LOSS is 0.0786537304512846
LOSS is 0.0736032437098523
LOSS is 0.0810587660522045
LOSS is 0.07549396001578619
LOSS is 0.0737211973379211
LOSS is 0.07606163158935185
LOSS is 0.07502873243453602
LOSS is 0.0721418739700069
LOSS is 0.07784935666713864
LOSS is 0.07587996096660694
LOSS is 0.07940982986940072
LOSS is 0.07982517219497823
LOSS is 0.07114699356490746
LOSS is 0.07667288090257596
LOSS is 0.07154725328243028
LOSS is 0.0729551451614437
LOSS is 0.07465902285727984
LOSS is 0.07499115757721787
LOSS is 0.07411889197382455
LOSS is 0.08033005957608112
LOSS is 0.07301993549374554
LOSS is 0.07294770888790178
LOSS is 0.07525322816528691
LOSS is 0.07870922075817362
LOSS is 0.07140921527286992
LOSS is 0.07140856078903501
LOSS is 0.07623357043485157
LOSS is 0.07499748974650478
LOSS is 0.07771195736985344
LOSS is 0.0750021492317319
LOSS is 0.06911479768576101
LOSS is 0.07629851696469511
LOSS is 0.07730281953234226
LOSS is 0.07209299263195135
LOSS is 0.07810568938303429
LOSS is 0.07730535309839373
LOSS is 0.0763811403423703
LOSS is 0.07612715858461647
LOSS is 0.07714666335688283
LOSS is 0.07019641618787621
LOSS is 0.0707845736888703
LOSS is 0.07136913952184841
LOSS is 0.06934062754075664
LOSS is 0.07111089958692901
LOSS is 0.07073067792148019
LOSS is 0.07466934524825775
LOSS is 0.07446882247381534
LOSS is 0.07607569105457515
LOSS is 0.07394775636377744
LOSS is 0.06403108355550406
LOSS is 0.07824488137227795
LOSS is 0.07572342278862683
LOSS is 0.06898681590023141
LOSS is 0.07250630460679532
LOSS is 0.066441399671603
LOSS is 0.06930615180754103
LOSS is 0.07049844738794492
LOSS is 0.07024900006945245
LOSS is 0.07017604654693665
LOSS is 0.07109774573512065
LOSS is 0.07136952386160071
LOSS is 0.0655471784924157
LOSS is 0.07225707444905614
LOSS is 0.07433291927406875
LOSS is 0.07541124984622002
LOSS is 0.07224649455126686
LOSS is 0.06580821905013484
LOSS is 0.06637906383102139
LOSS is 0.07332607851130889
LOSS is 0.06967040475729543
LOSS is 0.06681486705240483
LOSS is 0.06452030141779688
LOSS is 0.07457784175057897
LOSS is 0.06799410180343936
LOSS is 0.06391987756205102
LOSS is 0.0648034397563121
LOSS is 0.06813639969332144
LOSS is 0.061383230192974834
LOSS is 0.06945894137412931
LOSS is 0.06582843046907025
LOSS is 0.06902496935877328
LOSS is 0.07144723600746754
LOSS is 0.06421772797009909
LOSS is 0.06495583168463782
LOSS is 0.06313730601570569
LOSS is 0.0754460758137672
LOSS is 0.06839088419723946
LOSS is 0.06483761682446736
LOSS is 0.07469166721605386
LOSS is 0.07314497551415117
Epoch: [1][400/2183]	Per Sample Total Time 3.73997	Per Sample Data Time 3.72280	Per Sample DNN Time 0.01717	Train Loss 0.1099	
wandb: Network error (ReadTimeout), entering retry loop.
LOSS is 0.06717496394218567
LOSS is 0.06433339832467028
LOSS is 0.07079105701607963
LOSS is 0.0690947055630386
LOSS is 0.0615368456102442
LOSS is 0.07556470658125666
LOSS is 0.06810239290081276
LOSS is 0.06903425833093935
LOSS is 0.058924826710329704
LOSS is 0.06614442726092724
LOSS is 0.07106904510835496
LOSS is 0.0643177457091709
LOSS is 0.06149272716604173
LOSS is 0.06526149513393951
LOSS is 0.06750717347681834
LOSS is 0.07041508451957877
LOSS is 0.06617164313987209
LOSS is 0.06847449294446657
LOSS is 0.0659799967345316
LOSS is 0.06315660033995907
LOSS is 0.061047780497077236
LOSS is 0.06890103167893055
LOSS is 0.060809396225182964
LOSS is 0.06287679970807707
LOSS is 0.06695854837385318
LOSS is 0.0617931241375239
LOSS is 0.06102695444327159
LOSS is 0.06183124896992619
LOSS is 0.06318089421566887
LOSS is 0.06601591426103066
LOSS is 0.062193338283880926
LOSS is 0.07016399264762489
LOSS is 0.06200508580698322
LOSS is 0.07248343016098564
LOSS is 0.06621646063984372
LOSS is 0.06736274134988586
LOSS is 0.06667777201587645
LOSS is 0.06938350226730108
LOSS is 0.06035442549851723
LOSS is 0.06137732101061071
LOSS is 0.06789485119166784
LOSS is 0.05998475061651941
LOSS is 0.06576302008121275
LOSS is 0.06489156917319633
LOSS is 0.061339104944296806
LOSS is 0.06893689116598883
LOSS is 0.06491615286290957
LOSS is 0.05566246558795684
LOSS is 0.06244657470767076
LOSS is 0.06495274450397119
LOSS is 0.06424336606985889
LOSS is 0.05900029894042139
LOSS is 0.05543589239707217
LOSS is 0.0641082921026585
LOSS is 0.06939404229439484
LOSS is 0.06885097014795369
LOSS is 0.06111406530602836
LOSS is 0.06504132195414665
LOSS is 0.07058041561123295
LOSS is 0.06679853323691835
LOSS is 0.05635637255695959
LOSS is 0.06325166821014136
LOSS is 0.06598280867988554
LOSS is 0.05535736886512799
LOSS is 0.059924508136852336
LOSS is 0.05369606910506264
LOSS is 0.0635141688065293
LOSS is 0.059412775118059175
LOSS is 0.06037415004781603
LOSS is 0.06586743017445164
LOSS is 0.05966794694773853
LOSS is 0.06049084154927793
LOSS is 0.06559836908748064
LOSS is 0.06065238638431766
LOSS is 0.06222242456298167
LOSS is 0.06280348893643047
LOSS is 0.060755237723933535
LOSS is 0.061075503063621
LOSS is 0.056678808076928065
LOSS is 0.056195832669036466
LOSS is 0.058211052177551514
LOSS is 0.060662559419676354
LOSS is 0.06274047382719194
LOSS is 0.05560664689013114
LOSS is 0.06432794474414551
LOSS is 0.059720760012666387
LOSS is 0.06639127467448513
LOSS is 0.0636341038053312
LOSS is 0.05661450754463052
LOSS is 0.05955351878811295
LOSS is 0.05858897153132906
LOSS is 0.059647872556233786
LOSS is 0.06383045505227831
LOSS is 0.05785766700515524
LOSS is 0.06362351757086193
LOSS is 0.057604831946082416
LOSS is 0.05501035312462288
LOSS is 0.06584243967236642
LOSS is 0.054774329002054103
LOSS is 0.05443494983444301
Epoch: [1][500/2183]	Per Sample Total Time 4.56963	Per Sample Data Time 4.55257	Per Sample DNN Time 0.01706	Train Loss 0.1006	
LOSS is 0.06076329139332908
LOSS is 0.06470373216007526
LOSS is 0.05284548186095587
LOSS is 0.05740091472223867
LOSS is 0.05631806231782927
LOSS is 0.05559606482546466
LOSS is 0.05767338125306803
LOSS is 0.06468452012864873
LOSS is 0.059405193525987374
LOSS is 0.06257563881925307
LOSS is 0.06232311471559418
LOSS is 0.0526312903718402
LOSS is 0.05877247646412192
LOSS is 0.06019757071315932
LOSS is 0.0577920183702372
LOSS is 0.059715076094532075
LOSS is 0.05496138047155304
LOSS is 0.05923701965366491
LOSS is 0.058989081937470475
LOSS is 0.057286710833820205
LOSS is 0.05845175329207754
LOSS is 0.059230779951321894
LOSS is 0.06047122879574696
LOSS is 0.05312069657995987
LOSS is 0.054434665474109356
LOSS is 0.051355394912534394
LOSS is 0.054179032208630816
LOSS is 0.05122907989289767
LOSS is 0.0542180256555245
LOSS is 0.05842380824246599
LOSS is 0.05624450324471885
LOSS is 0.05558849217544776
LOSS is 0.06133314759974989
LOSS is 0.06041287972766441
LOSS is 0.061800528812649046
LOSS is 0.05524516450648662
LOSS is 0.0543130513400926
LOSS is 0.05369931007813042
LOSS is 0.056171775468780355
LOSS is 0.05305326239652156
LOSS is 0.059437473963480446
LOSS is 0.05395075166365132
LOSS is 0.05740032096859068
LOSS is 0.05508949764848998
LOSS is 0.05195819854930354
LOSS is 0.05814615312944322
LOSS is 0.054900626279219676
LOSS is 0.05610415265002909
LOSS is 0.05430310549156275
LOSS is 0.057853499122041595
LOSS is 0.0629695156418408
LOSS is 0.04878128432785161
LOSS is 0.06049385437664266
LOSS is 0.053983722420913795
LOSS is 0.048430128601612527
LOSS is 0.0589944444881985
LOSS is 0.051563265579364574
LOSS is 0.05482463114118825
LOSS is 0.053364979016672205
LOSS is 0.05184106332017109
LOSS is 0.0502239282867716
LOSS is 0.04718400668993127
LOSS is 0.055318084419899
LOSS is 0.05512784726743121
LOSS is 0.05531879017362371
LOSS is 0.0561163133297426
LOSS is 0.05491663827075779
LOSS is 0.05074607329501305
LOSS is 0.04907872583387265
LOSS is 0.05663654678423579
LOSS is 0.04970742539619096
LOSS is 0.05566532154761566
LOSS is 0.053055167719333746
LOSS is 0.047326230770171
LOSS is 0.05356133646506351
LOSS is 0.0552436618617503
LOSS is 0.059445245880245544
LOSS is 0.055871149813562324
LOSS is 0.058618551171578784
LOSS is 0.05492930047175226
LOSS is 0.05200770348698522
LOSS is 0.04970470000407659
LOSS is 0.05982364739873447
LOSS is 0.053776636616676116
LOSS is 0.05533590054993208
LOSS is 0.052481195298799625
LOSS is 0.05918991525929111
LOSS is 0.057599701117142105
LOSS is 0.05246980261659095
LOSS is 0.0619981161163499
LOSS is 0.049585830941020204
LOSS is 0.05993021784350276
LOSS is 0.05221456228736012
LOSS is 0.054983467692509294
LOSS is 0.04759691604723533
LOSS is 0.05107045180707549
LOSS is 0.05405916166724638
LOSS is 0.04548772347528333
LOSS is 0.06107266023793879
LOSS is 0.05468381480469058
Epoch: [1][600/2183]	Per Sample Total Time 5.40017	Per Sample Data Time 5.38320	Per Sample DNN Time 0.01697	Train Loss 0.0931	
LOSS is 0.04784830944108156
LOSS is 0.05777895268964737
LOSS is 0.051509126053230526
LOSS is 0.051851909756660465
LOSS is 0.05068808443262242
LOSS is 0.05742411749747892
LOSS is 0.060194132935915454
LOSS is 0.05301110486170122
LOSS is 0.05662801862054039
LOSS is 0.05225434716887928
LOSS is 0.049205064258033726
LOSS is 0.05078349802817684
LOSS is 0.06198156641543998
LOSS is 0.05755385589436628
LOSS is 0.05066702956372562
LOSS is 0.05842678469702757
LOSS is 0.05649027658277191
LOSS is 0.05318484011746478
LOSS is 0.05006905370760554
LOSS is 0.054863351228802155
LOSS is 0.054476909544512946
LOSS is 0.04988506227033213
LOSS is 0.05001427613683821
LOSS is 0.06477887591347099
LOSS is 0.056064711624640044
LOSS is 0.056398271314295326
LOSS is 0.06047718347089055
LOSS is 0.05101155521910793
LOSS is 0.05040889988187701
LOSS is 0.04974233854930693
LOSS is 0.04832612745405641
LOSS is 0.05126343678372602
LOSS is 0.05005077146342956
LOSS is 0.04859195136969599
LOSS is 0.055114487172492475
LOSS is 0.04969810478660899
LOSS is 0.05403966738493182
LOSS is 0.0417632120277267
LOSS is 0.05659992998912155
LOSS is 0.0497687892089986
LOSS is 0.05130264715563196
LOSS is 0.0513862750104939
LOSS is 0.04687971357838251
LOSS is 0.05229768010845874
LOSS is 0.05571287642970371
LOSS is 0.05364464736970452
LOSS is 0.048985245406511245
LOSS is 0.045275933971279304
LOSS is 0.04338320995603378
LOSS is 0.05046352494022965
LOSS is 0.05229204521475671
LOSS is 0.05112113889714238
LOSS is 0.05539524596494933
LOSS is 0.04854922363786803
LOSS is 0.0573953636388372
LOSS is 0.04615317104267888
LOSS is 0.05864206708249792
LOSS is 0.052596718929708006
LOSS is 0.04480117557764364
LOSS is 0.04829930797665535
LOSS is 0.047716335257282486
LOSS is 0.05146318439782287
LOSS is 0.044485774118026414
LOSS is 0.05171886537224055
LOSS is 0.05173421714881746
LOSS is 0.051025377856761533
LOSS is 0.050295267799519934
LOSS is 0.04745334703824483
LOSS is 0.04703118500299752
LOSS is 0.0485325265058782
LOSS is 0.04813722226341876
LOSS is 0.04428447789531977
LOSS is 0.051670314656415334
LOSS is 0.0497494597604964
LOSS is 0.04764329855058653
LOSS is 0.05318695586523973
LOSS is 0.0400933215963111
LOSS is 0.05154162105774352
LOSS is 0.055962705793596496
LOSS is 0.05449317543418147
LOSS is 0.049362540385530645
LOSS is 0.04292031672142912
LOSS is 0.04182359050396675
LOSS is 0.04937624760088511
LOSS is 0.048845774126239125
LOSS is 0.04765355034013434
LOSS is 0.04891807536031896
LOSS is 0.04998972133190061
LOSS is 0.04785244101115192
LOSS is 0.05477831336689027
LOSS is 0.045926361589614925
LOSS is 0.046434147089409336
LOSS is 0.047152272196932854
LOSS is 0.04661876289445597
LOSS is 0.04837467819238858
LOSS is 0.0425706453450645
LOSS is 0.0477412175933326
LOSS is 0.04328373940700355
LOSS is 0.054481369965166476
LOSS is 0.04753111802778828
Epoch: [1][700/2183]	Per Sample Total Time 6.22854	Per Sample Data Time 6.21164	Per Sample DNN Time 0.01690	Train Loss 0.0871	
LOSS is 0.0507611963802871
LOSS is 0.04189807217063693
LOSS is 0.0564918814128032
LOSS is 0.04669949485900968
LOSS is 0.05141842562918707
LOSS is 0.04926347454117301
LOSS is 0.05116571839685397
LOSS is 0.04546816643928954
LOSS is 0.05500549326903032
LOSS is 0.046692547291556065
LOSS is 0.04717309089455133
LOSS is 0.04051559380701899
LOSS is 0.0505330295439732
LOSS is 0.04720725439139642
LOSS is 0.052103818224083324
LOSS is 0.0507152739984061
LOSS is 0.05202463217525898
LOSS is 0.04451686630083714
LOSS is 0.04149637435262169
LOSS is 0.041218159703615434
LOSS is 0.048237922451905134
LOSS is 0.04855750550341327
LOSS is 0.050375335956147564
LOSS is 0.05193919762697382
LOSS is 0.049089136509379995
LOSS is 0.046352475302022265
LOSS is 0.060205928992945704
LOSS is 0.04766332571491755
LOSS is 0.053398692733317155
LOSS is 0.048942723231739364
LOSS is 0.05036226707918104
LOSS is 0.04372710620130723
LOSS is 0.04661762074489768
LOSS is 0.051648828777057745
LOSS is 0.043396747024671646
LOSS is 0.04597019147564425
LOSS is 0.04906932329370951
LOSS is 0.0493643627523367
LOSS is 0.04753302431432531
LOSS is 0.04287038201815449
LOSS is 0.04719192485169818
LOSS is 0.039413724652064656
LOSS is 0.03883845565258526
LOSS is 0.04780218167055864
LOSS is 0.04588464222014106
LOSS is 0.04662494254657456
LOSS is 0.043697447121764224
LOSS is 0.048566135864045165
LOSS is 0.046861102814048844
LOSS is 0.05851646128925495
LOSS is 0.04109447397311063
LOSS is 0.04468087961218165
LOSS is 0.04360062652868995
LOSS is 0.04280050261159583
LOSS is 0.05195798615236224
LOSS is 0.052497676333684164
LOSS is 0.044817036393603
LOSS is 0.038582524410837024
LOSS is 0.047708981863106605
LOSS is 0.0478502270631725
LOSS is 0.05192636791393549
LOSS is 0.04772978482913459
LOSS is 0.04789145835772312
LOSS is 0.04640192465293998
LOSS is 0.04152329765202012
LOSS is 0.051192235011646214
LOSS is 0.04708100637032961
LOSS is 0.050314271029977445
LOSS is 0.0453540521520578
LOSS is 0.04164598866055409
LOSS is 0.04846700666365602
LOSS is 0.04611002798075788
LOSS is 0.040407359267507374
LOSS is 0.043984449908214936
LOSS is 0.051214780711937545
LOSS is 0.04940806738062141
LOSS is 0.04282542068520949
LOSS is 0.048672221353820835
LOSS is 0.04460293629614171
LOSS is 0.047942674086273965
LOSS is 0.04330230644768259
LOSS is 0.04293016271394057
LOSS is 0.049101923831913155
LOSS is 0.044048175946421304
LOSS is 0.04510587162493418
LOSS is 0.04463627217686735
LOSS is 0.05095793162200911
LOSS is 0.04724132432282204
LOSS is 0.04319720616641765
LOSS is 0.051426359447262565
LOSS is 0.052173717412515544
LOSS is 0.043996551746677146
LOSS is 0.04306214531049288
LOSS is 0.04786825876993438
LOSS is 0.04748602192848921
LOSS is 0.0449384566078273
LOSS is 0.04186956187457933
LOSS is 0.05299270871192373
LOSS is 0.054032770110352436
LOSS is 0.045882273373814925
Epoch: [1][800/2183]	Per Sample Total Time 7.05603	Per Sample Data Time 7.03919	Per Sample DNN Time 0.01684	Train Loss 0.0821	
LOSS is 0.041419432900050505
LOSS is 0.04396499815047719
LOSS is 0.04325421030283906
LOSS is 0.04155060411217468
LOSS is 0.04488501800951781
LOSS is 0.043954127880473
LOSS is 0.04848395785627266
LOSS is 0.04459264645915634
LOSS is 0.04500638549875779
LOSS is 0.04192921621278705
LOSS is 0.046603287168351624
LOSS is 0.03537457475322299
LOSS is 0.04666627294092905
LOSS is 0.05378794224224597
LOSS is 0.04994843003854234
LOSS is 0.04212316570202044
LOSS is 0.04700956197843577
LOSS is 0.047545528620054636
LOSS is 0.044142228662967686
LOSS is 0.0382359127982636
LOSS is 0.04508284755982459
LOSS is 0.046655651516339276
LOSS is 0.03914939534869821
LOSS is 0.04776509229365426
LOSS is 0.043916172972725084
LOSS is 0.04204843353353984
LOSS is 0.04324006184043053
LOSS is 0.0525120895333627
LOSS is 0.03978068220070175
LOSS is 0.04800961719845267
LOSS is 0.03970507186614365
LOSS is 0.03848119525665728
LOSS is 0.04006109725004838
LOSS is 0.04401305782123624
LOSS is 0.045531488849082966
LOSS is 0.04142943587668318
LOSS is 0.04054622582450975
LOSS is 0.0427814058358005
LOSS is 0.035593170064482066
LOSS is 0.04690321190913286
LOSS is 0.04681622455459244
LOSS is 0.048277252722570364
LOSS is 0.05017434043450825
LOSS is 0.038387140640794924
LOSS is 0.04338660677545704
LOSS is 0.04618604583869456
LOSS is 0.0404303484299453
LOSS is 0.04593838356221871
LOSS is 0.04223538857807095
LOSS is 0.04515815916888338
LOSS is 0.042277277561661324
LOSS is 0.047596480282760846
LOSS is 0.04421897932392312
LOSS is 0.03896059325954412
LOSS is 0.05181492735932504
LOSS is 0.04991702204240331
LOSS is 0.04657874722577011
LOSS is 0.04356603582418757
LOSS is 0.03794359137983217
LOSS is 0.043651904852401155
LOSS is 0.042047019328262346
LOSS is 0.04574673479859485
LOSS is 0.04124541895648387
LOSS is 0.04216077185206814
LOSS is 0.04578990772318017
LOSS is 0.03764627163570064
LOSS is 0.051922788347195215
LOSS is 0.04635257661653062
LOSS is 0.0414773336048044
LOSS is 0.0405802536738338
LOSS is 0.04982403857158109
LOSS is 0.03823168623717114
LOSS is 0.038209156510032094
LOSS is 0.03914519083870498
LOSS is 0.047894632722503355
LOSS is 0.04527797052229289
LOSS is 0.047641651243902745
LOSS is 0.041123242133859705
LOSS is 0.043861338634063336
LOSS is 0.040654374496856084
LOSS is 0.04041009626729647
LOSS is 0.03930810467689298
LOSS is 0.046485180214513096
LOSS is 0.03675819263609204
LOSS is 0.04401934018688432
LOSS is 0.044987823357126526
LOSS is 0.0392148595204344
LOSS is 0.04055593329559391
LOSS is 0.04585152794209232
LOSS is 0.039268175897110874
LOSS is 0.04455416419844066
LOSS is 0.045618375962658324
LOSS is 0.042044236104896604
LOSS is 0.03286779391753953
LOSS is 0.050807473028350314
LOSS is 0.038509067121776756
LOSS is 0.03823774721221222
LOSS is 0.039278369797975755
LOSS is 0.043598058849165684
LOSS is 0.04288192436215468
Epoch: [1][900/2183]	Per Sample Total Time 7.88250	Per Sample Data Time 7.86571	Per Sample DNN Time 0.01679	Train Loss 0.0778	
LOSS is 0.03867056395693605
LOSS is 0.04320904348918703
LOSS is 0.04070293712361794
LOSS is 0.047555725860293024
LOSS is 0.0503571845649276
LOSS is 0.0367518278118223
LOSS is 0.04915900612822346
LOSS is 0.03985568980472939
LOSS is 0.04387042553086455
LOSS is 0.043593684900843074
LOSS is 0.051944525751653904
LOSS is 0.043560747972903
LOSS is 0.04496782096384171
LOSS is 0.03830303288317131
LOSS is 0.04115072788573647
LOSS is 0.04008290694648167
LOSS is 0.04137824913321917
LOSS is 0.03979756259922094
LOSS is 0.04321457453489226
LOSS is 0.040428409017719484
LOSS is 0.034606618688364205
LOSS is 0.03927900699762783
LOSS is 0.045388933309586724
LOSS is 0.0386190400699464
LOSS is 0.03186989436314131
LOSS is 0.03614415608831526
LOSS is 0.04294938361058788
LOSS is 0.041304486606871554
LOSS is 0.04188959621154936
LOSS is 0.04721647649460162
LOSS is 0.038145517684364076
LOSS is 0.046383208381400136
LOSS is 0.045197257643837176
LOSS is 0.04933819044924652
LOSS is 0.03955349698662758
LOSS is 0.04083192381251138
LOSS is 0.04184490266973929
LOSS is 0.04048105975923439
LOSS is 0.03746351348808579
LOSS is 0.04916032995960753
LOSS is 0.03589052406023257
LOSS is 0.04374440158979269
LOSS is 0.04238434520491865
LOSS is 0.048673419295810164
LOSS is 0.04225660221350457
LOSS is 0.04073461540412003
LOSS is 0.04750151056485872
LOSS is 0.044207926758002336
LOSS is 0.03220644049472564
LOSS is 0.037841639026980074
LOSS is 0.04115945289609954
LOSS is 0.03976372635816612
LOSS is 0.04433335780156389
LOSS is 0.05320013725693571
LOSS is 0.04052650899112147
LOSS is 0.03992177533451468
LOSS is 0.04401808772808485
LOSS is 0.03938893673407923
LOSS is 0.04264922561609031
LOSS is 0.041241525821484784
LOSS is 0.04438001459260704
LOSS is 0.03526315721004115
LOSS is 0.042120507665871026
LOSS is 0.04275645299814642
LOSS is 0.04870100294298027
LOSS is 0.03808680951799034
LOSS is 0.043549240408950335
LOSS is 0.04498762690607692
LOSS is 0.05199383593465124
LOSS is 0.04773088768444723
LOSS is 0.042014489026041706
LOSS is 0.04541319767187815
LOSS is 0.039490555818968764
LOSS is 0.03705229347804562
LOSS is 0.038521016060064237
LOSS is 0.044606426916531446
LOSS is 0.04821278920901629
LOSS is 0.04451907104395408
LOSS is 0.038177168985809355
LOSS is 0.043178566244314424
LOSS is 0.036571637921733784
LOSS is 0.04021256289881422
LOSS is 0.04669069915194996
LOSS is 0.046700886611749114
LOSS is 0.04393503368783665
LOSS is 0.03110146168910433
LOSS is 0.043177644325963535
LOSS is 0.042996885295918524
LOSS is 0.04802215960536463
LOSS is 0.03909024178690743
LOSS is 0.032431234890342844
LOSS is 0.04226524885268494
LOSS is 0.03898214763656142
LOSS is 0.04703786860433563
LOSS is 0.039305866826616694
LOSS is 0.034084462644823364
LOSS is 0.039739508491669163
LOSS is 0.03911868748633424
LOSS is 0.04700373585151586
LOSS is 0.041737780202044335
Epoch: [1][1000/2183]	Per Sample Total Time 8.70812	Per Sample Data Time 8.69137	Per Sample DNN Time 0.01675	Train Loss 0.0743	
LOSS is 0.037214805913681634
LOSS is 0.032251681556129674
LOSS is 0.047466900423072125
LOSS is 0.04498730325237072
LOSS is 0.04223220435514426
LOSS is 0.03971397419266093
LOSS is 0.05154524865948285
LOSS is 0.04018471285953031
LOSS is 0.04631882121320814
LOSS is 0.04172899283255296
LOSS is 0.0418284941148401
LOSS is 0.04581326241246037
LOSS is 0.035383008806966244
LOSS is 0.04630189299583436
LOSS is 0.04167809693346499
LOSS is 0.0364236787185655
LOSS is 0.043722880386534
LOSS is 0.03848685939544036
LOSS is 0.040404258256797526
LOSS is 0.03963787573855371
LOSS is 0.03743464287215223
LOSS is 0.036067968329104284
LOSS is 0.03559319847632045
LOSS is 0.04286668149676795
LOSS is 0.041476235493319115
LOSS is 0.03737519602485312
LOSS is 0.05007364822347882
LOSS is 0.03867150658819204
LOSS is 0.04147333542544705
LOSS is 0.04733868581660015
LOSS is 0.043828153270005714
LOSS is 0.04492568979641268
LOSS is 0.03817638535799536
LOSS is 0.043190315511698524
LOSS is 0.03797111954064652
LOSS is 0.04354362590054128
LOSS is 0.03914012916822685
LOSS is 0.0459517494715207
LOSS is 0.04254069901347975
LOSS is 0.03555011251854012
LOSS is 0.04569777563097887
LOSS is 0.03997163453440104
LOSS is 0.0328661594541821
LOSS is 0.03280776896261765
LOSS is 0.03941690440493403
LOSS is 0.05081570749374805
LOSS is 0.04246944582613651
LOSS is 0.04949765305956438
LOSS is 0.04017841719585704
LOSS is 0.04540379710364505
LOSS is 0.039117653050537535
LOSS is 0.042041435712211146
LOSS is 0.04374127685577454
LOSS is 0.038194802991056355
LOSS is 0.037223933791683525
LOSS is 0.03664966346948253
LOSS is 0.03653931857639691
LOSS is 0.035141575535041436
LOSS is 0.035313376691095376
LOSS is 0.04318709920800757
LOSS is 0.03373516238343048
LOSS is 0.033970707991490295
LOSS is 0.03200197822695676
LOSS is 0.03883780833488951
LOSS is 0.035970463902437286
LOSS is 0.035846180897691135
LOSS is 0.04296309666096931
LOSS is 0.04025159490838026
LOSS is 0.039030629324794675
LOSS is 0.0351413835650601
LOSS is 0.03370563248211208
LOSS is 0.04641879667833564
LOSS is 0.04890685409482103
LOSS is 0.0356869717357525
LOSS is 0.035586635839038844
LOSS is 0.040147146671418645
LOSS is 0.02992497317560871
LOSS is 0.03701521846203832
LOSS is 0.03862311822808503
LOSS is 0.034246971334602375
LOSS is 0.037296760519578434
LOSS is 0.0387919842482855
LOSS is 0.03596818031578247
LOSS is 0.038158154784856985
LOSS is 0.047434622919342175
LOSS is 0.03944340067682788
LOSS is 0.04825149305048399
LOSS is 0.034251077549512655
LOSS is 0.03884738110122271
LOSS is 0.04012710250729773
LOSS is 0.03929882568074391
LOSS is 0.04158810415392509
LOSS is 0.03968548325297889
LOSS is 0.041553985196514986
LOSS is 0.038832541400624906
LOSS is 0.029972630449240874
LOSS is 0.03732149817437554
LOSS is 0.03923489493356708
LOSS is 0.03592425254872069
LOSS is 0.04218448721376869
Epoch: [1][1100/2183]	Per Sample Total Time 9.53293	Per Sample Data Time 9.51621	Per Sample DNN Time 0.01672	Train Loss 0.0711	
LOSS is 0.03647318960739843
LOSS is 0.03522687081659873
LOSS is 0.040987745545377645
LOSS is 0.03837453245059199
LOSS is 0.040878503872275665
LOSS is 0.03871284071525831
LOSS is 0.03859305962992949
LOSS is 0.034616334202971
LOSS is 0.037657170749153014
LOSS is 0.03823024220987767
LOSS is 0.040628896513468744
LOSS is 0.03501541025199307
LOSS is 0.04230610017756893
LOSS is 0.047375704906395796
LOSS is 0.034927684645323705
LOSS is 0.03361089026584523
LOSS is 0.03244064507225024
LOSS is 0.04074144968365242
LOSS is 0.03996172173690866
LOSS is 0.03241791841205365
LOSS is 0.0454379299596864
LOSS is 0.034875979169543526
LOSS is 0.03891049555929688
LOSS is 0.029186073999638514
LOSS is 0.03379206796671497
LOSS is 0.04201576646184549
LOSS is 0.035337338409056736
LOSS is 0.04111844799030223
LOSS is 0.02965988760843175
LOSS is 0.04080452026522835
LOSS is 0.04107817755245682
LOSS is 0.04061734437166403
LOSS is 0.034269569113869996
LOSS is 0.04256454684364144
LOSS is 0.03397510729574909
LOSS is 0.04124151146927034
LOSS is 0.039584315550115816
LOSS is 0.03733052568151228
LOSS is 0.04470836412083979
LOSS is 0.04120318372743593
LOSS is 0.0351548734306319
LOSS is 0.031223336306672234
LOSS is 0.033228883227081195
LOSS is 0.03663029634141519
LOSS is 0.040324317398869124
LOSS is 0.03924171867600914
LOSS is 0.032199217242790235
LOSS is 0.03264316821606675
LOSS is 0.032381958613405
LOSS is 0.044215011092504336
LOSS is 0.03674604822823312
LOSS is 0.03864431170882502
LOSS is 0.03832009080166851
LOSS is 0.04778869116969873
LOSS is 0.03755461785010994
LOSS is 0.04799967164144619
LOSS is 0.03964822408966332
LOSS is 0.04541024873430918
LOSS is 0.03951691168476828
LOSS is 0.034118542110857866
LOSS is 0.03710560349893058
LOSS is 0.034678000125374336
LOSS is 0.03888231188136464
LOSS is 0.03423480925276332
LOSS is 0.036695924865780404
LOSS is 0.04016678578559853
LOSS is 0.04013903483835748
LOSS is 0.04340245191376502
LOSS is 0.04013473473091533
LOSS is 0.04269871134708714
LOSS is 0.04381764116988052
LOSS is 0.03907852310716407
LOSS is 0.039066349573404296
LOSS is 0.033506485404989995
LOSS is 0.03652954278232452
LOSS is 0.037349090188120805
LOSS is 0.042116195861308374
LOSS is 0.03971574575756677
LOSS is 0.03489683846618088
LOSS is 0.03296231332603687
LOSS is 0.03499257873467286
LOSS is 0.039267452528874855
LOSS is 0.036199322483783665
LOSS is 0.03448577271483373
LOSS is 0.03808707752497867
LOSS is 0.03472151151392609
LOSS is 0.0367632328918747
LOSS is 0.044829923207483566
LOSS is 0.038594561000403094
LOSS is 0.03793151732131567
LOSS is 0.03367560628219508
LOSS is 0.03989045673845491
LOSS is 0.02707301740796538
LOSS is 0.04534347901174139
LOSS is 0.03167472341175502
LOSS is 0.03392361106477135
LOSS is 0.033978278833189206
LOSS is 0.03870362960136845
LOSS is 0.03265860372058039
LOSS is 0.029052270782801013
Epoch: [1][1200/2183]	Per Sample Total Time 10.35776	Per Sample Data Time 10.34106	Per Sample DNN Time 0.01670	Train Loss 0.0684	
LOSS is 0.039924795603243794
LOSS is 0.03938195718326218
LOSS is 0.04423982272613406
LOSS is 0.04305242224222942
LOSS is 0.033068376111914405
LOSS is 0.04687251921316299
LOSS is 0.030957437704782934
LOSS is 0.03814997491101774
LOSS is 0.031505540746002224
LOSS is 0.03662619522336172
LOSS is 0.03748604328749935
LOSS is 0.036661505583906545
LOSS is 0.03958619728504952
LOSS is 0.029399288867910704
LOSS is 0.034983663553333225
LOSS is 0.02699335897981655
LOSS is 0.03522489404548348
LOSS is 0.03256142230849946
LOSS is 0.03941719302577743
LOSS is 0.03761232246256744
LOSS is 0.03322033361245606
LOSS is 0.0387802806373414
LOSS is 0.036641134661282805
LOSS is 0.035398948700264256
LOSS is 0.041314542835607426
LOSS is 0.034261954666726525
LOSS is 0.03669887099060967
LOSS is 0.03580895323611913
LOSS is 0.03189671769597529
LOSS is 0.033996622387639945
LOSS is 0.045467550850395735
LOSS is 0.031079820830200337
LOSS is 0.03546372831255819
LOSS is 0.035531794103735595
LOSS is 0.03279233700023421
LOSS is 0.03164864334462133
LOSS is 0.04188599739713633
LOSS is 0.03894273492279657
LOSS is 0.034734984346384105
LOSS is 0.03722464914288139
LOSS is 0.048705520209575
LOSS is 0.04009010871799547
LOSS is 0.031482466444237314
LOSS is 0.03219064431085523
LOSS is 0.03654410334255469
LOSS is 0.030676151912436278
LOSS is 0.03180072384284965
LOSS is 0.03862871967726581
LOSS is 0.03372719923150726
LOSS is 0.034500323056660516
LOSS is 0.03722645893091491
LOSS is 0.04007937368092826
LOSS is 0.030344923966282904
LOSS is 0.0331621635396732
LOSS is 0.03716301526030293
LOSS is 0.04216911317620543
LOSS is 0.03341318604458744
LOSS is 0.04066954125194267
LOSS is 0.0374136251405677
LOSS is 0.03654740004897273
LOSS is 0.03638318739603468
LOSS is 0.04189700893572687
LOSS is 0.040631180671480256
LOSS is 0.04182399210229051
LOSS is 0.03113190231350018
LOSS is 0.03374668654452156
LOSS is 0.04137677942339603
LOSS is 0.035749399884031544
LOSS is 0.04068928462273713
LOSS is 0.030186080415005565
LOSS is 0.035786838744097625
LOSS is 0.029521754194720418
LOSS is 0.03563130954629742
LOSS is 0.028962671641396206
LOSS is 0.03365437332429186
LOSS is 0.041521336452957865
LOSS is 0.034752799128812815
LOSS is 0.05114666556633893
LOSS is 0.03308178179189175
LOSS is 0.037071032829796124
LOSS is 0.034418978569544074
LOSS is 0.034909781586029566
LOSS is 0.0283259519362764
LOSS is 0.035238347849032535
LOSS is 0.037752867499754456
LOSS is 0.03653880222108759
LOSS is 0.02999280953241396
LOSS is 0.03599193716358665
LOSS is 0.036083580468402945
LOSS is 0.027740155066518734
LOSS is 0.03519369503536533
LOSS is 0.03747201380184076
LOSS is 0.03364764840710753
LOSS is 0.03696896676202111
LOSS is 0.04519422109524991
LOSS is 0.031532280602259564
LOSS is 0.03516307556346874
LOSS is 0.03653998442348287
LOSS is 0.03388586731879817
LOSS is 0.03879537683019104
Epoch: [1][1300/2183]	Per Sample Total Time 11.18283	Per Sample Data Time 11.16614	Per Sample DNN Time 0.01669	Train Loss 0.0659	
LOSS is 0.032249207046891874
LOSS is 0.04645807478305263
LOSS is 0.03898754174287509
LOSS is 0.031937486682339415
LOSS is 0.03981362260470633
LOSS is 0.029726535801164574
LOSS is 0.0379349911424409
LOSS is 0.02971794205581925
LOSS is 0.03583816314120971
LOSS is 0.03662545786442934
LOSS is 0.038244238768529615
LOSS is 0.03695401506760391
LOSS is 0.036960382959659914
LOSS is 0.0363354602502659
LOSS is 0.03157853161004217
LOSS is 0.03680540206230944
LOSS is 0.0319728260752648
LOSS is 0.025724054339516442
LOSS is 0.03127642056143183
LOSS is 0.036776657513401007
LOSS is 0.039159236824731734
LOSS is 0.04033388046445907
LOSS is 0.039926684182137254
LOSS is 0.03687424117602253
LOSS is 0.03931599657322901
LOSS is 0.030070916195982136
LOSS is 0.0475323272192812
LOSS is 0.028482862391441207
LOSS is 0.04291128849608261
LOSS is 0.034253146887349435
LOSS is 0.03863081571296789
LOSS is 0.03154722137017719
LOSS is 0.033074574881902664
LOSS is 0.03436878993937474
LOSS is 0.027979785258988463
LOSS is 0.03277154191169151
LOSS is 0.04189191761572147
LOSS is 0.037239375328499594
LOSS is 0.039968356962199324
LOSS is 0.030489606740108383
LOSS is 0.03478424514401316
LOSS is 0.03674744966536916
LOSS is 0.036268313868495175
LOSS is 0.03264823981022346
LOSS is 0.03865472481295001
LOSS is 0.03756964159207807
LOSS is 0.03833853633890006
LOSS is 0.04024667203865344
LOSS is 0.03373755920421294
LOSS is 0.031185760793596275
LOSS is 0.03159143082094185
LOSS is 0.03197941563790664
LOSS is 0.025035526767945463
LOSS is 0.029087762286702252
LOSS is 0.02873517153299569
LOSS is 0.03814341032499215
LOSS is 0.027350706546276343
LOSS is 0.03292771161621204
LOSS is 0.03529225757922783
LOSS is 0.04069307990294571
LOSS is 0.03637571855913848
LOSS is 0.031455682000717695
LOSS is 0.03068006593734026
LOSS is 0.024542281532194468
LOSS is 0.026267130006842006
LOSS is 0.03736002946966134
LOSS is 0.03284611093816542
LOSS is 0.03798082008686227
LOSS is 0.02917410434757282
LOSS is 0.02702968648688208
LOSS is 0.03826542787185948
LOSS is 0.032934429637874324
LOSS is 0.0319578958284789
LOSS is 0.03801842648545668
LOSS is 0.031299073348685244
LOSS is 0.039952136356635796
LOSS is 0.03418621424818412
LOSS is 0.03314220613179108
LOSS is 0.03393820127632353
LOSS is 0.030782758242373046
LOSS is 0.031711403097433505
LOSS is 0.03475261207475948
LOSS is 0.032978790879181666
LOSS is 0.03043231169547653
LOSS is 0.02676120609690164
LOSS is 0.029715108713135124
LOSS is 0.033913834382450055
LOSS is 0.02978262050392611
LOSS is 0.037269385455341156
LOSS is 0.04101279982889537
LOSS is 0.039833075685528456
LOSS is 0.033737168791509856
LOSS is 0.03884071463306706
LOSS is 0.03279339049147287
LOSS is 0.0305737928217665
LOSS is 0.027144217495563984
LOSS is 0.03562617686703258
LOSS is 0.03411374631827736
LOSS is 0.028564231816465814
LOSS is 0.03078348931943765
Epoch: [1][1400/2183]	Per Sample Total Time 12.00807	Per Sample Data Time 11.99140	Per Sample DNN Time 0.01667	Train Loss 0.0636	
LOSS is 0.03791586210087795
LOSS is 0.034182132123872486
LOSS is 0.03339189603119545
LOSS is 0.03133518978448895
LOSS is 0.04223102498440615
LOSS is 0.032360672972038936
LOSS is 0.03815020157058219
LOSS is 0.03692573133793
LOSS is 0.045757563515314056
LOSS is 0.034053361255743465
LOSS is 0.029651382778683913
LOSS is 0.03400560443978369
LOSS is 0.03399323915897791
LOSS is 0.02917338302252271
LOSS is 0.028912724561135597
LOSS is 0.03530542079766747
LOSS is 0.03945375310155214
LOSS is 0.03587923960197562
LOSS is 0.02839729788063172
LOSS is 0.030375450579061485
LOSS is 0.028954588872438763
LOSS is 0.03388134802128965
LOSS is 0.03850225196161773
LOSS is 0.0390575400988746
LOSS is 0.03981382323298022
LOSS is 0.029739121489546962
LOSS is 0.049221060407289775
LOSS is 0.03794345390997478
LOSS is 0.03178771826535619
LOSS is 0.03711571741315614
LOSS is 0.03367121170498043
LOSS is 0.03535898508397319
LOSS is 0.030603401436868204
LOSS is 0.03466844588750974
LOSS is 0.042537428534560606
LOSS is 0.03487354043701392
LOSS is 0.043622347430743204
LOSS is 0.03989290690718918
LOSS is 0.025807677053720304
LOSS is 0.031295314390154094
LOSS is 0.03109902594306429
LOSS is 0.03729652971296067
LOSS is 0.029987598179917165
LOSS is 0.0327675374315974
LOSS is 0.03879885833254472
LOSS is 0.03957167347058809
LOSS is 0.030347536793027152
LOSS is 0.035275855584865595
LOSS is 0.033099871114924705
LOSS is 0.03357010006172156
LOSS is 0.023027974814370588
LOSS is 0.03200505035953635
LOSS is 0.031904688721018225
LOSS is 0.033596919204622586
LOSS is 0.034426964868859315
LOSS is 0.026123549599336306
LOSS is 0.03133770826466692
LOSS is 0.029245311844036528
LOSS is 0.029350153313037787
LOSS is 0.03004058158049399
LOSS is 0.03192596077914156
LOSS is 0.027177201254235117
LOSS is 0.03672454161574327
LOSS is 0.029151482965583758
LOSS is 0.026923383834315855
LOSS is 0.023979802569228926
LOSS is 0.02787464094765407
LOSS is 0.03562599789239661
LOSS is 0.03332709248774336
LOSS is 0.0378431915838155
LOSS is 0.026896153901131283
LOSS is 0.030639692229160576
LOSS is 0.033052642419934275
LOSS is 0.03854187200088442
LOSS is 0.026122403804135202
LOSS is 0.03548517137057691
LOSS is 0.033510047609161125
LOSS is 0.027112115981726675
LOSS is 0.03604772690079699
LOSS is 0.037389100550693305
LOSS is 0.03341827183714486
LOSS is 0.033861129267267344
LOSS is 0.04209712911693108
LOSS is 0.023702404301584466
LOSS is 0.028755484981035504
LOSS is 0.036373617418430516
LOSS is 0.03183420540847389
LOSS is 0.024430704691718954
LOSS is 0.030028314574641023
LOSS is 0.035998341249263224
LOSS is 0.036266016565496106
LOSS is 0.02777294978872912
LOSS is 0.02725395650365196
LOSS is 0.031161417499824894
LOSS is 0.03523410717554119
LOSS is 0.027265308209025535
LOSS is 0.03438270591049029
LOSS is 0.0342134148243349
LOSS is 0.0283691878032793
LOSS is 0.033197667156734195
Epoch: [1][1500/2183]	Per Sample Total Time 12.83330	Per Sample Data Time 12.81665	Per Sample DNN Time 0.01666	Train Loss 0.0616	
LOSS is 0.035189933311679245
LOSS is 0.021235851078139
LOSS is 0.038058979403867856
LOSS is 0.04220151270196463
LOSS is 0.03253717209292518
LOSS is 0.03440537538563755
LOSS is 0.028678854294412307
LOSS is 0.033458428337132015
LOSS is 0.04316185858527509
LOSS is 0.041375120536007066
LOSS is 0.034138819484263276
LOSS is 0.029357473930528313
LOSS is 0.02886699016070149
LOSS is 0.02652811989998251
LOSS is 0.03203800052123067
LOSS is 0.04255177010195136
LOSS is 0.032608475067681865
LOSS is 0.037072379854119694
LOSS is 0.0280562362887819
LOSS is 0.03145385620320061
LOSS is 0.029074517126088422
LOSS is 0.03896715565967801
LOSS is 0.031221869798561484
LOSS is 0.025164541891911845
LOSS is 0.039444123190187384
LOSS is 0.030441765174546165
LOSS is 0.03591970229725121
LOSS is 0.031734207471017725
LOSS is 0.03224964094134824
LOSS is 0.03185006640870901
LOSS is 0.02412706840244937
LOSS is 0.03422896780156104
LOSS is 0.032455249502066484
LOSS is 0.036971142521554916
LOSS is 0.03376598153680486
LOSS is 0.03775101797674628
LOSS is 0.031815765424689745
LOSS is 0.03306733388099626
LOSS is 0.03508871553708256
LOSS is 0.028918683714873626
LOSS is 0.027469251436608225
LOSS is 0.032770547569040596
LOSS is 0.030387989974018034
LOSS is 0.02930970891330314
LOSS is 0.03396129191668782
LOSS is 0.018774715084728087
LOSS is 0.02930962038890963
LOSS is 0.03179010196142675
LOSS is 0.03504441114331712
LOSS is 0.029275828640384135
LOSS is 0.023993916360632285
LOSS is 0.03503462986040783
LOSS is 0.03566211440377326
LOSS is 0.03514944707383014
LOSS is 0.024943866391986376
LOSS is 0.028066120535319594
LOSS is 0.02226490791833688
LOSS is 0.02309932031532905
LOSS is 0.030266125637620767
LOSS is 0.03706812549721993
LOSS is 0.02840080003535453
LOSS is 0.027284861123237837
LOSS is 0.03436457732745718
LOSS is 0.038216936298122166
LOSS is 0.03229834076410043
LOSS is 0.03170276688183852
LOSS is 0.029580597076079965
LOSS is 0.025218072267841007
LOSS is 0.03620302641521751
LOSS is 0.034739368501274535
LOSS is 0.03657519634672402
LOSS is 0.02731928973036702
LOSS is 0.026389436279132497
LOSS is 0.03706682786311528
LOSS is 0.03518922289949842
LOSS is 0.02285765269654803
LOSS is 0.03814101967810227
LOSS is 0.022525224605439386
LOSS is 0.032145133809923815
LOSS is 0.04970594938926903
LOSS is 0.03538578099711837
LOSS is 0.029409143975111268
LOSS is 0.04278379513542556
LOSS is 0.03116852315011784
LOSS is 0.03261542765244182
LOSS is 0.03218788924641558
LOSS is 0.03131930893126992
LOSS is 0.03540799271523914
LOSS is 0.03894614861996767
LOSS is 0.028729435295050888
LOSS is 0.03812552205922354
LOSS is 0.029153404477304627
LOSS is 0.025193306936125738
LOSS is 0.03505378386777011
LOSS is 0.025466503670419725
LOSS is 0.03165250021071794
LOSS is 0.03293242282641586
LOSS is 0.029749397226648096
LOSS is 0.03328455885954706
LOSS is 0.031242337780713572
Epoch: [1][1600/2183]	Per Sample Total Time 13.65845	Per Sample Data Time 13.64180	Per Sample DNN Time 0.01665	Train Loss 0.0598	
LOSS is 0.03479175079415048
LOSS is 0.024774534652678995
LOSS is 0.02525903328176355
LOSS is 0.0351338979251644
LOSS is 0.0309346164490853
LOSS is 0.026355118638942562
LOSS is 0.0309746900103346
LOSS is 0.036888673897289365
LOSS is 0.023141698417845573
LOSS is 0.027928875974879096
LOSS is 0.0283688925911459
LOSS is 0.0293799947387015
LOSS is 0.031220562078863926
LOSS is 0.03132678231158934
LOSS is 0.03098387295554858
LOSS is 0.03147087218623104
LOSS is 0.03522407041687984
LOSS is 0.025568663920712425
LOSS is 0.03862377492871019
LOSS is 0.0293324359493757
LOSS is 0.03679293692094992
LOSS is 0.03056965605171475
LOSS is 0.027808071894639096
LOSS is 0.039841402688374127
LOSS is 0.027682105143758236
LOSS is 0.03208265358793142
LOSS is 0.02844269604281484
LOSS is 0.032800626206444576
LOSS is 0.026110642012839285
LOSS is 0.030097963800653816
LOSS is 0.029312279209358773
LOSS is 0.04174287872728504
LOSS is 0.024151530276819055
LOSS is 0.029606119915940025
LOSS is 0.025211201819765848
LOSS is 0.02997867259012613
LOSS is 0.038950358435686214
LOSS is 0.03008287654161298
LOSS is 0.0255950263775109
LOSS is 0.027003961199273667
LOSS is 0.028893619542795932
LOSS is 0.02667303271761436
LOSS is 0.027810802287810174
LOSS is 0.030754744855318376
LOSS is 0.03177398862972041
LOSS is 0.03482219389819269
LOSS is 0.03334116701832197
LOSS is 0.02725658962085921
LOSS is 0.025700767176846665
LOSS is 0.031753000175570686
LOSS is 0.029926656879251824
LOSS is 0.02632856762565401
LOSS is 0.03196167241272633
LOSS is 0.028194437629378327
LOSS is 0.02558887780357812
LOSS is 0.03032463850385587
LOSS is 0.03356594627630936
LOSS is 0.02399044558190023
LOSS is 0.026081777976238906
LOSS is 0.03056930901652474
LOSS is 0.03427731655989192
LOSS is 0.02946692904792144
LOSS is 0.03130007843875016
LOSS is 0.0357127237325282
LOSS is 0.029391556605066097
LOSS is 0.030106817535609783
LOSS is 0.03237595806800528
LOSS is 0.03360401192447171
LOSS is 0.031146598135674143
LOSS is 0.029919150808127598
LOSS is 0.04270959206303815
LOSS is 0.03816305996811328
LOSS is 0.035016576313816286
LOSS is 0.02943063331200392
LOSS is 0.023505766343829844
LOSS is 0.029392498498394468
LOSS is 0.028709246047753066
LOSS is 0.031429184051618604
LOSS is 0.02918007867599954
LOSS is 0.03217421880937764
LOSS is 0.03643614509676506
LOSS is 0.02765910439906293
LOSS is 0.03702961911010789
LOSS is 0.03187744091798474
LOSS is 0.026836603167466822
LOSS is 0.026341922495970116
LOSS is 0.02847059612351586
LOSS is 0.032237361133302334
LOSS is 0.03493727434067599
LOSS is 0.03237219043590206
LOSS is 0.021793347758211894
LOSS is 0.027122483316925357
LOSS is 0.021966941329495362
LOSS is 0.036450522625430797
LOSS is 0.03186478745987794
LOSS is 0.03691185837281713
LOSS is 0.03193750262730949
LOSS is 0.0323820050926103
LOSS is 0.03369734444868906
LOSS is 0.030551164906113024
Epoch: [1][1700/2183]	Per Sample Total Time 14.48361	Per Sample Data Time 14.46698	Per Sample DNN Time 0.01664	Train Loss 0.0581	
LOSS is 0.03290730837393009
LOSS is 0.02206794591077293
LOSS is 0.024265348023036495
LOSS is 0.02430191375586825
LOSS is 0.02990459445102412
LOSS is 0.040670914905422254
LOSS is 0.025640095542621567
LOSS is 0.02383538923454277
LOSS is 0.026932730277912927
LOSS is 0.024852014002438715
LOSS is 0.03654944697103929
LOSS is 0.025984840604020672
LOSS is 0.03467292105633533
LOSS is 0.03099634019517301
LOSS is 0.03467912607535254
LOSS is 0.033301201765934825
LOSS is 0.03380516391800484
LOSS is 0.03295798702733009
LOSS is 0.032086478391526424
LOSS is 0.03502450582872067
LOSS is 0.030647257428693897
LOSS is 0.02658351148784277
LOSS is 0.028932200637524757
LOSS is 0.033748356298068154
LOSS is 0.03405542117296136
LOSS is 0.03193056307558436
LOSS is 0.02985131454789856
LOSS is 0.0411377545875439
LOSS is 0.03251902943544944
LOSS is 0.03814232937150033
LOSS is 0.029669250721247712
LOSS is 0.027208679675265256
LOSS is 0.028859905237380495
LOSS is 0.033356473584717605
LOSS is 0.026600329981689964
LOSS is 0.027909046023511717
LOSS is 0.036510080148097285
LOSS is 0.02939532130180548
LOSS is 0.028444630377780417
LOSS is 0.03850221149468174
LOSS is 0.03204864920000546
LOSS is 0.026998332244402266
LOSS is 0.022999967690266205
LOSS is 0.03149854688381311
LOSS is 0.020901310001063395
LOSS is 0.033381058720600175
LOSS is 0.03778515430123662
LOSS is 0.028092587425805202
LOSS is 0.02164332191554422
LOSS is 0.028824502779898467
LOSS is 0.03055871489016378
LOSS is 0.031132026250804
LOSS is 0.02905090218919213
LOSS is 0.03408773935133164
LOSS is 0.027922663122735687
LOSS is 0.024084745750248358
LOSS is 0.026381192761594625
LOSS is 0.023900129043856095
LOSS is 0.03536497540170482
LOSS is 0.02992325661509919
LOSS is 0.025868818063196766
LOSS is 0.03747944245488422
LOSS is 0.03250862637580819
LOSS is 0.029069684197214278
LOSS is 0.030194231047062207
LOSS is 0.0338069515588965
LOSS is 0.023818899467199423
LOSS is 0.030939000518022418
LOSS is 0.03167002298306519
LOSS is 0.028441030437049146
LOSS is 0.028435700698610164
LOSS is 0.028850380491106383
LOSS is 0.026234377986741796
LOSS is 0.02693940787302078
LOSS is 0.029569034198114727
LOSS is 0.03459888362684675
LOSS is 0.03299056789823226
LOSS is 0.031020760375540704
LOSS is 0.03215509997030798
LOSS is 0.033540157093448215
LOSS is 0.0338776656058811
LOSS is 0.029073415739403575
LOSS is 0.029409558221304907
LOSS is 0.023516470215254232
LOSS is 0.029932253590037972
LOSS is 0.027538596457306996
LOSS is 0.030913509530728335
LOSS is 0.031190231501386737
LOSS is 0.031524541363760365
LOSS is 0.02625594952235891
LOSS is 0.028307210349691258
LOSS is 0.03039271512985579
LOSS is 0.0350556200115049
LOSS is 0.028636796471764685
LOSS is 0.029437598326961356
LOSS is 0.025406245827301367
LOSS is 0.026891481075047827
LOSS is 0.037959113579466554
LOSS is 0.03792487619219174
LOSS is 0.03044347488959223
Epoch: [1][1800/2183]	Per Sample Total Time 15.30902	Per Sample Data Time 15.29240	Per Sample DNN Time 0.01663	Train Loss 0.0565	
LOSS is 0.022853359363555988
LOSS is 0.02445330113871023
LOSS is 0.030230480527631397
LOSS is 0.02932120941850978
LOSS is 0.0270864661984524
LOSS is 0.022599963413716374
LOSS is 0.035609836959508055
LOSS is 0.037102679315018275
LOSS is 0.024879227397589906
LOSS is 0.03258114392432617
LOSS is 0.027607759361150484
LOSS is 0.02457860078284284
LOSS is 0.026510847576403953
LOSS is 0.026415042991672334
LOSS is 0.025178497584905323
LOSS is 0.03317016453433704
LOSS is 0.024748826260717276
LOSS is 0.025792365238060787
LOSS is 0.025222549163348353
LOSS is 0.02942610685500161
LOSS is 0.026958027093593653
LOSS is 0.026783154421912817
LOSS is 0.024503286744681343
LOSS is 0.028679480968009256
LOSS is 0.024259773906039
LOSS is 0.018646818656173614
LOSS is 0.028195442744812078
LOSS is 0.03123913300956095
LOSS is 0.025182747182164653
LOSS is 0.03487419534115664
LOSS is 0.023826683344329167
LOSS is 0.02129557801425108
LOSS is 0.03256351169460686
LOSS is 0.02908242128842782
LOSS is 0.038589416256921444
LOSS is 0.03368493452377152
LOSS is 0.03987751000725742
LOSS is 0.034133081139831725
LOSS is 0.018470267598774324
LOSS is 0.03205704400810646
LOSS is 0.0330044900750363
LOSS is 0.03130483275895434
LOSS is 0.028492996865146173
LOSS is 0.03196294662297684
LOSS is 0.028322952368156015
LOSS is 0.032739446054838486
LOSS is 0.03868481163682494
LOSS is 0.03481073218698536
LOSS is 0.028339058111426615
LOSS is 0.024669169784707873
LOSS is 0.03552394631134424
LOSS is 0.0192711777818234
LOSS is 0.02735641375538156
LOSS is 0.023860426433893737
LOSS is 0.0315203902390446
LOSS is 0.021365088353001438
LOSS is 0.03079655435193369
LOSS is 0.031037101672845895
LOSS is 0.026348552388929726
LOSS is 0.027805543223415347
LOSS is 0.031128052105123063
LOSS is 0.02832166186058506
LOSS is 0.031015476678973454
LOSS is 0.02937200779366928
LOSS is 0.027171944230794907
LOSS is 0.03729791050296626
LOSS is 0.03615668992308201
LOSS is 0.03227543974285557
LOSS is 0.02790911594388793
LOSS is 0.027607964375395873
LOSS is 0.025674848711035644
LOSS is 0.02379127307799839
LOSS is 0.022696455917757704
LOSS is 0.029944969826707774
LOSS is 0.037748795969503895
LOSS is 0.03161709546596588
LOSS is 0.030364280321324866
LOSS is 0.026973705441681282
LOSS is 0.0322327637806787
LOSS is 0.03190806787121498
LOSS is 0.030941560184389046
LOSS is 0.02158782195588477
LOSS is 0.024897132347881174
LOSS is 0.028766753582652505
LOSS is 0.026643289752003817
LOSS is 0.027425716143334285
LOSS is 0.026130857120733708
LOSS is 0.030642388528285665
LOSS is 0.030679899385528796
LOSS is 0.02857341713664937
LOSS is 0.02069722858313374
LOSS is 0.02847071252266081
LOSS is 0.03301028506323443
LOSS is 0.023980473079791412
LOSS is 0.0316019342468159
LOSS is 0.026619007121383525
LOSS is 0.03256365184376288
LOSS is 0.023406746427402446
LOSS is 0.04115800826009945
LOSS is 0.030935521320449577
Epoch: [1][1900/2183]	Per Sample Total Time 16.13431	Per Sample Data Time 16.11769	Per Sample DNN Time 0.01662	Train Loss 0.0551	
LOSS is 0.03881455926083921
LOSS is 0.02453713178741358
LOSS is 0.030291979376440095
LOSS is 0.03132971295461175
LOSS is 0.021755696524099526
LOSS is 0.03110276379096225
LOSS is 0.040321307577420765
LOSS is 0.029802504643036326
LOSS is 0.019329682787210915
LOSS is 0.02328306117655302
LOSS is 0.030541755749460817
LOSS is 0.03383303411503827
LOSS is 0.029849218742116743
LOSS is 0.02369623691318945
LOSS is 0.03175245652681042
LOSS is 0.034442842636902546
LOSS is 0.03282543826530552
LOSS is 0.02688388416504798
LOSS is 0.02360772377784694
LOSS is 0.03303429916830889
LOSS is 0.03450489194568945
LOSS is 0.029979802423670966
LOSS is 0.03278817416066886
LOSS is 0.03260672201266668
LOSS is 0.027049101541075895
LOSS is 0.025895330077012963
LOSS is 0.04021524684348454
LOSS is 0.024252124207923772
LOSS is 0.025886696492816556
LOSS is 0.035546838902907135
LOSS is 0.03106820466797217
LOSS is 0.03269789479959097
LOSS is 0.0388636597336396
LOSS is 0.03141417922629141
LOSS is 0.025437572607236992
LOSS is 0.02607842869622497
LOSS is 0.028875418546376753
LOSS is 0.0232832528960959
LOSS is 0.03246894219819903
LOSS is 0.02718767208546827
LOSS is 0.025972007791036353
LOSS is 0.024926555942777973
LOSS is 0.03209163408020686
LOSS is 0.024485442313889508
LOSS is 0.03570797171999099
LOSS is 0.029118852178459446
LOSS is 0.02949001141896588
LOSS is 0.02779745139492055
LOSS is 0.022624492289784637
LOSS is 0.02565638602078252
LOSS is 0.025520645551732744
LOSS is 0.02463420307224927
LOSS is 0.027496474457730076
LOSS is 0.027770057531015482
LOSS is 0.028276202169339137
LOSS is 0.028193103828962195
LOSS is 0.026352595525143747
LOSS is 0.03114590224751737
LOSS is 0.030867379561435277
LOSS is 0.022166227134584916
LOSS is 0.03329343807563419
LOSS is 0.029931677514856956
LOSS is 0.020018842525945123
LOSS is 0.03037058782038609
LOSS is 0.029771282848038635
LOSS is 0.02534651784306334
LOSS is 0.032060510483715915
LOSS is 0.02929703256139571
LOSS is 0.03524623870594951
LOSS is 0.03422191761734818
LOSS is 0.027959946851042333
LOSS is 0.0278212355334108
LOSS is 0.027136939888926768
LOSS is 0.032239214082898496
LOSS is 0.030927382507070434
LOSS is 0.036136788291114504
LOSS is 0.030368327221319005
LOSS is 0.034615145995776406
LOSS is 0.029381531561909167
LOSS is 0.021385509631218158
LOSS is 0.027534812124213204
LOSS is 0.029319865103122237
LOSS is 0.02440399068441669
LOSS is 0.027233593735412193
LOSS is 0.026414172026100764
LOSS is 0.02989069061656968
LOSS is 0.020368601935236562
LOSS is 0.025419193644581055
LOSS is 0.031448808159038895
LOSS is 0.021762007095676383
LOSS is 0.026014149404994292
LOSS is 0.02948013078659036
LOSS is 0.03402083547674314
LOSS is 0.036910340054794995
LOSS is 0.030445859155855338
LOSS is 0.0378292990543802
LOSS is 0.03859612655617336
LOSS is 0.036505940204442595
LOSS is 0.025954584733747958
LOSS is 0.023130510981621534
Epoch: [1][2000/2183]	Per Sample Total Time 16.95931	Per Sample Data Time 16.94270	Per Sample DNN Time 0.01661	Train Loss 0.0538	
LOSS is 0.02749248398523681
LOSS is 0.03375953497787123
LOSS is 0.03869645807601046
LOSS is 0.02321809235516412
LOSS is 0.04040537504879467
LOSS is 0.035362096360307384
LOSS is 0.028137042996725845
LOSS is 0.02777669348647275
LOSS is 0.026495694516852384
LOSS is 0.032891318217783315
LOSS is 0.030764706216408137
LOSS is 0.021588505630061264
LOSS is 0.030688946690303663
LOSS is 0.02530748746319053
LOSS is 0.028033955256978518
LOSS is 0.0273587460933292
LOSS is 0.02488823281816925
LOSS is 0.03456570411901339
LOSS is 0.025650928149128355
LOSS is 0.025646720205477324
LOSS is 0.02801839016717471
LOSS is 0.025305533751428206
LOSS is 0.040336738602491096
LOSS is 0.0281888770909427
LOSS is 0.025466189524304357
LOSS is 0.029010325849740184
LOSS is 0.024514067534400965
LOSS is 0.028585457868709155
LOSS is 0.02585926948321382
LOSS is 0.030006170027748646
LOSS is 0.027091046816058225
LOSS is 0.026903502546968716
LOSS is 0.026716058193439193
LOSS is 0.031127318455595136
LOSS is 0.03265299945184476
LOSS is 0.02605545167384359
LOSS is 0.021721342567204073
LOSS is 0.027769749232102188
LOSS is 0.020124327434799246
LOSS is 0.025405691540630264
LOSS is 0.024892240970293644
LOSS is 0.03195085057236914
LOSS is 0.026222147575050866
LOSS is 0.028080059595304192
LOSS is 0.031008952130384085
LOSS is 0.02369997863129053
LOSS is 0.02008593371724904
LOSS is 0.032153653809800745
LOSS is 0.03154755331132037
LOSS is 0.024910022488353813
LOSS is 0.028423415325548075
LOSS is 0.02698431756068506
LOSS is 0.031409335546074245
LOSS is 0.02436944554935811
LOSS is 0.026948459250464418
LOSS is 0.025954880279581025
LOSS is 0.019355747399749816
LOSS is 0.023315265994945853
LOSS is 0.03411485270301152
LOSS is 0.025007230221939004
LOSS is 0.029787330568506153
LOSS is 0.02807013827781096
LOSS is 0.02065361271944615
LOSS is 0.031456809921837706
LOSS is 0.03671109938327088
LOSS is 0.02737956503563813
LOSS is 0.020069036497006892
LOSS is 0.024249031123957442
LOSS is 0.035905637104733615
LOSS is 0.021077842458641197
LOSS is 0.03711548901752394
LOSS is 0.030116659783258608
LOSS is 0.024590176870963966
LOSS is 0.022858464005500234
LOSS is 0.025633730775080042
LOSS is 0.03358440403089238
LOSS is 0.030437765623379773
LOSS is 0.027074135877434555
LOSS is 0.038575361449426665
LOSS is 0.027872757969307715
LOSS is 0.029822204465793527
LOSS is 0.02599629814224803
LOSS is 0.027883419577822983
LOSS is 0.023676194185900386
LOSS is 0.021195544647731976
LOSS is 0.028686010936435198
LOSS is 0.030775128903575633
LOSS is 0.021689496573599174
LOSS is 0.031119200965792213
LOSS is 0.02847930245496779
LOSS is 0.02888120913654954
LOSS is 0.03516718013942106
LOSS is 0.031140194419834493
LOSS is 0.030454554497846405
LOSS is 0.03645465485977184
LOSS is 0.022803278021286438
LOSS is 0.029291241741860482
LOSS is 0.0239171461390409
LOSS is 0.0172422267904282
LOSS is 0.029370890727999116
Epoch: [1][2100/2183]	Per Sample Total Time 17.78418	Per Sample Data Time 17.76758	Per Sample DNN Time 0.01660	Train Loss 0.0525	
LOSS is 0.03652611920452425
LOSS is 0.03562765568443865
LOSS is 0.03161378176603952
LOSS is 0.03892061673276961
LOSS is 0.03074972772995049
LOSS is 0.02406261695827804
LOSS is 0.01794726680222084
LOSS is 0.023560657435955364
LOSS is 0.02576392227987526
LOSS is 0.027468163572557387
LOSS is 0.02895952567443601
LOSS is 0.023205026065625134
LOSS is 0.021063039594422055
LOSS is 0.031442596014894665
LOSS is 0.030140631516345214
LOSS is 0.02267119853643332
LOSS is 0.03353079503101374
LOSS is 0.02857330060374807
LOSS is 0.025632074621632153
LOSS is 0.017459252036545272
LOSS is 0.03283359510693117
LOSS is 0.03410335259276811
LOSS is 0.026261301371099156
LOSS is 0.028768780997658422
LOSS is 0.03309764176789031
LOSS is 0.028356754644798157
LOSS is 0.02237945517428064
LOSS is 0.024920286571238348
LOSS is 0.027193755456537475
LOSS is 0.034062285654654266
LOSS is 0.026914999573564274
LOSS is 0.03039690490300321
LOSS is 0.0360040742046355
LOSS is 0.026609528351230743
LOSS is 0.02010131434542321
LOSS is 0.02854730323820453
LOSS is 0.02796590450583608
LOSS is 0.02444389237716678
LOSS is 0.0371935978486484
LOSS is 0.027153113371338505
LOSS is 0.030842690627517492
LOSS is 0.026704651249989792
LOSS is 0.027455472163055675
LOSS is 0.024013664919281537
LOSS is 0.02616965510947921
LOSS is 0.0329966425083209
LOSS is 0.029451733775931646
LOSS is 0.03657059587796539
LOSS is 0.0233250350804398
LOSS is 0.031066634434076455
LOSS is 0.03476262964262181
LOSS is 0.028768446110577012
LOSS is 0.03197820208134848
LOSS is 0.02603791869087824
LOSS is 0.02455863610111313
LOSS is 0.032472250562835446
LOSS is 0.0318789381969206
LOSS is 0.02576690434800791
LOSS is 0.026909572352160467
LOSS is 0.029367045028848227
LOSS is 0.028263247809469857
LOSS is 0.020921063079003946
LOSS is 0.027550391962407352
LOSS is 0.019959334070323773
LOSS is 0.030698298888722397
LOSS is 0.029470454641147323
LOSS is 0.03799830638080797
LOSS is 0.020485819327417026
LOSS is 0.025296377151438114
LOSS is 0.021728712441545214
LOSS is 0.022423761480725567
LOSS is 0.023088646051376904
LOSS is 0.028839534906704406
LOSS is 0.026809045689684962
LOSS is 0.031575953301289705
LOSS is 0.013509862419741695
LOSS is 0.026437238909444813
LOSS is 0.024415147187634528
LOSS is 0.030935589602934976
LOSS is 0.031020828470136624
LOSS is 0.036116230099917934
LOSS is 0.027196953512199497
inside validate
LOSS in validation is 0.05878764351209005
LOSS in validation is 0.05412172138690949
LOSS in validation is 0.049483461181322734
LOSS in validation is 0.026950756609439852
LOSS in validation is 0.03703417986631394
LOSS in validation is 0.032209235032399496
LOSS in validation is 0.043476208150386816
LOSS in validation is 0.04454159826040268
LOSS in validation is 0.06550959358612697
LOSS in validation is 0.06697046210368475
LOSS in validation is 0.03775774031877518
LOSS in validation is 0.03292153626680375
LOSS in validation is 0.023020769158999126
LOSS in validation is 0.025657156109809877
LOSS in validation is 0.04960253496964773
LOSS in validation is 0.03456397394339244
LOSS in validation is 0.029692145784695947
LOSS in validation is 0.05382112602392833
LOSS in validation is 0.0642760819196701
LOSS in validation is 0.03955558806657791
LOSS in validation is 0.04725002278884252
LOSS in validation is 0.05655114978551865
LOSS in validation is 0.05673100968201956
LOSS in validation is 0.07585072924693426
LOSS in validation is 0.029889782170454664
LOSS in validation is 0.015104772051175436
LOSS in validation is 0.0102585502465566
LOSS in validation is 0.013346357345581055
LOSS in validation is 0.01672998239596685
LOSS in validation is 0.020350906550884246
LOSS in validation is 0.01577892690896988
LOSS in validation is 0.015844253897666933
LOSS in validation is 0.018373535573482515
LOSS in validation is 0.010470683375994366
LOSS in validation is 0.008773172100385031
LOSS in validation is 0.02183117707570394
LOSS in validation is 0.01065162738164266
LOSS in validation is 0.011252019703388214
LOSS in validation is 0.010655874411265056
LOSS in validation is 0.08182341754436494
LOSS in validation is 0.06262222975492478
LOSS in validation is 0.0328561927874883
LOSS in validation is 0.018342166940371198
LOSS in validation is 0.021974394122759502
LOSS in validation is 0.026574540436267856
LOSS in validation is 0.02589426378409068
LOSS in validation is 0.024998786846796673
LOSS in validation is 0.02905142684777578
LOSS in validation is 0.03509040892124176
LOSS in validation is 0.023488996525605522
LOSS in validation is 0.021705366571744284
LOSS in validation is 0.01249663253625234
LOSS in validation is 0.02111018419265747
LOSS in validation is 0.011848233739535015
LOSS in validation is 0.04192293196916581
LOSS in validation is 0.014927551647027335
LOSS in validation is 0.03371414800484975
LOSS in validation is 0.05072700927654902
LOSS in validation is 0.045936251382033035
LOSS in validation is 0.029256790677706403
LOSS in validation is 0.017050900161266327
LOSS in validation is 0.02488156984249751
LOSS in validation is 0.032371377348899846
LOSS in validation is 0.01986371219158173
LOSS in validation is 0.03747588723897934
LOSS in validation is 0.021436571975549063
LOSS in validation is 0.016303280194600426
LOSS in validation is 0.0179748938481013
LOSS in validation is 0.01419641524553299
LOSS in validation is 0.0347043659289678
LOSS in validation is 0.02437174330155055
LOSS in validation is 0.04630492568016053
LOSS in validation is 0.028690842588742577
LOSS in validation is 0.033388267258803055
LOSS in validation is 0.020821248690287272
LOSS in validation is 0.02667368749777476
LOSS in validation is 0.02672652125358582
LOSS in validation is 0.015768374800682067
LOSS in validation is 0.01868066837390264
LOSS in validation is 0.021004515488942464
LOSS in validation is 0.015613686442375183
LOSS in validation is 0.020670372446378073
LOSS in validation is 0.03240887741247813
LOSS in validation is 0.01253822018702825
LOSS in validation is 0.01108547886212667
LOSS in validation is 0.015926994681358338
LOSS in validation is 0.021420805553595226
LOSS in validation is 0.03646897415320079
LOSS in validation is 0.025916596353054048
LOSS in validation is 0.028339085082213087
LOSS in validation is 0.027212943037350973
LOSS in validation is 0.04268281330664953
LOSS in validation is 0.040824791789054876
LOSS in validation is 0.01656002551317215
LOSS in validation is 0.020744157433509828
LOSS in validation is 0.028207073410352074
LOSS in validation is 0.030545207758744558
LOSS in validation is 0.0196176090836525
LOSS in validation is 0.04003481060266495
LOSS in validation is 0.02465629349152247
LOSS in validation is 0.019738546510537466
LOSS in validation is 0.028103386064370475
LOSS in validation is 0.019688997964064283
LOSS in validation is 0.021906439264615378
LOSS in validation is 0.019608791768550873
LOSS in validation is 0.04536262750625611
LOSS in validation is 0.028420919875303905
LOSS in validation is 0.02590171168247859
LOSS in validation is 0.037252039810021724
LOSS in validation is 0.020792770683765414
LOSS in validation is 0.015773474176724752
LOSS in validation is 0.014454216957092286
LOSS in validation is 0.02397138684988022
LOSS in validation is 0.022500760555267334
LOSS in validation is 0.0155780166387558
LOSS in validation is 0.04603557487328847
LOSS in validation is 0.024395292699337007
LOSS in validation is 0.01686329692602158
LOSS in validation is 0.01485015074412028
LOSS in validation is 0.01659844795862834
LOSS in validation is 0.025988611380259197
LOSS in validation is 0.022492045958836873
LOSS in validation is 0.01906392296155294
LOSS in validation is 0.019124712149302166
LOSS in validation is 0.025498089293638868
LOSS in validation is 0.010750145415465038
LOSS in validation is 0.020047386686007182
LOSS in validation is 0.025312750538190206
LOSS in validation is 0.025129841367403667
LOSS in validation is 0.022606489459673564
LOSS in validation is 0.017853275338808698
LOSS in validation is 0.033656886418660485
LOSS in validation is 0.07251217305660249
LOSS in validation is 0.0319658049941063
LOSS in validation is 0.006460512280464173
LOSS in validation is 0.010502247313658397
LOSS in validation is 0.00803136388460795
LOSS in validation is 0.013916191657384237
LOSS in validation is 0.00487557053565979
LOSS in validation is 0.008571592271327973
LOSS in validation is 0.02241643329461416
LOSS in validation is 0.016485131482283276
LOSS in validation is 0.007829647362232208
LOSS in validation is 0.005106706221898397
LOSS in validation is 0.009901867707570395
LOSS in validation is 0.009202988743782043
LOSS in validation is 0.010445693035920462
LOSS in validation is 0.02287578413883845
LOSS in validation is 0.013013443748156231
LOSS in validation is 0.006303860147794088
LOSS in validation is 0.008033080299695333
LOSS in validation is 0.008494574626286826
LOSS in validation is 0.01063170741001765
LOSS in validation is 0.011597975691159567
LOSS in validation is 0.016606784959634146
LOSS in validation is 0.02280871570110321
LOSS in validation is 0.011572795112927756
LOSS in validation is 0.018053269187609355
LOSS in validation is 0.02355066776275635
LOSS in validation is 0.009896556635697683
LOSS in validation is 0.015372610986232759
LOSS in validation is 0.009307728111743928
LOSS in validation is 0.030633064707120263
LOSS in validation is 0.023601749340693156
LOSS in validation is 0.0164877059062322
LOSS in validation is 0.024168249368667603
LOSS in validation is 0.01870298782984416
LOSS in validation is 0.027661737600962324
LOSS in validation is 0.036254848142464956
LOSS in validation is 0.03143323669830958
LOSS in validation is 0.02527487377325694
LOSS in validation is 0.02854855537414551
LOSS in validation is 0.046894940038522084
LOSS in validation is 0.03848619212706884
LOSS in validation is 0.018838766117890676
LOSS in validation is 0.02231088350216548
LOSS in validation is 0.08348385840654374
LOSS in validation is 0.07872144182523091
LOSS in validation is 0.025719274878501893
LOSS in validation is 0.008896519144376119
LOSS in validation is 0.017010630667209627
LOSS in validation is 0.016233573456605276
LOSS in validation is 0.01865167001883189
LOSS in validation is 0.01013384610414505
LOSS in validation is 0.01582807590564092
LOSS in validation is 0.01355453352133433
LOSS in validation is 0.012781520982583365
LOSS in validation is 0.013403814534346263
LOSS in validation is 0.01102802852789561
LOSS in validation is 0.012612717250982921
LOSS in validation is 0.01554033656915029
LOSS in validation is 0.01346021205186844
LOSS in validation is 0.018726368745168052
LOSS in validation is 0.023723278144995374
LOSS in validation is 0.016463852226734164
LOSS in validation is 0.012023314038912457
LOSS in validation is 0.010183505713939667
LOSS in validation is 0.014025253752867381
LOSS in validation is 0.021305164992809298
LOSS in validation is 0.037740034162998204
LOSS in validation is 0.030868371526400252
LOSS in validation is 0.06230474134286245
LOSS in validation is 0.03330957571665446
LOSS in validation is 0.020756732026735943
LOSS in validation is 0.030730800926685335
LOSS in validation is 0.036238376994927726
LOSS in validation is 0.03386697858572006
LOSS in validation is 0.0414988515774409
LOSS in validation is 0.04332631270090739
LOSS in validation is 0.022415841817855837
LOSS in validation is 0.0334231107433637
LOSS in validation is 0.03012643943230311
LOSS in validation is 0.031043375035127007
LOSS in validation is 0.05740523894627889
LOSS in validation is 0.033277810613314314
LOSS in validation is 0.04324905713399252
LOSS in validation is 0.06331101487080257
LOSS in validation is 0.05825740605592728
LOSS in validation is 0.037511917750040695
LOSS in validation is 0.0506242506702741
LOSS in validation is 0.03643741538127264
LOSS in validation is 0.07034683893124263
LOSS in validation is 0.06971394936243694
LOSS in validation is 0.020453519026438396
LOSS in validation is 0.02930133144060771
LOSS in validation is 0.023695952991644544
LOSS in validation is 0.027480253577232362
LOSS in validation is 0.03383089105288188
LOSS in validation is 0.020196928878625234
LOSS in validation is 0.029156420727570853
LOSS in validation is 0.048863467971483866
LOSS in validation is 0.02575969924529394
LOSS in validation is 0.024758560657501223
LOSS in validation is 0.040774571398894
LOSS in validation is 0.03545779466629029
LOSS in validation is 0.030770547787348432
LOSS in validation is 0.03846983790397644
LOSS in validation is 0.026115204294522604
LOSS in validation is 0.026983358959356947
LOSS in validation is 0.02455503990252813
LOSS in validation is 0.022155522505442303
LOSS in validation is 0.02009263426065445
LOSS in validation is 0.025097042322158813
LOSS in validation is 0.020250369211037955
LOSS in validation is 0.02194079120953878
LOSS in validation is 0.022035305599371592
LOSS in validation is 0.01798152138789495
LOSS in validation is 0.015701605280240376
LOSS in validation is 0.02614142139752706
LOSS in validation is 0.03236359884341558
LOSS in validation is 0.026102371712525687
LOSS in validation is 0.01763248274723689
LOSS in validation is 0.033176179925600686
LOSS in validation is 0.03258969734112422
LOSS in validation is 0.045832289755344396
LOSS in validation is 0.01636999030907949
LOSS in validation is 0.019987159868081413
LOSS in validation is 0.017020885745684305
LOSS in validation is 0.02076065331697464
LOSS in validation is 0.04927026311556498
LOSS in validation is 0.08138981848955155
LOSS in validation is 0.08527397821346919
LOSS in validation is 0.06809091170628866
LOSS in validation is 0.046349165240923564
LOSS in validation is 0.0310231218735377
LOSS in validation is 0.042373048663139345
LOSS in validation is 0.04865426381429037
LOSS in validation is 0.05107237766186397
LOSS in validation is 0.04294433842102687
LOSS in validation is 0.06094809763055099

----------------------------------------STATS-----------------------------------

[{'precision_micro': 0.899460593347318, 'precision_macro': 0.7993841029531952, 'precision_classes': array([0.66666667, 0.83333333, 1.        , 0.97297297, 0.88817204,
       0.9       , 0.81773399, 0.        , 0.9080292 , 0.        ,
       0.86671803, 0.67073171, 0.93243243, 0.81538462, 0.93512658,
       0.86539644, 0.92200557, 1.        , 0.93793794, 0.97716895,
       1.        , 0.93103448, 0.93333333, 1.        , 0.97497219,
       0.84615385, 1.        , 0.86986301, 1.        , 0.95454545,
       0.98571429, 0.96428571, 0.86666667, 0.80314961, 0.70754717,
       1.        , 0.        , 0.93939394, 0.93285372, 0.96353167,
       0.92814371, 0.87270502, 0.84496124, 1.        , 0.        ,
       0.        , 0.        , 0.78873239, 0.95180723, 1.        ]), 'recall_micro': 0.6948184729349126, 'recall_macro': 0.46002790655400233, 'recall_classes': array([0.19917012, 0.61122244, 0.00763359, 0.32432432, 0.62481089,
       0.30681818, 0.42894057, 0.        , 0.86993007, 0.        ,
       0.73337679, 0.28497409, 0.82142857, 0.72311396, 0.70567164,
       0.76108108, 0.78436019, 0.15217391, 0.91370063, 0.8699187 ,
       0.56164384, 0.72497966, 0.09395973, 0.0877193 , 0.86997519,
       0.54609929, 0.49285714, 0.66145833, 0.15384615, 0.70542636,
       0.5390625 , 0.45762712, 0.19548872, 0.52849741, 0.39267016,
       0.05405405, 0.        , 0.27433628, 0.71245421, 0.63303909,
       0.77629382, 0.77248104, 0.75043029, 0.4516129 , 0.        ,
       0.        , 0.        , 0.57435897, 0.48170732, 0.38666667]), 'f1_micro': 0.7840055723645545, 'f1_macro': 0.5464929246409476, 'f1_classes': array([0.30670927, 0.70520231, 0.01515152, 0.48648649, 0.73357016,
       0.45762712, 0.56271186, 0.        , 0.88857143, 0.        ,
       0.79449153, 0.4       , 0.87341772, 0.76648235, 0.80435522,
       0.80989359, 0.84763124, 0.26415094, 0.92566066, 0.92043011,
       0.71929825, 0.81518756, 0.17073171, 0.16129032, 0.91948597,
       0.6637931 , 0.66028708, 0.75147929, 0.26666667, 0.81129272,
       0.6969697 , 0.62068966, 0.3190184 , 0.6375    , 0.50505051,
       0.1025641 , 0.        , 0.42465753, 0.807892  , 0.76407915,
       0.84545455, 0.81954023, 0.79489517, 0.62222222, 0.        ,
       0.        , 0.        , 0.66468843, 0.63967611, 0.55769231]), 'acc': 0.7710945638334812}]
Precision Micro: 0.899461
Precision Macro: 0.799384
Recall Micro: 0.694818
Recall Macro: 0.460028
F1 Micro: 0.784006
F1 Micro: 0.546493
classes- precision - recall - f1:
[0.66666667 0.83333333 1.         0.97297297 0.88817204 0.9
 0.81773399 0.         0.9080292  0.         0.86671803 0.67073171
 0.93243243 0.81538462 0.93512658 0.86539644 0.92200557 1.
 0.93793794 0.97716895 1.         0.93103448 0.93333333 1.
 0.97497219 0.84615385 1.         0.86986301 1.         0.95454545
 0.98571429 0.96428571 0.86666667 0.80314961 0.70754717 1.
 0.         0.93939394 0.93285372 0.96353167 0.92814371 0.87270502
 0.84496124 1.         0.         0.         0.         0.78873239
 0.95180723 1.        ]
[0.19917012 0.61122244 0.00763359 0.32432432 0.62481089 0.30681818
 0.42894057 0.         0.86993007 0.         0.73337679 0.28497409
 0.82142857 0.72311396 0.70567164 0.76108108 0.78436019 0.15217391
 0.91370063 0.8699187  0.56164384 0.72497966 0.09395973 0.0877193
 0.86997519 0.54609929 0.49285714 0.66145833 0.15384615 0.70542636
 0.5390625  0.45762712 0.19548872 0.52849741 0.39267016 0.05405405
 0.         0.27433628 0.71245421 0.63303909 0.77629382 0.77248104
 0.75043029 0.4516129  0.         0.         0.         0.57435897
 0.48170732 0.38666667]
[0.30670927 0.70520231 0.01515152 0.48648649 0.73357016 0.45762712
 0.56271186 0.         0.88857143 0.         0.79449153 0.4
 0.87341772 0.76648235 0.80435522 0.80989359 0.84763124 0.26415094
 0.92566066 0.92043011 0.71929825 0.81518756 0.17073171 0.16129032
 0.91948597 0.6637931  0.66028708 0.75147929 0.26666667 0.81129272
 0.6969697  0.62068966 0.3190184  0.6375     0.50505051 0.1025641
 0.         0.42465753 0.807892   0.76407915 0.84545455 0.81954023
 0.79489517 0.62222222 0.         0.         0.         0.66468843
 0.63967611 0.55769231]
train_loss: 0.051628
valid_loss: 0.060948
validation finished
Traceback (most recent call last):
  File "/scratch/users/fsofian19/COMP491_model/models/ast_custom/egs/custom/../../src/run.py", line 150, in <module>
    train(audio_model, train_loader, val_loader, args)
  File "/scratch/users/fsofian19/COMP491_model/models/ast_custom/src/traintest.py", line 259, in train
    if f1_macro > best_f1_macro:
UnboundLocalError: local variable 'best_f1_macro' referenced before assignment
wandb: Waiting for W&B process to finish, PID 95851... (failed 1). Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.10MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.10MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.10MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.10MB uploaded (0.00MB deduped)wandb: / 0.09MB of 0.10MB uploaded (0.00MB deduped)wandb: - 0.09MB of 0.10MB uploaded (0.00MB deduped)wandb: \ 0.09MB of 0.10MB uploaded (0.00MB deduped)wandb: | 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: / 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: - 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: \ 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: | 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: / 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: - 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: \ 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: | 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: / 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: - 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: \ 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:   Train loss 
wandb: 
wandb: Run summary:
wandb:   Train loss 0.05255
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced daily-dew-3: https://wandb.ai/birdsongs/istangul-50-class/runs/m7jrmowk
wandb: Find logs at: ./wandb/run-20220508_203109-m7jrmowk/logs/debug.log
wandb: 

+ (( fold++ ))
+ (( fold<=1 ))

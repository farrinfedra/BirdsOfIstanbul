+ export TORCH_HOME=../../pretrained_models
+ TORCH_HOME=../../pretrained_models
+ model=ast
+ dataset=custom
+ nocall=True
+ timetest = True
/var/spool/slurm/d/job3243043/slurm_script: line 24: timetest: command not found
+ imagenetpretrain=True
+ audiosetpretrain=True
+ bal=False
+ '[' True == True ']'
+ lr=1e-5
+ freqm=24
+ timem=96
+ mixup=0
+ epoch=6
+ batch_size=48
+ fstride=10
+ tstride=10
+ base_exp_dir=./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_50class_solid
+ '[' -d ./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_50class_solid ']'
+ mkdir -p
mkdir: missing operand
Try 'mkdir --help' for more information.
+ (( fold=1 ))
+ (( fold<=1 ))
+ echo 'now process fold1'
now process fold1
+ exp_dir=./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_50class_solid/fold1
+ tr_data=./data/datafiles/custom_train_data_1.json
+ te_data=./data/datafiles/custom_eval_data_1.json
+ CUDA_CACHE_DISABLE=1
+ python -W ignore ../../src/run.py --model ast --dataset custom --data-train ./data/datafiles/custom_train_data_1.json --data-val ./data/datafiles/custom_eval_data_1.json --exp-dir ./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_50class_solid/fold1 --label-csv ./data/custom_labels.csv --n_class 50 --lr 1e-5 --n-epochs 6 --batch-size 48 --save_model False --freqm 24 --timem 96 --mixup 0 --bal False --tstride 10 --fstride 10 --imagenet_pretrain True --audioset_pretrain True
wandb: Currently logged in as: birdsongs (use `wandb login --relogin` to force relogin)
I am process 90659, running on ai06.kuacc.ku.edu.tr: starting (Sun May  8 19:12:09 2022)
wandb: Tracking run with wandb version 0.12.9
wandb: Syncing run wobbly-dust-2
wandb:  View project at https://wandb.ai/birdsongs/istangul-50-class
wandb:  View run at https://wandb.ai/birdsongs/istangul-50-class/runs/bsw933h1
wandb: Run data is saved locally in /scratch/users/fsofian19/COMP491_model/models/ast_custom/egs/custom/wandb/run-20220508_191209-bsw933h1
wandb: Run `wandb offline` to turn off syncing.

now train a audio spectrogram transformer model
balanced sampler is not used
---------------the train dataloader---------------
now using following mask: 24 freq, 96 time
now using mix-up with rate 0.000000
now process custom
use dataset mean -6.442 and std 4.178 to normalize the input.
number of classes is 50
---------------the evaluation dataloader---------------
now using following mask: 0 freq, 0 time
now using mix-up with rate 0.000000
now process custom
use dataset mean -6.442 and std 4.178 to normalize the input.
number of classes is 50
---------------AST Model Summary---------------
ImageNet pretraining: True, AudioSet pretraining: True
frequncey stride=10, time stride=10
number of patches=600

Creating experiment directory: ./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_50class_solid/fold1
Now starting training for 6 epochs
2022-05-08 19:12:33.770689
running on cuda
state dict to model completed => pretrained weights loaded
<generator object Module.named_parameters at 0x2b83d83c86d0>
Total parameter number is : 87.295 million
Total trainable parameter number is : 87.295 million
scheduler for custom dataset is used
now training with custom, main metrics: mAP, loss function: BCEWithLogitsLoss(), learning rate scheduler: <torch.optim.lr_scheduler.MultiStepLR object at 0x2b8405a0a670>
current #steps=0, #epochs=1
start training...
---------------
2022-05-08 19:12:34.185474
current #epochs=1, #steps=0
LOSS is 0.7212380999140442
LOSS is 0.6278528455862155
LOSS is 0.543663735127387
LOSS is 0.4772055144452801
LOSS is 0.43352040143062676
LOSS is 0.38316657179035246
LOSS is 0.3547720410022885
LOSS is 0.33013434029494726
LOSS is 0.30819669491921864
LOSS is 0.28480879886851956
LOSS is 0.26737930700182916
LOSS is 0.2580905892979354
LOSS is 0.2399337735461692
LOSS is 0.23298419223477446
LOSS is 0.22401302566130957
LOSS is 0.21557758894904208
LOSS is 0.21064610738928122
LOSS is 0.20320820356719196
LOSS is 0.20022697453852745
LOSS is 0.19165071091769886
LOSS is 0.1888172213577976
LOSS is 0.18486146258190275
LOSS is 0.17989977352010708
LOSS is 0.1771349891104425
LOSS is 0.1756359911275407
LOSS is 0.17230389866201828
LOSS is 0.17003338577691468
LOSS is 0.16791199390931677
LOSS is 0.16817323827184738
LOSS is 0.16456034822234264
LOSS is 0.16401737296798577
LOSS is 0.16077008777763696
LOSS is 0.16040678006131204
LOSS is 0.15758226008309673
LOSS is 0.15564762736825893
LOSS is 0.15501356575482836
LOSS is 0.15352852394338698
LOSS is 0.15178448660299182
LOSS is 0.1499872258786733
LOSS is 0.1510863791856294
LOSS is 0.14942260840907695
LOSS is 0.14854658839913706
LOSS is 0.14727483172590536
LOSS is 0.1462583231072252
LOSS is 0.14512422636461755
LOSS is 0.1449147222765411
LOSS is 0.144473130403397
LOSS is 0.1443522043991834
LOSS is 0.14194522483895222
LOSS is 0.14180377543748668
LOSS is 0.1411415814391027
LOSS is 0.1406659573999544
LOSS is 0.13929032958267878
LOSS is 0.13934600946183007
LOSS is 0.14053048012002062
LOSS is 0.13937669187628976
LOSS is 0.1383109508159881
LOSS is 0.1386124946937586
LOSS is 0.13675554129605494
LOSS is 0.1361588581977412
LOSS is 0.1360113298132395
LOSS is 0.13518958383395027
LOSS is 0.13422958459084233
LOSS is 0.13404576792226483
LOSS is 0.13331476799522837
LOSS is 0.1324845777110507
LOSS is 0.13311230823552858
LOSS is 0.13328673771737765
LOSS is 0.1333800148110216
LOSS is 0.13294959847349674
LOSS is 0.13131834213777135
LOSS is 0.13045322387013586
LOSS is 0.12998811988625678
LOSS is 0.12882488988495122
LOSS is 0.13042991555761546
LOSS is 0.12860530593122046
LOSS is 0.12905387861188503
LOSS is 0.12937643414518488
LOSS is 0.12747044729844978
LOSS is 0.12999674020800742
LOSS is 0.12712639096348236
LOSS is 0.12609520060320695
LOSS is 0.12622278071784726
LOSS is 0.1253144057871153
LOSS is 0.12532367696675162
LOSS is 0.12489495377677183
LOSS is 0.12462789898893487
LOSS is 0.12518281248398125
LOSS is 0.12290208659600467
LOSS is 0.1257577138285463
LOSS is 0.12289611312405517
LOSS is 0.12451867993610602
LOSS is 0.12171494761481882
LOSS is 0.12211650899921855
LOSS is 0.12187397580128163
LOSS is 0.12405699760497858
LOSS is 0.1231002757480989
LOSS is 0.12103782125438253
LOSS is 0.12229924217797816
LOSS is 0.1222696919258063
LOSS is 0.12015849962830544
Epoch: [1][100/2183]	Per Sample Total Time 1.21511	Per Sample Data Time 1.19658	Per Sample DNN Time 0.01853	Train Loss 0.1792	
LOSS is 0.12097095720004292
LOSS is 0.11890081233810634
LOSS is 0.12068757356765371
LOSS is 0.1186673487443477
LOSS is 0.12107024387146037
LOSS is 0.12065251180281243
LOSS is 0.12008382933524749
LOSS is 0.1195105512191852
LOSS is 0.11769288515827309
LOSS is 0.11901090608288845
LOSS is 0.11672698638246706
LOSS is 0.11714145539483677
LOSS is 0.11781552730904273
LOSS is 0.11676033161037291
LOSS is 0.1171058339563509
LOSS is 0.11689354012875508
LOSS is 0.1167834335876008
LOSS is 0.11507594168341409
LOSS is 0.11499679546337575
LOSS is 0.11662494389961164
LOSS is 0.11762354645257196
LOSS is 0.11652752353344113
LOSS is 0.11450058189453557
LOSS is 0.11504111019894482
LOSS is 0.11228164551779629
LOSS is 0.11410567039235807
LOSS is 0.11410136871195088
LOSS is 0.11441822033065062
LOSS is 0.11366899818802874
LOSS is 0.11385211799216147
LOSS is 0.11220850012342756
LOSS is 0.11601836594442527
LOSS is 0.11389486407007401
LOSS is 0.11152794335658352
LOSS is 0.10976174372869234
LOSS is 0.11423195764732859
LOSS is 0.10891145109431818
LOSS is 0.11256254962878302
LOSS is 0.11272407757118345
LOSS is 0.11033580192209533
LOSS is 0.10836173313825082
LOSS is 0.10994033371564002
LOSS is 0.11068850570979219
LOSS is 0.11156647166004405
LOSS is 0.10983009325262781
LOSS is 0.11035350125360614
LOSS is 0.10814236729405821
LOSS is 0.10996984474748994
LOSS is 0.11032604846482476
LOSS is 0.10803318853800496
LOSS is 0.109699358571476
LOSS is 0.11029668420630818
LOSS is 0.1064692580839619
LOSS is 0.10472353431551408
LOSS is 0.10789750336824606
LOSS is 0.10846597562155998
LOSS is 0.10564477065422884
LOSS is 0.10597055537238097
LOSS is 0.10716373696923256
LOSS is 0.10701587547315285
LOSS is 0.10804610048653558
LOSS is 0.10336598608487597
LOSS is 0.10766812119322519
LOSS is 0.10309326066325109
LOSS is 0.10438214382234341
LOSS is 0.1062673227993461
LOSS is 0.10643264470001063
LOSS is 0.10497741957195104
LOSS is 0.10423967296024785
LOSS is 0.10588084460934624
LOSS is 0.10315005837629239
LOSS is 0.10372588157576199
LOSS is 0.10424374033541729
LOSS is 0.10296025860278557
LOSS is 0.10179872567377364
LOSS is 0.1036525212530978
LOSS is 0.10438526674173773
LOSS is 0.10215402238614237
LOSS is 0.10214699128332239
LOSS is 0.10090549596197283
LOSS is 0.09834248582134024
LOSS is 0.10042023806910341
LOSS is 0.10066552956045295
LOSS is 0.104312527460667
LOSS is 0.1028193619203133
LOSS is 0.10321521870248641
LOSS is 0.10087934659483533
LOSS is 0.10042855526863909
LOSS is 0.10185281482758002
LOSS is 0.0997751268465072
LOSS is 0.1012493297783658
LOSS is 0.10250490175948168
LOSS is 0.10063810717547313
LOSS is 0.09638957512487348
LOSS is 0.09804180242742103
LOSS is 0.09941610370995478
LOSS is 0.0997595649251404
LOSS is 0.09746746365213767
LOSS is 0.10001101984021565
LOSS is 0.10251011941737186
Epoch: [1][200/2183]	Per Sample Total Time 2.01328	Per Sample Data Time 1.99599	Per Sample DNN Time 0.01729	Train Loss 0.1442	
LOSS is 0.09588414852662633
LOSS is 0.09632663327346867
LOSS is 0.09550010219759619
LOSS is 0.09805163184491297
LOSS is 0.09809720582406348
LOSS is 0.09646759759091462
LOSS is 0.09868710301506023
LOSS is 0.10235352511672925
LOSS is 0.09508853256314373
LOSS is 0.09694872485240923
LOSS is 0.09269599500810728
LOSS is 0.09548031448076169
LOSS is 0.09931334605595718
LOSS is 0.0952898622971649
LOSS is 0.09141748257990306
LOSS is 0.09294082458519067
LOSS is 0.09385112093606343
LOSS is 0.09208875871573886
LOSS is 0.09133537174202502
LOSS is 0.09382519536574062
LOSS is 0.09334946495791277
LOSS is 0.09147010835663727
LOSS is 0.08970109655987472
LOSS is 0.0957792383284929
LOSS is 0.09009861269112056
LOSS is 0.09336579786225534
LOSS is 0.09146631924978768
LOSS is 0.0897766490164213
LOSS is 0.09339274494365479
LOSS is 0.09176485625328497
LOSS is 0.09054181813883286
LOSS is 0.0931771781668067
LOSS is 0.09133275981526823
LOSS is 0.0925377104858247
LOSS is 0.09290796467879167
LOSS is 0.0898928684857674
LOSS is 0.09073744305409491
LOSS is 0.09219269830617123
LOSS is 0.09584139631440243
LOSS is 0.08813488342411196
LOSS is 0.08564657606106874
LOSS is 0.09154308060339342
LOSS is 0.08911922846338713
LOSS is 0.08774444274643126
LOSS is 0.09160223144378203
LOSS is 0.08624846504574332
LOSS is 0.08975804302259349
LOSS is 0.08749408645361352
LOSS is 0.0871897756207424
LOSS is 0.0914146539514574
LOSS is 0.0889244648294213
LOSS is 0.08732721109253665
LOSS is 0.08478065372832741
LOSS is 0.08511748986861979
LOSS is 0.08879686185702061
LOSS is 0.08748399800543363
LOSS is 0.08913223373548439
LOSS is 0.08306831106233101
LOSS is 0.08464047839088987
LOSS is 0.08670835369227765
LOSS is 0.08576725388333822
LOSS is 0.08531118830433115
LOSS is 0.08740163492659728
LOSS is 0.08506360347149894
LOSS is 0.08627478687674739
LOSS is 0.08560770784349492
LOSS is 0.08572764407804546
LOSS is 0.08714569804413866
LOSS is 0.08391628121918378
LOSS is 0.0841141033374394
LOSS is 0.08777662558830343
LOSS is 0.08790720110875555
LOSS is 0.08494916111851733
LOSS is 0.08720793668491146
LOSS is 0.08364131676848047
LOSS is 0.08220374926924706
LOSS is 0.0808662481365415
LOSS is 0.08197959435832067
LOSS is 0.08244784542359412
LOSS is 0.08012673689673344
LOSS is 0.08411309157575791
LOSS is 0.0868648645200301
LOSS is 0.08125530802101519
LOSS is 0.07835632337839343
LOSS is 0.0801109232668144
LOSS is 0.08304733915021643
LOSS is 0.0849704865901731
LOSS is 0.07633188845201706
LOSS is 0.07801733226243718
LOSS is 0.08400194594791781
LOSS is 0.08815213599785542
LOSS is 0.08451543837203644
LOSS is 0.08527312830789015
LOSS is 0.08371490631019697
LOSS is 0.07852084635562885
LOSS is 0.0820495581937333
LOSS is 0.08291586168886474
LOSS is 0.07910773137002253
LOSS is 0.07609168344332527
LOSS is 0.07880772497039289
Epoch: [1][300/2183]	Per Sample Total Time 2.81611	Per Sample Data Time 2.79922	Per Sample DNN Time 0.01689	Train Loss 0.1257	
LOSS is 0.0765412411148039
LOSS is 0.0809188780541687
LOSS is 0.08297713492764161
LOSS is 0.07379829648455295
LOSS is 0.07812834675462607
LOSS is 0.07737010418903083
LOSS is 0.08180309936559449
LOSS is 0.08017040109960363
LOSS is 0.08102137814120701
LOSS is 0.07874948569301826
LOSS is 0.08046874306785565
LOSS is 0.07534676149720326
LOSS is 0.07641449091994824
LOSS is 0.07848624967892344
LOSS is 0.07973930527611325
LOSS is 0.07313909983960912
LOSS is 0.07372195431108898
LOSS is 0.07553831165617643
LOSS is 0.07134254466276617
LOSS is 0.08101063868651788
LOSS is 0.07674121516213442
LOSS is 0.08260427200351843
LOSS is 0.08037806225203288
LOSS is 0.07858986298010374
LOSS is 0.07904278688249179
LOSS is 0.08079951662609043
LOSS is 0.07763507485079268
LOSS is 0.0784194080263842
LOSS is 0.07742859812259364
LOSS is 0.07674648080952466
LOSS is 0.07599493639194407
LOSS is 0.07624339373510641
LOSS is 0.07807059556129389
LOSS is 0.07386544958300269
LOSS is 0.07368407293222845
LOSS is 0.07331506082438864
LOSS is 0.07630309658241459
LOSS is 0.07537586565944367
LOSS is 0.07154913312018228
LOSS is 0.07614965206865842
LOSS is 0.08194113455247135
LOSS is 0.08221885833345975
LOSS is 0.07261370545912844
LOSS is 0.07228477978768448
LOSS is 0.06992685525290047
LOSS is 0.07613968377001584
LOSS is 0.07558673872496002
LOSS is 0.07868949232622982
LOSS is 0.07203043801826425
LOSS is 0.07721163173400176
LOSS is 0.07007732923142612
LOSS is 0.07515276948339307
LOSS is 0.07768830749167441
LOSS is 0.0704749979521148
LOSS is 0.0715627870161552
LOSS is 0.0668541451159399
LOSS is 0.07286666947261741
LOSS is 0.07036770705715753
LOSS is 0.06873474284696082
LOSS is 0.07404692748289866
LOSS is 0.0726532267833439
LOSS is 0.06848400656599551
LOSS is 0.07291186911558423
LOSS is 0.0783702636952512
LOSS is 0.07573083470420292
LOSS is 0.06445516909899501
LOSS is 0.0765992872886515
LOSS is 0.07360133086098358
LOSS is 0.07218791163216035
LOSS is 0.0731305219399898
LOSS is 0.07290138455185419
LOSS is 0.07246367444167845
LOSS is 0.06996887698885985
LOSS is 0.06666520848210591
LOSS is 0.0736367334460374
LOSS is 0.07050494917746013
LOSS is 0.07544438857701607
LOSS is 0.06530646618339234
LOSS is 0.07435666541918182
LOSS is 0.07331137202253256
LOSS is 0.06982996133233732
LOSS is 0.0759134919960828
LOSS is 0.06789442988384205
LOSS is 0.07346967896989857
LOSS is 0.07160480922049223
LOSS is 0.07198492866319915
LOSS is 0.07291583421252047
LOSS is 0.07066981529739376
LOSS is 0.06709590560911845
LOSS is 0.07445630353215772
LOSS is 0.07014919053646736
LOSS is 0.06448657297218839
LOSS is 0.06860211956858014
LOSS is 0.07076559583152024
LOSS is 0.06914149750528548
LOSS is 0.07110355556360447
LOSS is 0.06463706625780712
LOSS is 0.06830278613294165
LOSS is 0.06793354942191702
LOSS is 0.06784055735217408
Epoch: [1][400/2183]	Per Sample Total Time 3.62121	Per Sample Data Time 3.60452	Per Sample DNN Time 0.01669	Train Loss 0.1128	
LOSS is 0.06164560928048256
LOSS is 0.06999394277265916
LOSS is 0.06878225203991557
LOSS is 0.0678960937622469
LOSS is 0.06444600060582162
LOSS is 0.07181287780947362
LOSS is 0.06889190390938893
LOSS is 0.07373140399421876
LOSS is 0.0675049124925863
LOSS is 0.06799494279005255
LOSS is 0.0705070978422494
LOSS is 0.07197321239470815
LOSS is 0.0648738831980154
LOSS is 0.06542551349964924
LOSS is 0.06693795610801317
LOSS is 0.06494045636849478
LOSS is 0.06874968612683006
LOSS is 0.07106587888789363
LOSS is 0.0663850778931131
LOSS is 0.06205300910243144
LOSS is 0.06575517604787214
LOSS is 0.06318787258894494
LOSS is 0.06531533519659813
LOSS is 0.06989298133994454
LOSS is 0.06764721261686646
LOSS is 0.06165854556369595
LOSS is 0.06582353002508172
LOSS is 0.06435674004179115
LOSS is 0.06484143934329041
LOSS is 0.07176746308493118
LOSS is 0.06587074080404516
LOSS is 0.06890912550889577
LOSS is 0.06791576775761012
LOSS is 0.0656633520553199
LOSS is 0.06327659627383886
LOSS is 0.05964731924701482
LOSS is 0.06613382964705428
LOSS is 0.06754974535976847
LOSS is 0.06465985143013919
LOSS is 0.06362063065287657
LOSS is 0.06806061307007137
LOSS is 0.06838967556444307
LOSS is 0.06476978603634052
LOSS is 0.06571331918045568
LOSS is 0.07116194433396837
LOSS is 0.058870579588304585
LOSS is 0.06292226541709776
LOSS is 0.06392085426215394
LOSS is 0.06926825697378566
LOSS is 0.05831898438278586
LOSS is 0.061091495029007396
LOSS is 0.05644483536249027
LOSS is 0.06883488475267466
LOSS is 0.0652210286273233
LOSS is 0.06263873041879076
LOSS is 0.060560647234863915
LOSS is 0.06699150644708425
LOSS is 0.05982641596385899
LOSS is 0.07231875011192945
LOSS is 0.06482453297047566
LOSS is 0.06309655602245282
LOSS is 0.06188735044755352
LOSS is 0.05916768739698455
LOSS is 0.06432471186039039
LOSS is 0.06284165861007447
LOSS is 0.06410210443214359
LOSS is 0.057735792280873284
LOSS is 0.05877103416443182
LOSS is 0.07138582922789889
LOSS is 0.063613137009476
LOSS is 0.05813433196744882
LOSS is 0.06355495697391841
LOSS is 0.06702532776185156
LOSS is 0.06299119451199658
LOSS is 0.061457854992865275
LOSS is 0.0655978055643694
LOSS is 0.06789057249707792
LOSS is 0.06060928658815101
LOSS is 0.06055236174569776
LOSS is 0.05204973692966936
LOSS is 0.06560151451383718
LOSS is 0.05875502484811781
LOSS is 0.06709184046543669
LOSS is 0.0642467072584744
LOSS is 0.06876601027324797
LOSS is 0.06134899551553342
LOSS is 0.06749249776398453
LOSS is 0.06273865180516926
LOSS is 0.057311722337811566
LOSS is 0.06214133879169822
LOSS is 0.06253192206146196
LOSS is 0.05870046423825746
LOSS is 0.060330352542223416
LOSS is 0.05881655731393646
LOSS is 0.06205562654766254
LOSS is 0.06410039459044735
LOSS is 0.062443937905287995
LOSS is 0.05625687751104124
LOSS is 0.05468639517474609
LOSS is 0.0575479051466876
Epoch: [1][500/2183]	Per Sample Total Time 4.42452	Per Sample Data Time 4.40798	Per Sample DNN Time 0.01654	Train Loss 0.1031	
LOSS is 0.05928162282293973
LOSS is 0.05987352570945708
LOSS is 0.06373444437631406
LOSS is 0.05824881924044652
LOSS is 0.057832170707794534
LOSS is 0.060178786992716296
LOSS is 0.05942859840501721
LOSS is 0.06230227766248087
LOSS is 0.06474335862944523
LOSS is 0.06066950845221679
LOSS is 0.06217219546553679
LOSS is 0.049038016188424086
LOSS is 0.05388016569428146
LOSS is 0.05762488659432469
LOSS is 0.05652007956251813
LOSS is 0.05839283024989224
LOSS is 0.060001654013952556
LOSS is 0.06478262452369866
LOSS is 0.06719502860078744
LOSS is 0.05768091092448837
LOSS is 0.059507086071923065
LOSS is 0.06140363469564667
LOSS is 0.05943774603152027
LOSS is 0.05774600230273791
LOSS is 0.05715488560070905
LOSS is 0.05654588609778633
LOSS is 0.06275891826061222
LOSS is 0.05264873960036009
LOSS is 0.06019232839151907
LOSS is 0.057068946232902824
LOSS is 0.059466513981654624
LOSS is 0.0607985080650542
LOSS is 0.06491975243766017
LOSS is 0.05739881672159148
LOSS is 0.05071016468573362
LOSS is 0.05887185308034532
LOSS is 0.05693777030915954
LOSS is 0.060932662897588065
LOSS is 0.0525442201908057
LOSS is 0.055759872726630424
LOSS is 0.055203289923374546
LOSS is 0.05829896509375734
LOSS is 0.061351123710240554
LOSS is 0.05012827454328848
LOSS is 0.058957709017752984
LOSS is 0.05557774600146028
LOSS is 0.063074965205354
LOSS is 0.056149029019094694
LOSS is 0.05852214266856512
LOSS is 0.04913537650795964
LOSS is 0.05622902419883758
LOSS is 0.06350286393135321
LOSS is 0.05658188663752905
LOSS is 0.05282136866395983
LOSS is 0.061651099243317735
LOSS is 0.05911821168944395
LOSS is 0.05956281370929598
LOSS is 0.05719565931047934
LOSS is 0.05471451359141308
LOSS is 0.04925283804361243
LOSS is 0.05193875021922092
LOSS is 0.05544454270391725
LOSS is 0.05850530941136337
LOSS is 0.0565578434872441
LOSS is 0.04793905364853951
LOSS is 0.06035415143550685
LOSS is 0.060204059302923274
LOSS is 0.05932163140445482
LOSS is 0.057384248036930034
LOSS is 0.049484711832871356
LOSS is 0.0522177611188575
LOSS is 0.06199852111109067
LOSS is 0.056424563830563185
LOSS is 0.05105771077796817
LOSS is 0.05276313706902632
LOSS is 0.05346280947444029
LOSS is 0.06020636667245223
LOSS is 0.056078678121557465
LOSS is 0.051803064777244195
LOSS is 0.060627732448435084
LOSS is 0.0548881757092507
LOSS is 0.05611311163927894
LOSS is 0.04864184417451422
LOSS is 0.05396943338137741
LOSS is 0.04973679113783874
LOSS is 0.05040087862425329
LOSS is 0.057203005457025335
LOSS is 0.06482092085042193
LOSS is 0.049420233698911034
LOSS is 0.06390739427782441
LOSS is 0.05959637100614297
LOSS is 0.047990864758418565
LOSS is 0.059742418682241505
LOSS is 0.05391454200318549
LOSS is 0.0527977379776227
LOSS is 0.05361065804764319
LOSS is 0.059905939984600996
LOSS is 0.05382648370325721
LOSS is 0.05946938894824901
LOSS is 0.054018844764990116
Epoch: [1][600/2183]	Per Sample Total Time 5.22719	Per Sample Data Time 5.21074	Per Sample DNN Time 0.01645	Train Loss 0.0955	
LOSS is 0.05080384776054416
LOSS is 0.051459914170942894
LOSS is 0.05415465932358833
LOSS is 0.050471421596788184
LOSS is 0.053905634380062115
LOSS is 0.05327860172896181
LOSS is 0.04693226827308536
LOSS is 0.05198755171208177
LOSS is 0.05306968716904521
LOSS is 0.05380469330586493
LOSS is 0.054287341953992534
LOSS is 0.05664202943715888
LOSS is 0.048470644138287756
LOSS is 0.059196822332839176
LOSS is 0.05067170543693161
LOSS is 0.053165889764592675
LOSS is 0.055739314224726215
LOSS is 0.05487064585904591
LOSS is 0.049304815940752936
LOSS is 0.05238021189152884
LOSS is 0.05441679146800501
LOSS is 0.048622447027786016
LOSS is 0.05783915611158591
LOSS is 0.05095028294987666
LOSS is 0.05568017789026878
LOSS is 0.049313068840807925
LOSS is 0.05402351226133761
LOSS is 0.05409499257783561
LOSS is 0.05312246858239329
LOSS is 0.05453531092808893
LOSS is 0.055833843539973414
LOSS is 0.05357500692422036
LOSS is 0.05039845071403155
LOSS is 0.05184694965777453
LOSS is 0.05099862478712264
LOSS is 0.05324440668104217
LOSS is 0.0546527148692015
LOSS is 0.04874226295408638
LOSS is 0.04212089196002732
LOSS is 0.05518217431769396
LOSS is 0.056423872609351146
LOSS is 0.05805667160563947
LOSS is 0.05743892081760957
LOSS is 0.04901314535023024
LOSS is 0.050339947662626706
LOSS is 0.05425373570120428
LOSS is 0.04966872081102338
LOSS is 0.0512419072870398
LOSS is 0.0549704902729718
LOSS is 0.04804898277119113
LOSS is 0.05558527406945359
LOSS is 0.05159255992951027
LOSS is 0.05446515138319228
LOSS is 0.05638270472253984
LOSS is 0.055355486048695945
LOSS is 0.05068823251485204
LOSS is 0.053299489046912646
LOSS is 0.046801619670198615
LOSS is 0.05504057741723955
LOSS is 0.052756800108666846
LOSS is 0.05638444838424524
LOSS is 0.05371326803035724
LOSS is 0.04890608925760413
LOSS is 0.046625617157551466
LOSS is 0.04961805085224721
LOSS is 0.041825426592452766
LOSS is 0.0561673368808503
LOSS is 0.042724228390531305
LOSS is 0.05129467139951885
LOSS is 0.048783587258658376
LOSS is 0.05106578907580115
LOSS is 0.056763261427016314
LOSS is 0.043947872099233794
LOSS is 0.05434955252586709
LOSS is 0.04338232421432622
LOSS is 0.047954906022059735
LOSS is 0.04804908941461084
LOSS is 0.04689088097792895
LOSS is 0.04602180887944997
LOSS is 0.04785357559002781
LOSS is 0.04695764566616466
LOSS is 0.040780124756662796
LOSS is 0.049986744477452404
LOSS is 0.046729917111806574
LOSS is 0.047223309072433044
LOSS is 0.051603880291416625
LOSS is 0.047646707942476496
LOSS is 0.041316756313899534
LOSS is 0.04986631920367169
LOSS is 0.053010589374462146
LOSS is 0.05396485550736543
LOSS is 0.056361736826947895
LOSS is 0.046262813669745814
LOSS is 0.04534566958396075
LOSS is 0.053107020343110586
LOSS is 0.04559418602380902
LOSS is 0.04762675468208424
LOSS is 0.050210817659390165
LOSS is 0.05036480129754637
LOSS is 0.0462501883006189
Epoch: [1][700/2183]	Per Sample Total Time 6.03029	Per Sample Data Time 6.01390	Per Sample DNN Time 0.01639	Train Loss 0.0892	
LOSS is 0.04601047138838719
LOSS is 0.051237053098739126
LOSS is 0.04944214787431217
LOSS is 0.0426721029226125
LOSS is 0.060974344369606116
LOSS is 0.04661590875941329
LOSS is 0.04654169883734236
LOSS is 0.05239470208218942
LOSS is 0.05025803260854446
LOSS is 0.05336862245438776
LOSS is 0.04900863127491903
LOSS is 0.0478952469022867
LOSS is 0.04941260010644328
LOSS is 0.047266657912211185
LOSS is 0.048042756488430316
LOSS is 0.04766940618981608
LOSS is 0.04635997083503753
LOSS is 0.04488177111120119
LOSS is 0.052632610727256786
LOSS is 0.050521719148188524
LOSS is 0.04537669215118513
LOSS is 0.04975642009036771
LOSS is 0.043941753676044755
LOSS is 0.050390205369719
LOSS is 0.05294283200773255
LOSS is 0.04809312360652257
LOSS is 0.052717781819713615
LOSS is 0.04679185221573183
LOSS is 0.0506041264772648
LOSS is 0.04907319792197086
LOSS is 0.051049183988555645
LOSS is 0.04634068335018431
LOSS is 0.05132971905773351
LOSS is 0.043046653801963354
LOSS is 0.0510611605734448
LOSS is 0.04144739576314654
LOSS is 0.05517783409100958
LOSS is 0.04715012509434018
LOSS is 0.039795049545840205
LOSS is 0.042214278588362505
LOSS is 0.050630675289624685
LOSS is 0.048588633149047385
LOSS is 0.054403149216086605
LOSS is 0.04883836850960507
LOSS is 0.04379798878352934
LOSS is 0.04600432766834274
LOSS is 0.04568756647621437
LOSS is 0.05184732438763604
LOSS is 0.05936519291057873
LOSS is 0.05406155111209955
LOSS is 0.05417550359716795
LOSS is 0.04835116207463822
LOSS is 0.045457311986635135
LOSS is 0.039588815172319304
LOSS is 0.04667089530548159
LOSS is 0.05317698437216071
LOSS is 0.04618881361927682
LOSS is 0.049788628823783564
LOSS is 0.04013450058885307
LOSS is 0.050775859951972965
LOSS is 0.04561499958421337
LOSS is 0.05046526622027159
LOSS is 0.049155009366610714
LOSS is 0.047871708783980775
LOSS is 0.045666922826397546
LOSS is 0.05548725038980289
LOSS is 0.05079911359896263
LOSS is 0.04697384235546148
LOSS is 0.04906005399864322
LOSS is 0.04623215134410809
LOSS is 0.04298006999519809
LOSS is 0.04823946554097347
LOSS is 0.053382849823295456
LOSS is 0.05326961700591104
LOSS is 0.03588311940373387
LOSS is 0.048071878196205946
LOSS is 0.04943445552373305
LOSS is 0.042217481240513735
LOSS is 0.0492501382836296
LOSS is 0.05472826752792268
LOSS is 0.045136246137844865
LOSS is 0.05038745599197379
LOSS is 0.04471047466426778
LOSS is 0.04026496991495757
LOSS is 0.043996599433885425
LOSS is 0.04562361429484251
LOSS is 0.05056892841763329
LOSS is 0.04782480613619555
LOSS is 0.05003527261627218
LOSS is 0.0498063309766197
LOSS is 0.04020090423960938
LOSS is 0.04693069844836525
LOSS is 0.04791341191642763
LOSS is 0.05120311586069875
LOSS is 0.038437000332536025
LOSS is 0.050552747302184196
LOSS is 0.04630049680640999
LOSS is 0.04661698784543357
LOSS is 0.04985435511654942
LOSS is 0.047007799893617634
Epoch: [1][800/2183]	Per Sample Total Time 6.83271	Per Sample Data Time 6.81636	Per Sample DNN Time 0.01635	Train Loss 0.0841	
LOSS is 0.04321030162381552
LOSS is 0.045000652142916805
LOSS is 0.044408369339265244
LOSS is 0.05122109237883706
LOSS is 0.04149473929341184
LOSS is 0.04413815062881137
LOSS is 0.04204736096658356
LOSS is 0.03999017977825133
LOSS is 0.04546333783849453
LOSS is 0.05168035871698522
LOSS is 0.04928253065949927
LOSS is 0.04613907650151911
LOSS is 0.0440047568109973
LOSS is 0.0476072604424553
LOSS is 0.05154994519922184
LOSS is 0.04156554116915989
LOSS is 0.05727556099785337
LOSS is 0.046556018670089544
LOSS is 0.039733471564832146
LOSS is 0.05026840865010551
LOSS is 0.049529273992714784
LOSS is 0.04154406370245852
LOSS is 0.05362946110447714
LOSS is 0.04435617943478671
LOSS is 0.045834416762615245
LOSS is 0.04237877470994136
LOSS is 0.04899443107909368
LOSS is 0.04268420797239136
LOSS is 0.04538853542403862
LOSS is 0.04711788292935429
LOSS is 0.053090489254585316
LOSS is 0.0409826491206574
LOSS is 0.05008135805925122
LOSS is 0.04691798464744352
LOSS is 0.04794769145247604
LOSS is 0.05255097208272976
LOSS is 0.04594997754184685
LOSS is 0.04605581810581498
LOSS is 0.04352306017448428
LOSS is 0.04980697429835952
LOSS is 0.04703750862720578
LOSS is 0.04216209589901458
LOSS is 0.03926007483203042
LOSS is 0.049074586119483385
LOSS is 0.046934826475820354
LOSS is 0.041166486446939723
LOSS is 0.049913811052101666
LOSS is 0.043610578710407334
LOSS is 0.04863841150431351
LOSS is 0.03615833037455256
LOSS is 0.043484680486920606
LOSS is 0.04720876759800983
LOSS is 0.039092446132272016
LOSS is 0.040888960413770596
LOSS is 0.04843819548802761
LOSS is 0.0486801272262043
LOSS is 0.044976806072033165
LOSS is 0.04006324051122647
LOSS is 0.04848189502197784
LOSS is 0.0478493996396234
LOSS is 0.04464694689561535
LOSS is 0.04579130349797197
LOSS is 0.04243012500141049
LOSS is 0.0475960914970104
LOSS is 0.04446575253910851
LOSS is 0.04768762582263056
LOSS is 0.04483458839085264
LOSS is 0.04250034544616938
LOSS is 0.043143759953867024
LOSS is 0.04454573964215039
LOSS is 0.04913159430085216
LOSS is 0.043674658129360375
LOSS is 0.03818945217586588
LOSS is 0.037908591299298376
LOSS is 0.04015693744896756
LOSS is 0.04129532062933625
LOSS is 0.03903623887163121
LOSS is 0.045470874669457165
LOSS is 0.04615649828929842
LOSS is 0.04034035275311908
LOSS is 0.03744080819878339
LOSS is 0.043957356631872245
LOSS is 0.043665565349510875
LOSS is 0.04874112172653744
LOSS is 0.0433907528098401
LOSS is 0.05470245471243592
LOSS is 0.03947871500868739
LOSS is 0.04158196933819758
LOSS is 0.04322067391020634
LOSS is 0.04068715903713989
LOSS is 0.04239738246891648
LOSS is 0.03901713996824886
LOSS is 0.05276724757975899
LOSS is 0.04439589780871757
LOSS is 0.05015989785402781
LOSS is 0.048458255312192104
LOSS is 0.0417134503085011
LOSS is 0.0434625174147853
LOSS is 0.04455880554024286
LOSS is 0.04411579803272617
Epoch: [1][900/2183]	Per Sample Total Time 7.63615	Per Sample Data Time 7.61983	Per Sample DNN Time 0.01632	Train Loss 0.0797	
LOSS is 0.03957494777763108
LOSS is 0.04680878405401018
LOSS is 0.03590234819760857
LOSS is 0.04203655980343077
LOSS is 0.044303163653918703
LOSS is 0.04579344403163608
LOSS is 0.043910385038082804
LOSS is 0.049679492210464866
LOSS is 0.045386462828319055
LOSS is 0.04757582591390626
LOSS is 0.04969522814567123
LOSS is 0.039362830034806395
LOSS is 0.05154068026226014
LOSS is 0.036149869089810334
LOSS is 0.04094300544306558
LOSS is 0.04409950377875551
LOSS is 0.040851039997651245
LOSS is 0.03709348509194873
LOSS is 0.04100952830340248
LOSS is 0.04469519017477675
LOSS is 0.03446888041372101
LOSS is 0.03552833504189039
LOSS is 0.03915528918703785
LOSS is 0.04604047338429761
LOSS is 0.0449157031068656
LOSS is 0.039758351856144146
LOSS is 0.04859036088659196
LOSS is 0.042315685393987224
LOSS is 0.046809767358063255
LOSS is 0.033779777419016076
LOSS is 0.04394238864750757
LOSS is 0.0432690845654967
LOSS is 0.04004988811415387
LOSS is 0.04304561486642342
LOSS is 0.039882193568531274
LOSS is 0.04183323469886091
LOSS is 0.04038749868593489
LOSS is 0.03839537253273496
LOSS is 0.04308302138854439
LOSS is 0.05202805510828815
LOSS is 0.038499593273542514
LOSS is 0.04521726642153226
LOSS is 0.04698430493153865
LOSS is 0.03968548409427361
LOSS is 0.040456966247099144
LOSS is 0.042485138003927815
LOSS is 0.03975114724967474
LOSS is 0.036730110145193375
LOSS is 0.04995482501088797
LOSS is 0.051520412869964886
LOSS is 0.04997984062142981
LOSS is 0.04132234865120457
LOSS is 0.04029498035359817
LOSS is 0.04502698819269427
LOSS is 0.034770016079225266
LOSS is 0.042495462157142665
LOSS is 0.04987384981039213
LOSS is 0.04770553354319418
LOSS is 0.04759300425512872
LOSS is 0.048799309145057736
LOSS is 0.04746633112517884
LOSS is 0.0481880758452462
LOSS is 0.03827908220109142
LOSS is 0.03067905911060128
LOSS is 0.03429120457032696
LOSS is 0.046401209985294074
LOSS is 0.03290859039426626
LOSS is 0.04050740275376787
LOSS is 0.041907183178700506
LOSS is 0.052312506666057745
LOSS is 0.038534785243100486
LOSS is 0.04409921588182139
LOSS is 0.046550202564491584
LOSS is 0.04727824253806224
LOSS is 0.04182827564751885
LOSS is 0.04423974850583667
LOSS is 0.04419329788118679
LOSS is 0.038953300367769166
LOSS is 0.041420336080482234
LOSS is 0.03534943456227969
LOSS is 0.039403996644153574
LOSS is 0.03686152617951545
LOSS is 0.04237228204331284
LOSS is 0.03983453853104341
LOSS is 0.048933554417356695
LOSS is 0.04319727846886963
LOSS is 0.03720323931949679
LOSS is 0.03691171164944535
LOSS is 0.04023573466246793
LOSS is 0.04145988358204098
LOSS is 0.04093715208621385
LOSS is 0.04646648429402073
LOSS is 0.04529587383323815
LOSS is 0.039184704202004164
LOSS is 0.04295817764369228
LOSS is 0.04557178224184706
LOSS is 0.04504695559492878
LOSS is 0.04099317214684561
LOSS is 0.03594610868186768
LOSS is 0.039535765388185005
Epoch: [1][1000/2183]	Per Sample Total Time 8.44090	Per Sample Data Time 8.42460	Per Sample DNN Time 0.01631	Train Loss 0.0760	
LOSS is 0.04036286053849229
LOSS is 0.033708171174027184
LOSS is 0.04325391315913294
LOSS is 0.05248211531123768
LOSS is 0.0460796346111844
LOSS is 0.0392895351331875
LOSS is 0.03744178171492725
LOSS is 0.03937527117552236
LOSS is 0.04158935150083077
LOSS is 0.03315612304771397
LOSS is 0.04077062526659574
LOSS is 0.04106013321763991
LOSS is 0.03803572313753345
LOSS is 0.02794474935314308
LOSS is 0.043035554019734266
LOSS is 0.039215852856965897
LOSS is 0.03864074958565956
LOSS is 0.04072439831720355
LOSS is 0.042754705013358034
LOSS is 0.04655469214136247
LOSS is 0.03665711764188018
LOSS is 0.035207567104856334
LOSS is 0.04023067606845871
LOSS is 0.039893541093818694
LOSS is 0.04181383503057684
LOSS is 0.040362637744498595
LOSS is 0.041257772472648266
LOSS is 0.03415036903005481
LOSS is 0.04778416218274894
LOSS is 0.04577227346899842
LOSS is 0.03783000989147695
LOSS is 0.04195466194010806
LOSS is 0.0406902155171459
LOSS is 0.0443000532235601
LOSS is 0.04242042585111146
LOSS is 0.043497608716231
LOSS is 0.0441169176948218
LOSS is 0.03663300185852374
LOSS is 0.03734998150049554
LOSS is 0.04397024915039462
LOSS is 0.04100975309304583
LOSS is 0.03182860385044478
LOSS is 0.037822109366631294
LOSS is 0.03690133120369865
LOSS is 0.03499846662666339
LOSS is 0.037947516127217876
LOSS is 0.037053311344449565
LOSS is 0.03498422803308737
LOSS is 0.038800562440010254
LOSS is 0.0406557487063886
LOSS is 0.03391769575042417
LOSS is 0.04234429753589211
LOSS is 0.0382209935769788
LOSS is 0.03153071375670455
LOSS is 0.04275864565871113
LOSS is 0.04120201751754697
LOSS is 0.03777804330272678
LOSS is 0.03183166053427461
LOSS is 0.04458745278844921
LOSS is 0.03922507438862037
LOSS is 0.033388856083038265
LOSS is 0.03287882732256549
LOSS is 0.043856168078491464
LOSS is 0.0386385648441501
LOSS is 0.039269523886323446
LOSS is 0.044332245702195607
LOSS is 0.033040006971374776
LOSS is 0.03872089394416738
LOSS is 0.04404268386841674
LOSS is 0.042437630984835176
LOSS is 0.03826925789238885
LOSS is 0.03825653631036403
LOSS is 0.041150584475447734
LOSS is 0.03712093018005059
LOSS is 0.03461811945822168
LOSS is 0.04093370617027783
LOSS is 0.0369755675084889
LOSS is 0.03461436593604352
LOSS is 0.039942845868257186
LOSS is 0.033457061508185386
LOSS is 0.04739971053544044
LOSS is 0.04146359873644542
LOSS is 0.03914838559333778
LOSS is 0.04160430376699272
LOSS is 0.029669318112816354
LOSS is 0.03607153832524394
LOSS is 0.04383636119164294
LOSS is 0.038852269650136204
LOSS is 0.031206475307117218
LOSS is 0.043699288175897286
LOSS is 0.04354154544950386
LOSS is 0.04723228845619209
LOSS is 0.04499843221177192
LOSS is 0.037757836162636525
LOSS is 0.035808960495536064
LOSS is 0.04157869321332934
LOSS is 0.032728674282940726
LOSS is 0.03551718994781065
LOSS is 0.029716085671486023
LOSS is 0.04573833820283957
Epoch: [1][1100/2183]	Per Sample Total Time 9.24670	Per Sample Data Time 9.23040	Per Sample DNN Time 0.01629	Train Loss 0.0727	
LOSS is 0.04049327881143351
LOSS is 0.03575803058668195
LOSS is 0.03560297094411605
LOSS is 0.03546828284015646
LOSS is 0.037643488319784715
LOSS is 0.030264677375962494
LOSS is 0.037807361257534165
LOSS is 0.04295799394176963
LOSS is 0.04657325865109063
LOSS is 0.03169948591840997
LOSS is 0.040477602300622195
LOSS is 0.038811801876581745
LOSS is 0.04226093572894267
LOSS is 0.04416704075939682
LOSS is 0.0422613770813526
LOSS is 0.03458830519433832
LOSS is 0.04262079620801766
LOSS is 0.04122689420648385
LOSS is 0.04021033655192393
LOSS is 0.03733155059007307
LOSS is 0.03955768184976478
LOSS is 0.03554248509045768
LOSS is 0.03255387581108759
LOSS is 0.040104850974554816
LOSS is 0.03734148285972576
LOSS is 0.036390088263627454
LOSS is 0.03874036850039071
LOSS is 0.04124485086882487
LOSS is 0.03917832441579473
LOSS is 0.04201059954493152
LOSS is 0.05163669073284837
LOSS is 0.030246978312983026
LOSS is 0.047686266913272754
LOSS is 0.04450519856870718
LOSS is 0.038303316582751
LOSS is 0.04325316030425408
LOSS is 0.04422661419065359
LOSS is 0.04093102682430375
LOSS is 0.04327588764145427
LOSS is 0.032235715513816106
LOSS is 0.03204428658898299
LOSS is 0.04412905576008295
LOSS is 0.04105685345780027
LOSS is 0.04249770875719454
LOSS is 0.03686541146327121
LOSS is 0.03656793545077865
LOSS is 0.03614276631861382
LOSS is 0.03943373945706602
LOSS is 0.03405533322880122
LOSS is 0.04299016357486835
LOSS is 0.036597123545070644
LOSS is 0.037089723088235285
LOSS is 0.034049333341633126
LOSS is 0.03797170120087685
LOSS is 0.03731679128759424
LOSS is 0.03303266124605822
LOSS is 0.02983138910417135
LOSS is 0.03542745418768997
LOSS is 0.046183157664393855
LOSS is 0.032870474035056156
LOSS is 0.037420261953375306
LOSS is 0.03532760525209597
LOSS is 0.03475610306893941
LOSS is 0.03445357400700838
LOSS is 0.03732517299572161
LOSS is 0.03217437559476821
LOSS is 0.04075197516591288
LOSS is 0.041618046791748686
LOSS is 0.038962351643034104
LOSS is 0.04111908901055965
LOSS is 0.029247388244645362
LOSS is 0.03916810556887261
LOSS is 0.03713217105580649
LOSS is 0.03465865269943606
LOSS is 0.034797912992847464
LOSS is 0.030656888476272192
LOSS is 0.036552167476232474
LOSS is 0.03712429409924274
LOSS is 0.030900096452678556
LOSS is 0.037998626021435486
LOSS is 0.04201665693404114
LOSS is 0.03132575195408815
LOSS is 0.026746563235258997
LOSS is 0.031241251894486295
LOSS is 0.042995543398913774
LOSS is 0.0299643857475409
LOSS is 0.04075286799964185
LOSS is 0.03622000270857825
LOSS is 0.033693086264247545
LOSS is 0.032478515517141204
LOSS is 0.03805028906572261
LOSS is 0.047574692836108934
LOSS is 0.040467840748412226
LOSS is 0.027357888341551492
LOSS is 0.03594430674274918
LOSS is 0.03832595289801247
LOSS is 0.04001032775374673
LOSS is 0.03319808788840116
LOSS is 0.041177472439109505
LOSS is 0.04305485961570715
Epoch: [1][1200/2183]	Per Sample Total Time 10.05252	Per Sample Data Time 10.03625	Per Sample DNN Time 0.01627	Train Loss 0.0698	
LOSS is 0.02931555923326717
LOSS is 0.039720937524980404
LOSS is 0.03547970558264448
LOSS is 0.03461689798112881
LOSS is 0.0358397121510158
LOSS is 0.03622810431174003
LOSS is 0.03221210469928337
LOSS is 0.042535210775289066
LOSS is 0.03274484046191598
LOSS is 0.033795808320671014
LOSS is 0.03079763306353319
LOSS is 0.04312398331128255
LOSS is 0.03732168083452659
LOSS is 0.03643032645035419
LOSS is 0.031304711313471976
LOSS is 0.03526177943742369
LOSS is 0.041904737159299356
LOSS is 0.036465838468041814
LOSS is 0.0401623880880167
LOSS is 0.036832445346905544
LOSS is 0.032459986930674256
LOSS is 0.03313884655110694
LOSS is 0.04273627994843992
LOSS is 0.03887633727921639
LOSS is 0.02746831207109305
LOSS is 0.03449346551826845
LOSS is 0.03631476055350504
LOSS is 0.03291252016211123
LOSS is 0.03323176549640872
LOSS is 0.03304175770608708
LOSS is 0.028912807982706002
LOSS is 0.03915572914818768
LOSS is 0.03267185895126507
LOSS is 0.03955170525165159
LOSS is 0.032190397475157326
LOSS is 0.03671890165584046
LOSS is 0.030645879693571867
LOSS is 0.03935965465980796
LOSS is 0.027778096873371398
LOSS is 0.03301965839205271
LOSS is 0.03044711214693962
LOSS is 0.04015897604180888
LOSS is 0.039553028126974824
LOSS is 0.03317298154802605
LOSS is 0.036585708093965275
LOSS is 0.03235093033280767
LOSS is 0.0378285161878254
LOSS is 0.03260229569510557
LOSS is 0.03530361326799418
LOSS is 0.042226280655061905
LOSS is 0.03317809187991467
LOSS is 0.041702019033449084
LOSS is 0.041363852111147334
LOSS is 0.037291485498329474
LOSS is 0.04237676792487037
LOSS is 0.04116230339423055
LOSS is 0.02972539342522699
LOSS is 0.030581650491609862
LOSS is 0.04112477714845833
LOSS is 0.03401508273285193
LOSS is 0.03355880491003821
LOSS is 0.036247346923725374
LOSS is 0.03669650090479991
LOSS is 0.03732118017884204
LOSS is 0.034339842363163675
LOSS is 0.032348590969340875
LOSS is 0.04224726505184663
LOSS is 0.033447164825774964
LOSS is 0.030442895074569
LOSS is 0.03597027574171079
LOSS is 0.041499280987191016
LOSS is 0.03529390930508574
LOSS is 0.028550561272810835
LOSS is 0.03050388574454701
LOSS is 0.04203761303802215
LOSS is 0.038042265912032844
LOSS is 0.029698305040510607
LOSS is 0.036060430221453624
LOSS is 0.03396354404908683
LOSS is 0.03635348072440441
LOSS is 0.03464230114642609
LOSS is 0.03125077675707871
LOSS is 0.03331505202411791
LOSS is 0.03201717215085713
LOSS is 0.04264484609845871
LOSS is 0.024353179183963222
LOSS is 0.029750311993896807
LOSS is 0.03700596337730531
LOSS is 0.03510596596868709
LOSS is 0.033714021789395095
LOSS is 0.040774727406775735
LOSS is 0.03668842871387218
LOSS is 0.03910584327556232
LOSS is 0.044017019478196744
LOSS is 0.03079311062174384
LOSS is 0.03881643368290194
LOSS is 0.033296179096893565
LOSS is 0.037255213006962246
LOSS is 0.03809531857298377
LOSS is 0.03928747580643782
Epoch: [1][1300/2183]	Per Sample Total Time 10.85778	Per Sample Data Time 10.84153	Per Sample DNN Time 0.01625	Train Loss 0.0671	
LOSS is 0.033342851910419996
LOSS is 0.03499011592017875
LOSS is 0.049456806909875015
LOSS is 0.033033694874999735
LOSS is 0.036321695897786416
LOSS is 0.03114167059485529
LOSS is 0.04025013124424732
LOSS is 0.032123882931870566
LOSS is 0.034173144755671576
LOSS is 0.03498202978206488
LOSS is 0.0365471187865478
LOSS is 0.03881575575146902
LOSS is 0.04548943162536792
LOSS is 0.04014561065152521
LOSS is 0.030718914218208132
LOSS is 0.048102068123456174
LOSS is 0.0339360577762515
LOSS is 0.03390051441145867
LOSS is 0.03320796146183663
LOSS is 0.03398743698940962
LOSS is 0.036842088198269875
LOSS is 0.031580179843973995
LOSS is 0.03571632055585117
LOSS is 0.03373862250611031
LOSS is 0.033529389255127176
LOSS is 0.04231432314069632
LOSS is 0.03116339607445601
LOSS is 0.03348314008685217
LOSS is 0.040971492249227595
LOSS is 0.02989711292650706
LOSS is 0.02862137342943849
LOSS is 0.04122265397483716
LOSS is 0.03981647550438842
LOSS is 0.035338962910997605
LOSS is 0.037075536196158894
LOSS is 0.03964696713058705
LOSS is 0.03055921680798444
LOSS is 0.038536520260337666
LOSS is 0.03720099388219145
LOSS is 0.028158992297830993
LOSS is 0.032570473480785345
LOSS is 0.0367652324817027
LOSS is 0.034242833930475176
LOSS is 0.029558550234990738
LOSS is 0.03691916314821962
LOSS is 0.034412396658541795
LOSS is 0.03762240809116823
LOSS is 0.03663732759077296
LOSS is 0.03902298111177516
LOSS is 0.028290605559013787
LOSS is 0.035924718159052044
LOSS is 0.03245006056497611
LOSS is 0.03689844729369118
LOSS is 0.03773281300852735
LOSS is 0.03233701711704877
LOSS is 0.03478029347024858
LOSS is 0.03927735319821902
LOSS is 0.03625264432785722
LOSS is 0.03258040940097999
LOSS is 0.03411000648367917
LOSS is 0.029000515393224003
LOSS is 0.031475778770933785
LOSS is 0.042876594217038176
LOSS is 0.03113723250178737
LOSS is 0.03221334600699872
LOSS is 0.034735941991869676
LOSS is 0.03384828386585772
LOSS is 0.02689055877781357
LOSS is 0.04080262470864303
LOSS is 0.03215995745859497
LOSS is 0.034808660069490245
LOSS is 0.0406194739775674
LOSS is 0.03306097471446265
LOSS is 0.03620024332689355
LOSS is 0.03630221496530188
LOSS is 0.03431265898953522
LOSS is 0.03158829621082987
LOSS is 0.030724281760873662
LOSS is 0.028501289040917376
LOSS is 0.038547573256413066
LOSS is 0.03802041518172094
LOSS is 0.033812470266905925
LOSS is 0.0355044497087268
LOSS is 0.029971874710851505
LOSS is 0.03738643120062383
LOSS is 0.03350468017238503
LOSS is 0.03131059119594284
LOSS is 0.028067740384819143
LOSS is 0.04154794320978302
LOSS is 0.03433449875020112
LOSS is 0.031525540265914365
LOSS is 0.031729762293398384
LOSS is 0.03877679557471614
LOSS is 0.038400930760156674
LOSS is 0.039403793304615346
LOSS is 0.03550775521439695
LOSS is 0.03702347685413164
LOSS is 0.031135725696755497
LOSS is 0.028281599425196573
LOSS is 0.031919853664294354
Epoch: [1][1400/2183]	Per Sample Total Time 11.66267	Per Sample Data Time 11.64644	Per Sample DNN Time 0.01623	Train Loss 0.0648	
LOSS is 0.025564920111113075
LOSS is 0.04244965791741076
LOSS is 0.0396462498883678
LOSS is 0.034586239752728336
LOSS is 0.03165029421492364
LOSS is 0.03548939328281752
LOSS is 0.03382904858338104
LOSS is 0.027318435068251955
LOSS is 0.03198539776116377
LOSS is 0.026818112016044327
LOSS is 0.0362336829235331
LOSS is 0.035146230975757746
LOSS is 0.0362401265151857
LOSS is 0.027740306944566934
LOSS is 0.03306125637478544
LOSS is 0.03604127122273591
LOSS is 0.03090785636532625
LOSS is 0.04193586173205404
LOSS is 0.026018928589182908
LOSS is 0.026692630093796958
LOSS is 0.041599949798934784
LOSS is 0.03171248043319793
LOSS is 0.03756945683514156
LOSS is 0.03137479647154881
LOSS is 0.044850673788265946
LOSS is 0.03047966673431802
LOSS is 0.035777509526524226
LOSS is 0.03809820184590838
LOSS is 0.028944237479396787
LOSS is 0.030325413915949565
LOSS is 0.029490716123643023
LOSS is 0.03277234195003985
LOSS is 0.040149342035486674
LOSS is 0.03533575048427641
LOSS is 0.030164949249932155
LOSS is 0.030974652957617463
LOSS is 0.02882092194699605
LOSS is 0.03742036792556368
LOSS is 0.03874511717183244
LOSS is 0.0336242509643004
LOSS is 0.03317682943884089
LOSS is 0.038192014227145896
LOSS is 0.04200774499903977
LOSS is 0.03633456806031366
LOSS is 0.03875853698550297
LOSS is 0.030905552671223028
LOSS is 0.027313701582800907
LOSS is 0.028319894617937585
LOSS is 0.029352919512796994
LOSS is 0.035329546312423195
LOSS is 0.03430585526812744
LOSS is 0.040882983854632286
LOSS is 0.03698724069322149
LOSS is 0.041550677940394964
LOSS is 0.027270704635981628
LOSS is 0.03320509207827854
LOSS is 0.031879347025339184
LOSS is 0.03671338198813222
LOSS is 0.035768171254797684
LOSS is 0.0343290793360696
LOSS is 0.039931370339812325
LOSS is 0.040865853278276826
LOSS is 0.025486688639599988
LOSS is 0.027519722318732724
LOSS is 0.031238721271171624
LOSS is 0.032370282962074275
LOSS is 0.03788571950921323
LOSS is 0.033975367331683326
LOSS is 0.03863776789022571
LOSS is 0.03699598490750456
LOSS is 0.03149218324989003
LOSS is 0.033964366731912984
LOSS is 0.032707221226155526
LOSS is 0.039496957755569995
LOSS is 0.02829806096798469
LOSS is 0.03278487917753713
LOSS is 0.03389797914162045
LOSS is 0.03855214264049816
LOSS is 0.033308875364552175
LOSS is 0.03214003598661899
LOSS is 0.03425933746480345
LOSS is 0.037853485786714984
LOSS is 0.024895956359832782
LOSS is 0.030561768620294364
LOSS is 0.03379222804263312
LOSS is 0.031490976130735364
LOSS is 0.0333893788815476
LOSS is 0.036838431701374554
LOSS is 0.029601723838713952
LOSS is 0.031818893281379136
LOSS is 0.027249688333298158
LOSS is 0.037236682528261254
LOSS is 0.03285018253833793
LOSS is 0.03141837155169924
LOSS is 0.030801637128315636
LOSS is 0.029858573256594054
LOSS is 0.03609210432958207
LOSS is 0.034381734652609644
LOSS is 0.028318419402930887
LOSS is 0.02892749489915635
Epoch: [1][1500/2183]	Per Sample Total Time 12.46785	Per Sample Data Time 12.45161	Per Sample DNN Time 0.01623	Train Loss 0.0628	
LOSS is 0.038111517247016315
LOSS is 0.02860940547383507
LOSS is 0.03155868276977951
LOSS is 0.03611809640249703
LOSS is 0.0268337007885566
LOSS is 0.03500692340283422
LOSS is 0.03317450963824134
LOSS is 0.02947865292313509
LOSS is 0.030286052296141862
LOSS is 0.026906799717156295
LOSS is 0.021727040539456843
LOSS is 0.031884961829346144
LOSS is 0.03520173844585467
LOSS is 0.03287697632530277
LOSS is 0.028714166207646486
LOSS is 0.03915913609957594
LOSS is 0.04043021068607535
LOSS is 0.027339517723497315
LOSS is 0.026495046215374414
LOSS is 0.03295508847028638
LOSS is 0.027231477222327764
LOSS is 0.0280685650263816
LOSS is 0.02397073834562131
LOSS is 0.03576784450066043
LOSS is 0.02830914234213803
LOSS is 0.038002120795572414
LOSS is 0.03180155967231258
LOSS is 0.03794788512323673
LOSS is 0.03344425909856606
LOSS is 0.03558710296240558
LOSS is 0.03917552972915777
LOSS is 0.028118094932239428
LOSS is 0.03538022542120113
LOSS is 0.03083172122637431
LOSS is 0.036109130238619405
LOSS is 0.03715600721591424
LOSS is 0.0296689308496813
LOSS is 0.031122112501422335
LOSS is 0.03129593163801474
LOSS is 0.038343697631692826
LOSS is 0.04518018331281686
LOSS is 0.03392042665233021
LOSS is 0.041148298669189294
LOSS is 0.03460280967784153
LOSS is 0.03354008895558461
LOSS is 0.029979715578616988
LOSS is 0.034616192887381964
LOSS is 0.034162102021364264
LOSS is 0.03411351832017923
LOSS is 0.03143011778733732
LOSS is 0.03125883426560904
LOSS is 0.0371190044391066
LOSS is 0.027389695837991897
LOSS is 0.03356352520117071
LOSS is 0.029975382923245586
LOSS is 0.042553333255879504
LOSS is 0.03913961379769414
LOSS is 0.04065765841325629
LOSS is 0.0269332649087058
LOSS is 0.032487191926047675
LOSS is 0.031170127737568692
LOSS is 0.0325336346688952
LOSS is 0.03610463309654734
LOSS is 0.03350801120041676
LOSS is 0.032364116735213125
LOSS is 0.03411226499820866
LOSS is 0.027439947877185965
LOSS is 0.02961201890463902
LOSS is 0.029922816212977828
LOSS is 0.03869286683067912
LOSS is 0.032126856955001136
LOSS is 0.027068266647062654
LOSS is 0.039040478928072844
LOSS is 0.033655300234192206
LOSS is 0.028130054267191253
LOSS is 0.03684546164144801
LOSS is 0.029782166081519488
LOSS is 0.030095154537411874
LOSS is 0.027409959363382468
LOSS is 0.036428605754141856
LOSS is 0.024596252621510457
LOSS is 0.03399650145559766
LOSS is 0.03392041611548242
LOSS is 0.03676134873511425
LOSS is 0.030227370343248672
LOSS is 0.03169185173057486
LOSS is 0.035263410095067234
LOSS is 0.03077375187262078
LOSS is 0.028526089711716246
LOSS is 0.03228670276216387
LOSS is 0.03804247682584295
LOSS is 0.040294070201101324
LOSS is 0.03231846813418088
LOSS is 0.03459567281194419
LOSS is 0.02871291274985803
LOSS is 0.04394485514538247
LOSS is 0.03829436170737609
LOSS is 0.032804956744415296
LOSS is 0.03130802497131906
LOSS is 0.023409663099397827
Epoch: [1][1600/2183]	Per Sample Total Time 13.27344	Per Sample Data Time 13.25721	Per Sample DNN Time 0.01622	Train Loss 0.0609	
LOSS is 0.036028141577019904
LOSS is 0.03392579432804875
LOSS is 0.031435435994159586
LOSS is 0.02804003490911176
LOSS is 0.0303000243207498
LOSS is 0.02856118222155298
LOSS is 0.02943266230771163
LOSS is 0.042961436255088
LOSS is 0.035946321097920494
LOSS is 0.04506675324984826
LOSS is 0.030454038630511302
LOSS is 0.026260661325601786
LOSS is 0.02965221491462823
LOSS is 0.03644003262379556
LOSS is 0.03077800860453863
LOSS is 0.03513240972667215
LOSS is 0.031732076982783235
LOSS is 0.034202035617878816
LOSS is 0.04087355091280188
LOSS is 0.03213360079137298
LOSS is 0.03437102858159051
LOSS is 0.030034967437095477
LOSS is 0.03320040660328232
LOSS is 0.03836806858380442
LOSS is 0.03332073533611644
LOSS is 0.024568741011365396
LOSS is 0.026254130163773277
LOSS is 0.02744189818763213
LOSS is 0.022972746949769016
LOSS is 0.03286270725569921
LOSS is 0.03977050059120908
LOSS is 0.0337016786323026
LOSS is 0.02951320163740699
LOSS is 0.033374884749161234
LOSS is 0.035880080044444186
LOSS is 0.03285599555902688
LOSS is 0.03671861013562496
LOSS is 0.03703634275569736
LOSS is 0.0328126066205247
LOSS is 0.03628235970046566
LOSS is 0.04104195859933195
LOSS is 0.03378436805884121
LOSS is 0.02322570099446845
LOSS is 0.0287956519255143
LOSS is 0.028292906343704093
LOSS is 0.03930469717893478
LOSS is 0.024712722405965906
LOSS is 0.03181984837458004
LOSS is 0.025006664606917185
LOSS is 0.028757247340933344
LOSS is 0.030744202001030015
LOSS is 0.027926186219459245
LOSS is 0.026656548625711974
LOSS is 0.020954554699031482
LOSS is 0.025357650447646547
LOSS is 0.028135158033983317
LOSS is 0.030415434555203926
LOSS is 0.03568824645328277
LOSS is 0.033901381725930456
LOSS is 0.034376316742321555
LOSS is 0.022306476624653442
LOSS is 0.0329640620413799
LOSS is 0.03802300438124803
LOSS is 0.030408488182292786
LOSS is 0.02964413332961461
LOSS is 0.0317258177815044
LOSS is 0.021259928810565425
LOSS is 0.03142344216161291
LOSS is 0.03147680938913254
LOSS is 0.03679423943642178
LOSS is 0.035035708950745176
LOSS is 0.030030074197226593
LOSS is 0.028598967101133896
LOSS is 0.029770944876606035
LOSS is 0.034435082379932286
LOSS is 0.02734948189293694
LOSS is 0.026345680176309543
LOSS is 0.03249572083121166
LOSS is 0.034057280431152324
LOSS is 0.03378276495893564
LOSS is 0.03537813713744981
LOSS is 0.03157495480249054
LOSS is 0.034492509464350105
LOSS is 0.03200206120231693
LOSS is 0.035521691655061055
LOSS is 0.03204182525939056
LOSS is 0.03394819432408743
LOSS is 0.030735754117292045
LOSS is 0.029643487650319003
LOSS is 0.033373946749100776
LOSS is 0.023905524877045538
LOSS is 0.028214039489685094
LOSS is 0.027987183533623467
LOSS is 0.03181254630214729
LOSS is 0.04737648212816566
LOSS is 0.02580495101569492
LOSS is 0.03485573930539734
LOSS is 0.02852115419523519
LOSS is 0.04244351300837783
LOSS is 0.03545533995270186
Epoch: [1][1700/2183]	Per Sample Total Time 14.07913	Per Sample Data Time 14.06292	Per Sample DNN Time 0.01621	Train Loss 0.0592	
LOSS is 0.03393311606848632
LOSS is 0.03482318184969093
LOSS is 0.03559341872984078
LOSS is 0.02360284998571539
LOSS is 0.03134185556654605
LOSS is 0.02722037167225305
LOSS is 0.032135644110191305
LOSS is 0.031593830801478665
LOSS is 0.03290416633598701
LOSS is 0.037891949297481915
LOSS is 0.02550125125941122
LOSS is 0.032492457309951234
LOSS is 0.03137827196109962
LOSS is 0.03170591202106152
LOSS is 0.03075992745939099
LOSS is 0.027428420098634283
LOSS is 0.02969480467887479
LOSS is 0.03505798924806489
LOSS is 0.025423300362551044
LOSS is 0.03066954066086813
LOSS is 0.029409313668608474
LOSS is 0.03308503245415826
LOSS is 0.03573479856248014
LOSS is 0.026159796147646075
LOSS is 0.03195091996104263
LOSS is 0.024298363306976777
LOSS is 0.026194634060472406
LOSS is 0.02687633349785756
LOSS is 0.03499632033679518
LOSS is 0.03036239491440938
LOSS is 0.038862991896651995
LOSS is 0.027832627914758632
LOSS is 0.031048207595837693
LOSS is 0.029301910329474292
LOSS is 0.04121538381548211
LOSS is 0.031742506974114806
LOSS is 0.026983353286632338
LOSS is 0.0334573970757386
LOSS is 0.0343134900618558
LOSS is 0.03019152673140828
LOSS is 0.029290903606718835
LOSS is 0.027824066082539504
LOSS is 0.028245893251150848
LOSS is 0.030999162669322688
LOSS is 0.03464867365784206
LOSS is 0.03301761181578816
LOSS is 0.024944536204226705
LOSS is 0.023030500474900087
LOSS is 0.032926719452225375
LOSS is 0.022580611864007855
LOSS is 0.03295841705687053
LOSS is 0.025176090823248766
LOSS is 0.02897919136739802
LOSS is 0.02829612903777161
LOSS is 0.027731610383828716
LOSS is 0.03326553798903963
LOSS is 0.03350992424733704
LOSS is 0.02546179769128988
LOSS is 0.0363044646710235
LOSS is 0.030560550892939017
LOSS is 0.03121259857657909
LOSS is 0.027030380633320118
LOSS is 0.031042517079816513
LOSS is 0.02601030121363389
LOSS is 0.028651814670593013
LOSS is 0.026946518534071706
LOSS is 0.029646375710775223
LOSS is 0.026542283351106262
LOSS is 0.027981446394405796
LOSS is 0.035301195728558624
LOSS is 0.031019478188245558
LOSS is 0.027915188748399187
LOSS is 0.021304214940804134
LOSS is 0.032655359039005516
LOSS is 0.029784994970808235
LOSS is 0.03396008150026319
LOSS is 0.02880968391929249
LOSS is 0.0188414061810181
LOSS is 0.028973630386317385
LOSS is 0.03222953333686746
LOSS is 0.032272998981376685
LOSS is 0.02904373024852248
LOSS is 0.03597880583106113
LOSS is 0.027306906724261355
LOSS is 0.02118662159969972
LOSS is 0.021945444830683603
LOSS is 0.030910955904012858
LOSS is 0.03232134064174413
LOSS is 0.02483101755792935
LOSS is 0.030781036892682703
LOSS is 0.020884910149858722
LOSS is 0.0309245085590131
LOSS is 0.028914778736386024
LOSS is 0.0376940385929629
LOSS is 0.035061364531720755
LOSS is 0.030697412392279758
LOSS is 0.0358795753405866
LOSS is 0.02796256502333563
LOSS is 0.03001765036873015
LOSS is 0.024742749872966673
Epoch: [1][1800/2183]	Per Sample Total Time 14.88444	Per Sample Data Time 14.86824	Per Sample DNN Time 0.01620	Train Loss 0.0576	
LOSS is 0.02962127868765189
LOSS is 0.034796128490173335
LOSS is 0.027256680195084
LOSS is 0.031467716681315025
LOSS is 0.03405647797781664
LOSS is 0.027828863941006907
LOSS is 0.021039952367597533
LOSS is 0.03738890460134523
LOSS is 0.025215981567453128
LOSS is 0.029061203824530823
LOSS is 0.0315555726820215
LOSS is 0.026990476551800386
LOSS is 0.02797567188278966
LOSS is 0.032477892106641475
LOSS is 0.029473346340989037
LOSS is 0.024382824997058683
LOSS is 0.026662326304406937
LOSS is 0.036151255594983625
LOSS is 0.02414050254820419
LOSS is 0.033756752102732816
LOSS is 0.02580515530287812
LOSS is 0.03635030044266993
LOSS is 0.029890353745528654
LOSS is 0.030799788762936566
LOSS is 0.02636441994351723
LOSS is 0.02727852965166676
LOSS is 0.02502612855275705
LOSS is 0.024702596121666528
LOSS is 0.03369817643205655
LOSS is 0.03783336458206274
LOSS is 0.03701149641744754
LOSS is 0.027963913162820975
LOSS is 0.039708375738725106
LOSS is 0.03581635160934336
LOSS is 0.025308214186564634
LOSS is 0.03161600184592923
LOSS is 0.03607333269746354
LOSS is 0.02758653130726695
LOSS is 0.026310724602565946
LOSS is 0.025103124670325393
LOSS is 0.02710705011340906
LOSS is 0.02903604300799392
LOSS is 0.03112447751351283
LOSS is 0.027976997795800953
LOSS is 0.0299961037018511
LOSS is 0.021104433604001924
LOSS is 0.03626931019496018
LOSS is 0.03596998616461254
LOSS is 0.02463228875717808
LOSS is 0.025833627433069828
LOSS is 0.03065404077654724
LOSS is 0.024939343732985435
LOSS is 0.02427048217961177
LOSS is 0.023387034223705996
LOSS is 0.03118990687124703
LOSS is 0.020250132268750654
LOSS is 0.029782302059126478
LOSS is 0.0336849973625552
LOSS is 0.02887024372217032
LOSS is 0.031206951790785146
LOSS is 0.032439591357203125
LOSS is 0.03476610268376438
LOSS is 0.03523577838248457
LOSS is 0.030777175742769034
LOSS is 0.030327654876843253
LOSS is 0.020498953246472714
LOSS is 0.029717941753575967
LOSS is 0.031456963095843095
LOSS is 0.03569507121360706
LOSS is 0.025322268794504152
LOSS is 0.03357417907041963
LOSS is 0.041596277139324234
LOSS is 0.0260484171382268
LOSS is 0.03055865577698569
LOSS is 0.022508707751500576
LOSS is 0.02767434237294462
LOSS is 0.0339230105188229
LOSS is 0.026053326596399227
LOSS is 0.026285117016329122
LOSS is 0.032531917351055505
LOSS is 0.03298657716783055
LOSS is 0.029757517557372923
LOSS is 0.03988692807969831
LOSS is 0.026168845587550702
LOSS is 0.031793711651068104
LOSS is 0.023824435286514928
LOSS is 0.02742679038293621
LOSS is 0.025333511072094556
LOSS is 0.03561108685333845
LOSS is 0.037117485253596295
LOSS is 0.024454086984236106
LOSS is 0.03190722911075378
LOSS is 0.028146199383651645
LOSS is 0.02230371559346774
LOSS is 0.025993989137205065
LOSS is 0.028969188722621768
LOSS is 0.024905851921381936
LOSS is 0.02719765569912852
LOSS is 0.026664016469803757
LOSS is 0.025320755539432867
Epoch: [1][1900/2183]	Per Sample Total Time 15.68944	Per Sample Data Time 15.67325	Per Sample DNN Time 0.01619	Train Loss 0.0561	
LOSS is 0.02548756703322094
LOSS is 0.03639673936719191
LOSS is 0.018299898471838487
LOSS is 0.02181447179997728
LOSS is 0.032250066948933336
LOSS is 0.034493943029228834
LOSS is 0.02391292526185377
LOSS is 0.03393360805171445
LOSS is 0.03995668124067985
LOSS is 0.02745685959113568
LOSS is 0.02273128259854275
LOSS is 0.028153153090922086
LOSS is 0.029811318769886082
LOSS is 0.03900394464641674
LOSS is 0.028023514470720937
LOSS is 0.033357550868774226
LOSS is 0.028127646076997432
LOSS is 0.03167898250176222
LOSS is 0.03135033993814432
LOSS is 0.026974595729261638
LOSS is 0.03072806931367571
LOSS is 0.027263821114150537
LOSS is 0.033528542899875904
LOSS is 0.03506183419543959
LOSS is 0.030910806647104137
LOSS is 0.032754231027065546
LOSS is 0.03297185292210391
LOSS is 0.028354080168016178
LOSS is 0.025632683094081586
LOSS is 0.033526315507576024
LOSS is 0.022342926129228242
LOSS is 0.023322346506320174
LOSS is 0.027535960047631914
LOSS is 0.026069959893017466
LOSS is 0.037280341042715014
LOSS is 0.02797984299606469
LOSS is 0.032231547017727284
LOSS is 0.03272839936774593
LOSS is 0.023740324916968047
LOSS is 0.023114380399056247
LOSS is 0.02565691616124241
LOSS is 0.03399348197713456
LOSS is 0.029108531579598397
LOSS is 0.02491152826997374
LOSS is 0.030693648122966505
LOSS is 0.03377765572741434
LOSS is 0.02595775230836201
LOSS is 0.02394379618022261
LOSS is 0.029320072916210242
LOSS is 0.024372590511726838
LOSS is 0.025512127180966977
LOSS is 0.034176523257168204
LOSS is 0.02894376907310895
LOSS is 0.026440498242203226
LOSS is 0.023155676761089126
LOSS is 0.02869733217080163
LOSS is 0.03216108029174696
LOSS is 0.029407930624486957
LOSS is 0.026057349878198393
LOSS is 0.028926842511209545
LOSS is 0.020582811808005017
LOSS is 0.03199081536232067
LOSS is 0.02657646635217437
LOSS is 0.026724181835500835
LOSS is 0.02457012774080795
LOSS is 0.02884538082163393
LOSS is 0.023984568792948267
LOSS is 0.029590310319302565
LOSS is 0.034971943156391114
LOSS is 0.02769005359262034
LOSS is 0.02603638099433738
LOSS is 0.022795408329499577
LOSS is 0.02225415525395268
LOSS is 0.02768518959511615
LOSS is 0.03328477030712141
LOSS is 0.022487978042142157
LOSS is 0.018765921631954067
LOSS is 0.02936185491232512
LOSS is 0.020619785234497007
LOSS is 0.03353642903719447
LOSS is 0.033933834939186155
LOSS is 0.034886156724145016
LOSS is 0.03142229014677772
LOSS is 0.026976087818353943
LOSS is 0.030239876061289885
LOSS is 0.031820929265571375
LOSS is 0.025844342623289173
LOSS is 0.03566572454025542
LOSS is 0.03433328993975011
LOSS is 0.0295129453643555
LOSS is 0.035991499461209366
LOSS is 0.031086645550021788
LOSS is 0.029257556352677058
LOSS is 0.024088711315974554
LOSS is 0.022576417716821503
LOSS is 0.026827039716687675
LOSS is 0.025129982720827684
LOSS is 0.03530303020328575
LOSS is 0.02981034127507883
LOSS is 0.027796313661140933
Epoch: [1][2000/2183]	Per Sample Total Time 16.49414	Per Sample Data Time 16.47796	Per Sample DNN Time 0.01618	Train Loss 0.0547	
LOSS is 0.028074567855352752
LOSS is 0.026166901446219224
LOSS is 0.02832669823934945
LOSS is 0.023607239514458343
LOSS is 0.02628665010466648
LOSS is 0.030368863974557223
LOSS is 0.028915567721390593
LOSS is 0.03434469661556553
LOSS is 0.029418903098170027
LOSS is 0.036846512105233466
LOSS is 0.024000525313773932
LOSS is 0.03105394577061816
LOSS is 0.02764658472806332
LOSS is 0.024133690171317237
LOSS is 0.026009126241648726
LOSS is 0.027513108097409714
LOSS is 0.03203121809640531
LOSS is 0.033654212194572514
LOSS is 0.027828640840986435
LOSS is 0.030857756929957153
LOSS is 0.02528287336608628
LOSS is 0.026619000115194167
LOSS is 0.03464947794277881
LOSS is 0.03613755266451335
LOSS is 0.02637962591080092
LOSS is 0.03661526501548603
LOSS is 0.025189825565636665
LOSS is 0.026183231499162504
LOSS is 0.03052823797511034
LOSS is 0.03805011658504858
LOSS is 0.033927701834691106
LOSS is 0.024876188679991175
LOSS is 0.0242356493522675
LOSS is 0.02887987443735862
LOSS is 0.027214592644037718
LOSS is 0.027391541066123563
LOSS is 0.025841401614792026
LOSS is 0.02829757775247951
LOSS is 0.02657899102846083
LOSS is 0.028552220831479645
LOSS is 0.026257799456167658
LOSS is 0.02387636285738457
LOSS is 0.033220156450212625
LOSS is 0.028832883332628022
LOSS is 0.03405114756033679
LOSS is 0.0223744100696058
LOSS is 0.028495002737642307
LOSS is 0.04307535801805595
LOSS is 0.040190813329876014
LOSS is 0.03053411215648036
LOSS is 0.026785621707555645
LOSS is 0.020312728608793504
LOSS is 0.02875147068264293
LOSS is 0.025798214061360344
LOSS is 0.02990704758696666
LOSS is 0.03427452720847214
LOSS is 0.026266844394316046
LOSS is 0.021351925175404178
LOSS is 0.032919732149336293
LOSS is 0.029058555946903656
LOSS is 0.03114455038953262
LOSS is 0.025460193493151262
LOSS is 0.02656851502149948
LOSS is 0.035758891898391695
LOSS is 0.0310371721580547
LOSS is 0.021681357946799838
LOSS is 0.02617845714145612
LOSS is 0.028790516390799895
LOSS is 0.025496786784472838
LOSS is 0.03446650927745699
LOSS is 0.04199300079160215
LOSS is 0.033681806676249836
LOSS is 0.026743722906370143
LOSS is 0.026559483464346462
LOSS is 0.031774555425993944
LOSS is 0.030281859263195657
LOSS is 0.029799256927362877
LOSS is 0.026572824882508336
LOSS is 0.033163279546570265
LOSS is 0.03603916798090116
LOSS is 0.027542374642071082
LOSS is 0.034552356123385836
LOSS is 0.025523822052612864
LOSS is 0.02853610563637631
LOSS is 0.02437909380341201
LOSS is 0.023781375313192257
LOSS is 0.031893368657523145
LOSS is 0.028869018966506704
LOSS is 0.02488466537744292
LOSS is 0.023460996173525928
LOSS is 0.0284032521762371
LOSS is 0.02149498013345389
LOSS is 0.023720384968570825
LOSS is 0.022751777802307818
LOSS is 0.025313344452539847
LOSS is 0.031131344325282653
LOSS is 0.02665126852615989
LOSS is 0.03177722264069113
LOSS is 0.03079235731667723
LOSS is 0.024279229767998914
Epoch: [1][2100/2183]	Per Sample Total Time 17.29864	Per Sample Data Time 17.28247	Per Sample DNN Time 0.01618	Train Loss 0.0535	
LOSS is 0.029032341456040743
LOSS is 0.02954634142695189
LOSS is 0.03596013147873843
LOSS is 0.029176950924350728
LOSS is 0.022372994722803317
LOSS is 0.02890275007909319
LOSS is 0.023557979154769176
LOSS is 0.03305932490632889
LOSS is 0.02262206885798757
LOSS is 0.026413504819332353
LOSS is 0.032588799146809226
LOSS is 0.031751459787289306
LOSS is 0.024587227182710194
LOSS is 0.03361763384183481
LOSS is 0.02972702494870949
LOSS is 0.029098489065848603
LOSS is 0.0293752082711823
LOSS is 0.018258910020110004
LOSS is 0.02555167734739371
LOSS is 0.025958191341875745
LOSS is 0.022974492798239227
LOSS is 0.022358219132729576
LOSS is 0.034093483099956454
LOSS is 0.030100726042146565
LOSS is 0.02907450547834742
LOSS is 0.02511836595372491
LOSS is 0.025046184843862042
LOSS is 0.030044747917612163
LOSS is 0.03301394429635063
LOSS is 0.025483244582792396
LOSS is 0.025967444728885312
LOSS is 0.027255450934850767
LOSS is 0.025627888834472593
LOSS is 0.03126454785902752
LOSS is 0.030205557387210626
LOSS is 0.02683844383582861
LOSS is 0.027378658877375228
LOSS is 0.02732797897081279
LOSS is 0.03074361675831218
LOSS is 0.023625907835909555
LOSS is 0.02321032866323852
LOSS is 0.021106791382311106
LOSS is 0.027024569841717798
LOSS is 0.023113168815907557
LOSS is 0.03600032749707074
LOSS is 0.025803145118261456
LOSS is 0.02498094596718147
LOSS is 0.03326626161210394
LOSS is 0.027489825607820727
LOSS is 0.030555241015681533
LOSS is 0.028390643325741018
LOSS is 0.032058517567493255
LOSS is 0.03183377161915511
LOSS is 0.028497719007282287
LOSS is 0.039818504076392855
LOSS is 0.02421936190762305
LOSS is 0.03423399835897726
LOSS is 0.032070142424660546
LOSS is 0.03611423105746023
LOSS is 0.016270857317310098
LOSS is 0.03503542306306675
LOSS is 0.030190854791459668
LOSS is 0.026513831001332925
LOSS is 0.02011049488809173
LOSS is 0.030694164059799127
LOSS is 0.023845332456282148
LOSS is 0.03393329642793105
LOSS is 0.028922336084457736
LOSS is 0.028316412038645166
LOSS is 0.02397781332486678
LOSS is 0.029238045121116257
LOSS is 0.02958958200865406
LOSS is 0.02627014210486474
LOSS is 0.027414841185163824
LOSS is 0.02366139961940159
LOSS is 0.026060414526630972
LOSS is 0.028759617982990072
LOSS is 0.034538845825421355
LOSS is 0.029056395087876205
LOSS is 0.025532819323852894
LOSS is 0.033452376441879705
LOSS is 0.028237965963829388
inside validate
LOSS in validation is 0.05690806150436402
LOSS in validation is 0.048889091710249584
LOSS in validation is 0.051246517300605775
LOSS in validation is 0.026679217716058096
LOSS in validation is 0.03327389121055603
LOSS in validation is 0.030978870193163557
LOSS in validation is 0.0432000004251798
LOSS in validation is 0.042946691513061526
LOSS in validation is 0.06075445214907329
LOSS in validation is 0.05574118187030157
LOSS in validation is 0.033371179699897766
LOSS in validation is 0.026059262057145437
LOSS in validation is 0.017812266151110333
LOSS in validation is 0.01784586697816849
LOSS in validation is 0.03997158269087474
LOSS in validation is 0.024047513206799827
LOSS in validation is 0.02215234637260437
LOSS in validation is 0.060733371774355574
LOSS in validation is 0.08291366835435232
LOSS in validation is 0.038685730695724486
LOSS in validation is 0.049311522444089255
LOSS in validation is 0.05732104847828547
LOSS in validation is 0.06358070587118467
LOSS in validation is 0.07622176229953766
LOSS in validation is 0.03864272216955821
LOSS in validation is 0.016840541859467824
LOSS in validation is 0.011749076346556347
LOSS in validation is 0.0156710754831632
LOSS in validation is 0.0178510449330012
LOSS in validation is 0.020532815456390383
LOSS in validation is 0.016049689849217733
LOSS in validation is 0.01809576491514842
LOSS in validation is 0.023325177033742272
LOSS in validation is 0.013376980125904084
LOSS in validation is 0.010097321768601735
LOSS in validation is 0.026336331268151603
LOSS in validation is 0.011724865635236104
LOSS in validation is 0.011238116323947908
LOSS in validation is 0.010997665027777355
LOSS in validation is 0.07794644484917324
LOSS in validation is 0.05712758600711823
LOSS in validation is 0.03334452917178472
LOSS in validation is 0.02258786916732788
LOSS in validation is 0.01980888326962789
LOSS in validation is 0.0272688764333725
LOSS in validation is 0.025256562232971194
LOSS in validation is 0.025427509248256683
LOSS in validation is 0.03810403486092886
LOSS in validation is 0.041451468269030255
LOSS in validation is 0.0264933908979098
LOSS in validation is 0.025116879443327588
LOSS in validation is 0.015839444796244304
LOSS in validation is 0.021835171778996788
LOSS in validation is 0.013595509131749471
LOSS in validation is 0.04335148851076762
LOSS in validation is 0.016263942817846936
LOSS in validation is 0.03128413925568263
LOSS in validation is 0.046053086121877036
LOSS in validation is 0.0431929753224055
LOSS in validation is 0.0290853026509285
LOSS in validation is 0.022680703302224478
LOSS in validation is 0.03046839813391368
LOSS in validation is 0.042154670159022016
LOSS in validation is 0.02929955542087555
LOSS in validation is 0.05060537606477738
LOSS in validation is 0.028732318182786307
LOSS in validation is 0.022560180723667146
LOSS in validation is 0.027002959946791333
LOSS in validation is 0.02891118109226227
LOSS in validation is 0.04542039136091868
LOSS in validation is 0.03217117408911387
LOSS in validation is 0.058136110405127214
LOSS in validation is 0.03681580762068431
LOSS in validation is 0.041361804803212485
LOSS in validation is 0.031837771336237594
LOSS in validation is 0.036624663174152375
LOSS in validation is 0.03726694017648697
LOSS in validation is 0.025706243018309278
LOSS in validation is 0.024492793579896293
LOSS in validation is 0.028081545531749727
LOSS in validation is 0.02082746466000875
LOSS in validation is 0.031644995212554934
LOSS in validation is 0.04320325632890066
LOSS in validation is 0.01628407120704651
LOSS in validation is 0.015145053764184317
LOSS in validation is 0.024704342683156334
LOSS in validation is 0.02005955457687378
LOSS in validation is 0.026906979779402415
LOSS in validation is 0.01891586830218633
LOSS in validation is 0.02201442539691925
LOSS in validation is 0.0242846750219663
LOSS in validation is 0.04266610066095988
LOSS in validation is 0.0314753594994545
LOSS in validation is 0.015310880541801453
LOSS in validation is 0.016617838442325592
LOSS in validation is 0.017121844490369163
LOSS in validation is 0.02224147250254949
LOSS in validation is 0.013027883470058442
LOSS in validation is 0.03455836435159048
LOSS in validation is 0.020903009076913198
LOSS in validation is 0.018845304250717166
LOSS in validation is 0.020555354257424673
LOSS in validation is 0.016113993028799695
LOSS in validation is 0.01491612474123637
LOSS in validation is 0.0157798108458519
LOSS in validation is 0.03217328002055486
LOSS in validation is 0.02693645904461543
LOSS in validation is 0.020925041735172272
LOSS in validation is 0.03358919074138006
LOSS in validation is 0.017483697632948558
LOSS in validation is 0.013703591624895732
LOSS in validation is 0.012751410007476807
LOSS in validation is 0.01518014838298162
LOSS in validation is 0.021116306682427727
LOSS in validation is 0.010748586058616639
LOSS in validation is 0.0356425811847051
LOSS in validation is 0.02391249497731527
LOSS in validation is 0.01460840255022049
LOSS in validation is 0.011705313324928284
LOSS in validation is 0.01216898113489151
LOSS in validation is 0.018746284345785777
LOSS in validation is 0.017002494633197786
LOSS in validation is 0.01360920637845993
LOSS in validation is 0.02135793338219325
LOSS in validation is 0.031998393734296166
LOSS in validation is 0.011706079542636872
LOSS in validation is 0.021370803316434227
LOSS in validation is 0.026925344467163086
LOSS in validation is 0.023445753157138826
LOSS in validation is 0.023590634465217593
LOSS in validation is 0.019799990852673848
LOSS in validation is 0.03198398927847545
LOSS in validation is 0.057627830604712174
LOSS in validation is 0.02873513827721278
LOSS in validation is 0.00802032212416331
LOSS in validation is 0.014249354203542074
LOSS in validation is 0.009910373588403066
LOSS in validation is 0.016639271378517152
LOSS in validation is 0.007226423621177674
LOSS in validation is 0.012147805293401082
LOSS in validation is 0.023323076168696086
LOSS in validation is 0.020001273353894553
LOSS in validation is 0.010504042704900107
LOSS in validation is 0.006597705086072286
LOSS in validation is 0.014236118098100027
LOSS in validation is 0.012759222586949667
LOSS in validation is 0.013433392345905305
LOSS in validation is 0.03150678058465322
LOSS in validation is 0.018276200691858927
LOSS in validation is 0.008586500883102418
LOSS in validation is 0.011372541387875875
LOSS in validation is 0.012085569600264232
LOSS in validation is 0.016336169342199964
LOSS in validation is 0.017523152430852257
LOSS in validation is 0.019773433009783428
LOSS in validation is 0.025021215677261354
LOSS in validation is 0.010019087692101797
LOSS in validation is 0.01998621960481008
LOSS in validation is 0.020735631982485455
LOSS in validation is 0.00986344983180364
LOSS in validation is 0.014917780458927155
LOSS in validation is 0.009370696743329366
LOSS in validation is 0.024405739307403567
LOSS in validation is 0.022177938520908356
LOSS in validation is 0.018343656559785208
LOSS in validation is 0.023235843777656556
LOSS in validation is 0.016868080496788024
LOSS in validation is 0.03163432627916336
LOSS in validation is 0.03525123337904613
LOSS in validation is 0.032962770958741505
LOSS in validation is 0.027772441705067954
LOSS in validation is 0.025302735070387525
LOSS in validation is 0.03843528896570206
LOSS in validation is 0.037424961229165395
LOSS in validation is 0.01768822232882182
LOSS in validation is 0.018714880943298342
LOSS in validation is 0.07462161699930828
LOSS in validation is 0.06991500556468964
LOSS in validation is 0.023261732359727224
LOSS in validation is 0.009963362614313761
LOSS in validation is 0.01618652174870173
LOSS in validation is 0.016155366202195487
LOSS in validation is 0.018879717985788982
LOSS in validation is 0.01047650545835495
LOSS in validation is 0.014793315033117932
LOSS in validation is 0.014672928750514985
LOSS in validation is 0.010645838975906373
LOSS in validation is 0.013147302766640981
LOSS in validation is 0.011259013712406158
LOSS in validation is 0.012068198223908743
LOSS in validation is 0.01388327995936076
LOSS in validation is 0.012533146838347118
LOSS in validation is 0.01500326842069626
LOSS in validation is 0.021322981317838035
LOSS in validation is 0.015402278006076815
LOSS in validation is 0.012405997117360434
LOSS in validation is 0.010555682480335237
LOSS in validation is 0.012516649464766185
LOSS in validation is 0.01937592039505641
LOSS in validation is 0.03646640161673228
LOSS in validation is 0.03318619191646576
LOSS in validation is 0.0645524263381958
LOSS in validation is 0.04001842002073924
LOSS in validation is 0.025175127585728964
LOSS in validation is 0.03328196783860525
LOSS in validation is 0.03698324849208196
LOSS in validation is 0.03968104759852092
LOSS in validation is 0.04626869767904282
LOSS in validation is 0.04242790599664053
LOSS in validation is 0.020339119434356692
LOSS in validation is 0.030562626818815868
LOSS in validation is 0.031801307300726576
LOSS in validation is 0.027205250561237338
LOSS in validation is 0.056862039069334666
LOSS in validation is 0.03137163122495016
LOSS in validation is 0.04079269498586655
LOSS in validation is 0.037585569024086
LOSS in validation is 0.04818033168713252
LOSS in validation is 0.04334654211997986
LOSS in validation is 0.053882962763309485
LOSS in validation is 0.044539312422275545
LOSS in validation is 0.07085249582926433
LOSS in validation is 0.0646212183435758
LOSS in validation is 0.022762883802254996
LOSS in validation is 0.029891380170981093
LOSS in validation is 0.028053005437056227
LOSS in validation is 0.028625628749529522
LOSS in validation is 0.03337576409180959
LOSS in validation is 0.0220892142256101
LOSS in validation is 0.024292849997679396
LOSS in validation is 0.04239011327425639
LOSS in validation is 0.018981249928474428
LOSS in validation is 0.018113493422667187
LOSS in validation is 0.03174251963694891
LOSS in validation is 0.026489415168762208
LOSS in validation is 0.02601695368687312
LOSS in validation is 0.02972705960273743
LOSS in validation is 0.021666314601898193
LOSS in validation is 0.023340163826942446
LOSS in validation is 0.018991783459981284
LOSS in validation is 0.020333499411741895
LOSS in validation is 0.018332918882369997
LOSS in validation is 0.02311845491329829
LOSS in validation is 0.020206195414066316
LOSS in validation is 0.021655831237634024
LOSS in validation is 0.022078033884366355
LOSS in validation is 0.019006846547126772
LOSS in validation is 0.017562135457992553
LOSS in validation is 0.028602075278759003
LOSS in validation is 0.032747784554958345
LOSS in validation is 0.024770983556906385
LOSS in validation is 0.020162053108215332
LOSS in validation is 0.03333632012208303
LOSS in validation is 0.05566185245911281
LOSS in validation is 0.04984402636686961
LOSS in validation is 0.025726963182290397
LOSS in validation is 0.029118212362130486
LOSS in validation is 0.023255885044733686
LOSS in validation is 0.021252243022123973
LOSS in validation is 0.04032691488663356
LOSS in validation is 0.06657500247160594
LOSS in validation is 0.06208480815092723
LOSS in validation is 0.06398128469785055
LOSS in validation is 0.04443520585695903
LOSS in validation is 0.029701908628145854
LOSS in validation is 0.047385312517484034
LOSS in validation is 0.04928177783886592
LOSS in validation is 0.05038333088159561
LOSS in validation is 0.03656084418296814
LOSS in validation is 0.05689816394605135

----------------------------------------STATS-----------------------------------

[{'precision_micro': 0.9086250634195839, 'precision_macro': 0.8463230598855143, 'precision_classes': array([0.81690141, 0.75757576, 0.        , 0.89655172, 0.81538462,
       1.        , 0.92857143, 1.        , 0.94276875, 1.        ,
       0.88578275, 0.72413793, 0.92105263, 0.91147741, 0.89427012,
       0.80381471, 0.93777135, 0.96428571, 0.97814208, 0.97155689,
       0.98148148, 0.94337607, 1.        , 1.        , 0.97470489,
       0.83333333, 1.        , 0.94054054, 1.        , 0.94771242,
       0.96621622, 1.        , 0.69105691, 0.92      , 0.7012987 ,
       1.        , 0.        , 0.94117647, 0.9520202 , 0.88235294,
       0.86434783, 0.9162234 , 0.96174863, 0.94029851, 0.        ,
       1.        , 0.        , 0.80821918, 1.        , 1.        ]), 'recall_micro': 0.6909602993942667, 'recall_macro': 0.4799393349877571, 'recall_classes': array([0.2406639 , 0.65130261, 0.        , 0.46846847, 0.72163389,
       0.25568182, 0.40310078, 0.04672897, 0.85244755, 0.00621118,
       0.72294654, 0.32642487, 0.83333333, 0.59911717, 0.78268657,
       0.7972973 , 0.76777251, 0.19565217, 0.872745  , 0.87940379,
       0.7260274 , 0.7184703 , 0.08053691, 0.22807018, 0.86054591,
       0.60283688, 0.42857143, 0.60416667, 0.1025641 , 0.74935401,
       0.55859375, 0.52542373, 0.63909774, 0.47668394, 0.28272251,
       0.02702703, 0.        , 0.28318584, 0.69047619, 0.73770492,
       0.82971619, 0.74647887, 0.60585198, 0.50806452, 0.        ,
       0.04255319, 0.        , 0.60512821, 0.42682927, 0.48666667]), 'f1_micro': 0.7849832343465777, 'f1_macro': 0.5668106297653948, 'f1_classes': array([0.37179487, 0.70043103, 0.        , 0.61538462, 0.76565008,
       0.40723982, 0.56216216, 0.08928571, 0.89533603, 0.01234568,
       0.79612347, 0.45      , 0.875     , 0.72300242, 0.834766  ,
       0.80054274, 0.84429967, 0.3253012 , 0.92244267, 0.92318634,
       0.83464567, 0.81570439, 0.14906832, 0.37142857, 0.91407486,
       0.69958848, 0.6       , 0.73572939, 0.18604651, 0.83694084,
       0.70792079, 0.68888889, 0.6640625 , 0.62798635, 0.40298507,
       0.05263158, 0.        , 0.43537415, 0.80042463, 0.80357143,
       0.84667802, 0.82268657, 0.74340021, 0.65968586, 0.        ,
       0.08163265, 0.        , 0.69208211, 0.5982906 , 0.65470852]), 'acc': 0.7736023766349011}]
Traceback (most recent call last):
  File "/scratch/users/fsofian19/COMP491_model/models/ast_custom/egs/custom/../../src/run.py", line 150, in <module>
    train(audio_model, train_loader, val_loader, args)
  File "/scratch/users/fsofian19/COMP491_model/models/ast_custom/src/traintest.py", line 224, in train
    stat_dict_ord = stats_ord[0]
NameError: name 'stats_ord' is not defined
wandb: Waiting for W&B process to finish, PID 90738... (failed 1). Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.10MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.10MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.10MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.10MB uploaded (0.00MB deduped)wandb: / 0.08MB of 0.10MB uploaded (0.00MB deduped)wandb: - 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: \ 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: | 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: / 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: - 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: \ 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: | 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb: / 0.10MB of 0.10MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:   Train loss █▆▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:   Train loss 0.05351
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced wobbly-dust-2: https://wandb.ai/birdsongs/istangul-50-class/runs/bsw933h1
wandb: Find logs at: ./wandb/run-20220508_191209-bsw933h1/logs/debug.log
wandb: 

+ (( fold++ ))
+ (( fold<=1 ))

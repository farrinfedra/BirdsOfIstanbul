+ export TORCH_HOME=../../pretrained_models
+ TORCH_HOME=../../pretrained_models
+ model=ast
+ dataset=custom
+ nocall=True
+ timetest = True
/var/spool/slurm/d/job3043242/slurm_script: line 24: timetest: command not found
+ imagenetpretrain=True
+ audiosetpretrain=True
+ bal=True
+ '[' True == True ']'
+ lr=1e-5
+ freqm=24
+ timem=96
+ mixup=0
+ epoch=6
+ batch_size=48
+ fstride=10
+ tstride=10
+ base_exp_dir=./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_ncd_sm_1fold
+ '[' -d ./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_ncd_sm_1fold ']'
+ mkdir -p
mkdir: missing operand
Try 'mkdir --help' for more information.
+ (( fold=1 ))
+ (( fold<=1 ))
+ echo 'now process fold1'
now process fold1
+ exp_dir=./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_ncd_sm_1fold/fold1
+ tr_data=./data/datafiles/custom_train_data_1.json
+ te_data=./data/datafiles/custom_eval_data_1.json
+ CUDA_CACHE_DISABLE=1
+ python -W ignore ../../src/run.py --model ast --dataset custom --data-train ./data/datafiles/custom_train_data_1.json --data-val ./data/datafiles/custom_eval_data_1.json --exp-dir ./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_ncd_sm_1fold/fold1 --label-csv ./data/custom_labels.csv --n_class 21 --lr 1e-5 --n-epochs 6 --batch-size 48 --save_model False --freqm 24 --timem 96 --mixup 0 --bal True --tstride 10 --fstride 10 --imagenet_pretrain True --audioset_pretrain True
wandb: Currently logged in as: birdsongs (use `wandb login --relogin` to force relogin)
I am process 148040, running on ai03.kuacc.ku.edu.tr: starting (Tue Apr 26 08:49:46 2022)
wandb: Tracking run with wandb version 0.12.9
wandb: Syncing run legendary-firefly-30
wandb:  View project at https://wandb.ai/birdsongs/istangul-sm-dataset
wandb:  View run at https://wandb.ai/birdsongs/istangul-sm-dataset/runs/1re14rx9
wandb: Run data is saved locally in /scratch/users/fsofian19/COMP491_model/models/ast_custom/egs/custom/wandb/run-20220426_084946-1re14rx9
wandb: Run `wandb offline` to turn off syncing.

now train a audio spectrogram transformer model
balanced sampler is not used
---------------the train dataloader---------------
now using following mask: 24 freq, 96 time
now using mix-up with rate 0.000000
now process custom
use dataset mean -5.519 and std 4.572 to normalize the input.
number of classes is 21
---------------the evaluation dataloader---------------
now using following mask: 0 freq, 0 time
now using mix-up with rate 0.000000
now process custom
use dataset mean -5.519 and std 4.572 to normalize the input.
number of classes is 21
---------------AST Model Summary---------------
ImageNet pretraining: True, AudioSet pretraining: True
frequncey stride=10, time stride=10
number of patches=600

Creating experiment directory: ./exp/train-custom-f10-t10-impTrue-aspTrue-b48-lr1e-5-6-istangull_ncd_sm_1fold/fold1
Now starting training for 6 epochs
2022-04-26 08:50:12.334762
running on cuda
state dict to model completed => pretrained weights loaded
Total parameter number is : 87.273 million
Total trainable parameter number is : 87.273 million
scheduler for custom dataset is used
now training with custom, main metrics: mAP, loss function: BCEWithLogitsLoss(), learning rate scheduler: <torch.optim.lr_scheduler.MultiStepLR object at 0x7f6ebc8ef520>
current #steps=0, #epochs=1
start training...
---------------
2022-04-26 08:50:12.738200
current #epochs=1, #steps=0
Epoch: [1][100/3050]	Per Sample Total Time 16.28573	Per Sample Data Time 16.24630	Per Sample DNN Time 0.03943	Train Loss 0.0294	
0
Epoch: [1][200/3050]	Per Sample Total Time 27.80834	Per Sample Data Time 27.77852	Per Sample DNN Time 0.02982	Train Loss 0.0239	
0
Epoch: [1][300/3050]	Per Sample Total Time 39.90197	Per Sample Data Time 39.87523	Per Sample DNN Time 0.02674	Train Loss 0.0213	
0
Epoch: [1][400/3050]	Per Sample Total Time 52.44821	Per Sample Data Time 52.42305	Per Sample DNN Time 0.02516	Train Loss 0.0198	
0
Epoch: [1][500/3050]	Per Sample Total Time 65.33250	Per Sample Data Time 65.30843	Per Sample DNN Time 0.02406	Train Loss 0.0186	
0
Epoch: [1][600/3050]	Per Sample Total Time 78.16381	Per Sample Data Time 78.14037	Per Sample DNN Time 0.02344	Train Loss 0.0178	
0
Epoch: [1][700/3050]	Per Sample Total Time 91.32685	Per Sample Data Time 91.30398	Per Sample DNN Time 0.02287	Train Loss 0.0171	
0
Epoch: [1][800/3050]	Per Sample Total Time 104.43134	Per Sample Data Time 104.40881	Per Sample DNN Time 0.02253	Train Loss 0.0164	
wandb: Network error (ReadTimeout), entering retry loop.
0
Epoch: [1][900/3050]	Per Sample Total Time 117.69944	Per Sample Data Time 117.67723	Per Sample DNN Time 0.02221	Train Loss 0.0159	
0
Epoch: [1][1000/3050]	Per Sample Total Time 130.82389	Per Sample Data Time 130.80183	Per Sample DNN Time 0.02206	Train Loss 0.0155	
0
Epoch: [1][1100/3050]	Per Sample Total Time 144.17293	Per Sample Data Time 144.15101	Per Sample DNN Time 0.02192	Train Loss 0.0150	
0
Epoch: [1][1200/3050]	Per Sample Total Time 157.59706	Per Sample Data Time 157.57524	Per Sample DNN Time 0.02182	Train Loss 0.0147	
0
Epoch: [1][1300/3050]	Per Sample Total Time 171.07267	Per Sample Data Time 171.05099	Per Sample DNN Time 0.02168	Train Loss 0.0143	
0
Epoch: [1][1400/3050]	Per Sample Total Time 184.69150	Per Sample Data Time 184.66995	Per Sample DNN Time 0.02155	Train Loss 0.0140	
0
Epoch: [1][1500/3050]	Per Sample Total Time 198.25008	Per Sample Data Time 198.22859	Per Sample DNN Time 0.02149	Train Loss 0.0137	
0
Epoch: [1][1600/3050]	Per Sample Total Time 211.99596	Per Sample Data Time 211.97453	Per Sample DNN Time 0.02142	Train Loss 0.0134	
0
Epoch: [1][1700/3050]	Per Sample Total Time 225.87224	Per Sample Data Time 225.85092	Per Sample DNN Time 0.02132	Train Loss 0.0132	
0
Epoch: [1][1800/3050]	Per Sample Total Time 239.76953	Per Sample Data Time 239.74829	Per Sample DNN Time 0.02124	Train Loss 0.0130	
wandb: Network error (ReadTimeout), entering retry loop.
0
Epoch: [1][1900/3050]	Per Sample Total Time 253.61657	Per Sample Data Time 253.59536	Per Sample DNN Time 0.02122	Train Loss 0.0128	
0
Epoch: [1][2000/3050]	Per Sample Total Time 267.57509	Per Sample Data Time 267.55397	Per Sample DNN Time 0.02113	Train Loss 0.0126	
0
Epoch: [1][2100/3050]	Per Sample Total Time 281.49667	Per Sample Data Time 281.47558	Per Sample DNN Time 0.02110	Train Loss 0.0124	
0
Epoch: [1][2200/3050]	Per Sample Total Time 295.43875	Per Sample Data Time 295.41771	Per Sample DNN Time 0.02104	Train Loss 0.0122	
0
Epoch: [1][2300/3050]	Per Sample Total Time 309.54188	Per Sample Data Time 309.52088	Per Sample DNN Time 0.02099	Train Loss 0.0120	
0
Epoch: [1][2400/3050]	Per Sample Total Time 323.68051	Per Sample Data Time 323.65956	Per Sample DNN Time 0.02095	Train Loss 0.0119	
0
Epoch: [1][2500/3050]	Per Sample Total Time 337.86621	Per Sample Data Time 337.84530	Per Sample DNN Time 0.02090	Train Loss 0.0117	
0
Epoch: [1][2600/3050]	Per Sample Total Time 352.02589	Per Sample Data Time 352.00502	Per Sample DNN Time 0.02087	Train Loss 0.0116	
0
Epoch: [1][2700/3050]	Per Sample Total Time 366.20294	Per Sample Data Time 366.18211	Per Sample DNN Time 0.02083	Train Loss 0.0114	
0
Epoch: [1][2800/3050]	Per Sample Total Time 380.36653	Per Sample Data Time 380.34572	Per Sample DNN Time 0.02081	Train Loss 0.0113	
0
Epoch: [1][2900/3050]	Per Sample Total Time 394.54924	Per Sample Data Time 394.52846	Per Sample DNN Time 0.02077	Train Loss 0.0112	
0
Epoch: [1][3000/3050]	Per Sample Total Time 408.71898	Per Sample Data Time 408.69824	Per Sample DNN Time 0.02073	Train Loss 0.0111	
/datasets/xeno_canto/sm_dataset/373563.wav 0
/datasets/xeno_canto/sm_dataset/560149.wav 0
/datasets/xeno_canto/sm_dataset/140553.wav 0
/datasets/xeno_canto/sm_dataset/591241.wav 0
/datasets/xeno_canto/sm_dataset/562658.wav 0
/datasets/xeno_canto/sm_dataset/448264.wav 0
/datasets/xeno_canto/sm_dataset/477943.wav 0
/datasets/xeno_canto/sm_dataset/579375.wav 0
/datasets/xeno_canto/sm_dataset/377663.wav 0
/datasets/xeno_canto/sm_dataset/381263.wav 0
/datasets/xeno_canto/sm_dataset/240497.wav 0
/datasets/xeno_canto/sm_dataset/422052.wav 0
/datasets/xeno_canto/sm_dataset/317213.wav 0
/datasets/xeno_canto/sm_dataset/263550.wav 0
/datasets/xeno_canto/sm_dataset/422010.wav 0
0
inside validate
wandb: Network error (ReadTimeout), entering retry loop.
slurmstepd: error: Job 3043242 exceeded memory limit (203634548 > 199229440), being killed
slurmstepd: error: *** JOB 3043242 ON ai03 CANCELLED AT 2022-04-26T20:39:24 ***
slurmstepd: error: Exceeded job memory limit

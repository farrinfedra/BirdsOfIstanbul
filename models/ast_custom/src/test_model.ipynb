{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d1ad9a-909c-44f2-807e-74af01a442a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "sys.path.append(os.path.dirname(os.path.dirname(sys.path[0])))\n",
    "from utilities import *\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.cuda.amp import autocast,GradScaler\n",
    "import subprocess\n",
    "import gc\n",
    "import wandb\n",
    "from traintest import train, validate\n",
    "import dataloader_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38853205-8b82-4831-bdf1-df4a54cc56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ASTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e7cb46a-ed5d-4116-b9ec-584cd825e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ecd069-fe45-490a-a508-2427bcb97de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [-6.442367, 4.1776743]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0c4b037-8b35-44bc-9817-3046a538dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_csv = '../egs/custom/data/custom_labels.csv'\n",
    "data_train = '../egs/custom/data/datafiles/custom_train_data_1.json'\n",
    "data_eval = '../egs/custom/data/datafiles/custom_eval_data_1.json'\n",
    "\n",
    "audio_conf = {'num_mel_bins': 128, 'target_length': 512, 'freqm': 10, 'timem': 10, 'mixup': 0, \n",
    "              'dataset': 'custom', 'mode':'train', 'mean':mean[0], \n",
    "              'std':mean[1],'noise':False}\n",
    "val_audio_conf = {'num_mel_bins': 128, 'target_length': 512, \n",
    "                  'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': 'custom', 'mode':'evaluation', \n",
    "                  'mean':mean[0], 'std':mean[1], 'noise':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087514f5-ea4a-47ce-9664-154b960c6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f58829-9be3-41c5-9905-6c5dc88e1652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------the train dataloader---------------\n",
      "now using following mask: 10 freq, 10 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process custom\n",
      "use dataset mean -6.442 and std 4.178 to normalize the input.\n",
      "number of classes is 21\n",
      "---------------the evaluation dataloader---------------\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process custom\n",
      "use dataset mean -6.442 and std 4.178 to normalize the input.\n",
      "number of classes is 21\n",
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "            dataloader_custom.AudiosetDataset(data_train, label_csv=label_csv, audio_conf=audio_conf),\n",
    "            batch_size=48, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        dataloader_custom.AudiosetDataset(data_eval, label_csv=label_csv, audio_conf=val_audio_conf),\n",
    "        batch_size=batch_size*2, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "audio_model = ASTModel(label_dim=21, fstride=10, tstride=10, input_fdim=128,\n",
    "                                  input_tdim=512, imagenet_pretrain=True,\n",
    "                                  audioset_pretrain=False, model_size='base384')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6276f87-d760-4760-99d7-af9e5cafebe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: True\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=600\n"
     ]
    }
   ],
   "source": [
    "audio_model = ASTModel(label_dim=21, fstride=10, tstride=10, input_fdim=128,\n",
    "                                  input_tdim=512, imagenet_pretrain=True,\n",
    "                                  audioset_pretrain=True, model_size='base384')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if not isinstance(audio_model, nn.DataParallel):\n",
    "    audio_model = nn.DataParallel(audio_model)\n",
    "audio_model = audio_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46c7c7a8-90a3-457f-85f5-d4d8d91ce372",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_input, labels = next(iter(train_loader)) #now takes all data frome the first class => a000, lesred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab584c6c-4f6e-4e04-8e96-1f79806ab8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_grad_enabled(True)\n",
    "trainables = [p for p in audio_model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(trainables, 1e-5, weight_decay=5e-7, betas=(0.95, 0.999))\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction = 'none')#reduction = 'none'\n",
    "warmup = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "892bcfac-3be8-4cf9-aae8-b32a65b66e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d348bff2-5b60-464a-8ba3-cf3be79a11f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 512, 128])\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py\", line 139, in decorate_autocast\n    return func(*args, **kwargs)\n  File \"/scratch/users/fsofian19/COMP491_model/models/ast_custom/src/models/ast_models.py\", line 177, in forward\n    x = blk(x)\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/timm/models/vision_transformer.py\", line 198, in forward\n    x = x + self.drop_path(self.attn(self.norm1(x)))\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/timm/models/vision_transformer.py\", line 177, in forward\n    x = (attn @ v).transpose(1, 2).reshape(B, N, C)\nRuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 14.76 GiB total capacity; 13.42 GiB already allocated; 47.75 MiB free; 13.63 GiB reserved in total by PyTorch)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32099/1967714828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0maudio_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ast_env/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/torch/cuda/amp/autocast_mode.py\", line 139, in decorate_autocast\n    return func(*args, **kwargs)\n  File \"/scratch/users/fsofian19/COMP491_model/models/ast_custom/src/models/ast_models.py\", line 177, in forward\n    x = blk(x)\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/timm/models/vision_transformer.py\", line 198, in forward\n    x = x + self.drop_path(self.attn(self.norm1(x)))\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/kuacc/users/fsofian19/.conda/envs/ast_env/lib/python3.9/site-packages/timm/models/vision_transformer.py\", line 177, in forward\n    x = (attn @ v).transpose(1, 2).reshape(B, N, C)\nRuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 14.76 GiB total capacity; 13.42 GiB already allocated; 47.75 MiB free; 13.63 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "audio_model.train()\n",
    "scaler = GradScaler()\n",
    "for i, (audio_input, labels) in enumerate(train_loader):\n",
    "    print(audio_input.shape)\n",
    "    print(i)        \n",
    "    B = audio_input.size(0)\n",
    "    audio_input = audio_input.to(device, non_blocking=True)\n",
    "    labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "    with autocast():\n",
    "            audio_output = audio_model(audio_input)\n",
    "            \n",
    "            predictions = audio_output.to('cpu').detach()\n",
    "            print(\"audio_output is {out}\\n\".format(out = predictions))\n",
    "            loss = loss_fn(audio_output, labels)\n",
    "    print(loss)               \n",
    "    loss = loss.mean()\n",
    "    print('LOSS is {loss}'.format(loss = loss))\n",
    "            \n",
    "            # optimization if amp is not used\n",
    "            # optimizer.zero_grad()\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "\n",
    "            # optimiztion if amp is used\n",
    "    optimizer.zero_grad()\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scler.update()\n",
    "\n",
    "stats, valid_loss = validate(audio_model, test_loader, args, epoch)\n",
    "\n",
    "f1 = stats[0]\n",
    "avg_precision = stats[1]\n",
    "avg_recall = stats[2]\n",
    "\n",
    "print(\"f1_score: {:.6f}\".format(f1))\n",
    "print(\"avg_precision_score: {:.6f}\".format(avg_precision))\n",
    "print(\"avg_recall_score: {:.6f}\".format(avg_recall))\n",
    "print(\"train_loss: {:.6f}\".format(loss_meter.avg))\n",
    "\n",
    "if f1 > best_f1:\n",
    "    best_f1 = f1\n",
    "if main_metrics == 'mAP':\n",
    "    best_epoch = epoch\n",
    "\n",
    "scheduler.step()\n",
    "\n",
    "print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e24882a-acf9-4247-9c6a-b24529ded657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fbank(filename):\n",
    "        # mixup\n",
    "\n",
    "        waveform, sr = torchaudio.load(filename)\n",
    "        waveform = waveform - waveform.mean()\n",
    "\n",
    "\n",
    "        fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n",
    "                                                  window_type='hanning', num_mel_bins=128, dither=0.0, frame_shift=10)\n",
    "\n",
    "        target_length = 512\n",
    "        n_frames = fbank.shape[0]\n",
    "\n",
    "        p = target_length - n_frames\n",
    "\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "\n",
    "\n",
    "        return fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236afc9-8d23-4bef-80eb-cf851a50358c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "869ea1a6-2e50-4a77-8c70-42b6a411497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3119, 0.1915, 0.2662, 0.1010, 0.2327, 0.4213, 0.1695, 0.3143, 0.1613,\n",
       "         0.1568, 0.3740, 0.3630, 0.2916, 0.4650, 0.3148, 0.3887, 0.2310, 0.2693,\n",
       "         0.7696, 0.2022, 0.2180]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_fbank = get_fbank('/kuacc/users/bbiner21/birdclef-2022/birdclef-2022-16khz/skylar/XC362045.ogg')\n",
    "\n",
    "#/kuacc/users/bbiner21/birdclef-2022/birdclef-2022-16khz/akiapo/XC122401.ogg\n",
    "#/kuacc/users/bbiner21/ast/egs/custom/data/custom_data/audio_16k/XC109605.ogg\n",
    "# plt.figure()\n",
    "# cur_fbank = torch.flip(cur_fbank.t(), [0])\n",
    "# plt.imshow(cur_fbank.numpy(), cmap='gray',aspect='auto')\n",
    "print(cur_fbank.shape)\n",
    "\n",
    "norm_mean = -6.0138397\n",
    "norm_std = 4.589279\n",
    "\n",
    "cur_fbank = (cur_fbank - norm_mean) / (norm_std * 2)\n",
    "\n",
    "cur_fbank = cur_fbank.unsqueeze(0)\n",
    "out = audio_model(cur_fbank)\n",
    "out = torch.sigmoid(out)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4b72b43-2ca3-46e5-b772-7244232934de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "a = calculate_stats(out.detach(),temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
